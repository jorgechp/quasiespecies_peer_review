PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								Peak to average power ratio reduction of ZT DFT-s-OFDM signals using improved monarch butterfly optimization-PTS scheme	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Zero-tail discrete fourier transform-spread-orthogonal frequency division multiplexing; Improved monarch butterfly optimization; Orthogonal frequency division multiplexing; Peak-to-average power ratio (PAPR); Partial transmit sequence and phase rotation factors	PAPR REDUCTION; MOBILE	For traditional orthogonal multiplexing frequency division (OFDM) systems, a novel Zero Tail Discrete Fourier Transform Spread Orthogonal Frequency Division Multiplexing Division (ZT DFT-s-OFDM) waveform architecture has recently been proposed as a problem solution of low peak to average power ratio (PAPR) efficiency. The ZT DFT-s-OFDM system allows the delay in the transmission of the channel with multipath to be dynamically copied, thus the limitations to be rectified with the hard-coded Cyclic Prefix (CP). However it is affected by few roadblocks during the transmission of data, and the primary one involves the high peak-to-average power ratio (PAPR), which results in saturation observed in the power amplifier, production of more amount of interference and decreased resolution in elements such as digital/analog converters, which were considered as nonlinear. Partial transmit sequences (PTSs) is a promising plan and direct technique, ready to accomplish a viable PAPR decrease execution, yet it requires a thorough hunt to locate the ideal stage factors, which causes high computational multifaceted nature expanded with the quantity of sub-blocks. Right now, the author proposed a reduced computational complexity PTS scheme, in view of a new swarm knowledge algorithm, which is termed as Improved Monarch Butterfly Optimization (IMBO) for PAPR decrease with the ZT DFT-s-OFDM framework. The IMBO is a new swarm intelligence algorithm, with the capability of achieving an efficient optimization search that implements phase weighting process with less complexity for selecting optimum phase factors. Also, the proposed technique is quite efficient in looking for a fusion with optimal character of phase rotation factors for minimizng the computational difficulty involved. The outcomes of simulation indicate that IMBO-based PTS algorithm can substantially reduce PAPR employing an easy network structure in comparison with classical algorithms such as PTS scheme for conventional OFDM and PTS with ZT-DFT-s-OFDM.																	1868-5137	1868-5145															10.1007/s12652-020-01940-0		APR 2020											
J								Local Directional Maximum Edge Patterns for facial expression recognition	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										LDMEP (Local Directional Maximum Edge Patterns); Facial expression analysis; Feature extraction; Magnitude; Phase	NUMBER PATTERN; FACE; DESCRIPTOR	Cognitive science and neuroscience use human facial expressions of emotion. Every single facial expression can be seen at different passions in a face space. Nowadays, facial expression recognition and analysis is vital due to the demand of introducing advanced biometric applications in every domain space. The imperative task in facial expressions of emotion classification is precise feature extraction, which helps to get detailed description of facial marks. Existing feature descriptors are suffering from various problems such as intensity variations, discrimination, vulnerability etc. In this paper, propose a new feature descriptor method called LDMEP (Local Directional Maximum Edge Patterns) for facial expression analysis to overcome the hindrance. We calculated the gradients in four directions of reference pixel to elicit the more feature for better recognition instead of calculating the local differences among neighboring pixels. We also access the orientations of the pixels then thresholded based on the dynamic threshold to avoid the featureless area calculation. Furthermore, we considered only dominant magnitude and orientation directions instead of all eight directions to generate feature. Thus, imperative and efficient features are covered in dominant positions to detect the strong edges. The paper confers that how the subsequent model can be used for the recognition of facial expression of emotion.																	1868-5137	1868-5145															10.1007/s12652-020-01886-3		APR 2020											
J								Analyzing the kinematic and kinetic contributions of the human upper body's joints for ergonomics assessment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Ergonomics assessment; RULA; Joint angles; Joint torques; Upper human body; Industrial tasks; Evaluating performance	MUSCULOSKELETAL DISORDERS; RULA; CONSTRUCTION	During an eight-hour shift, an industrial worker will inevitably cycle through specific postures. Those postures can cause microtrauma on the musculoskeletal system that accumulates, which in turn can lead to chronic injury. To assess how problematic a posture is, the rapid upper limb assessment (RULA) scoring system is widely employed by the industry. Even though it is a very quick and efficient method of assessment, RULA is not a biomechanics-based measurement that is anchored in a physical parameter of the human body. As such RULA does not give a detailed description of the impact each posture has on the human joints but rather, an overarching, simplified assessment of a posture. To address this issue, this paper proposes the use of joint angles and torques as an alternative way of ergonomics evaluation. The cumulative motion and torque throughout a trial is compared with the average motions and torques for the same task. This allows the evaluation of each joint's kinematic and kinetic performance while still be able to assess a task"at-a-glance". To do this, an upper human body model was created and the mass of each segment were assigned. The joint torques and the RULA scores were calculated for simple range of motion (ROM) tasks, as well as actual tasks from a TV assembly line. The joint angles and torques series were integrated and then normalized to give the kinematic and kinetic contribution of each joint during a task as a percentage. This made possible to examine each joint's strain during each task as well as highlight joints that need to be more closely examined. Results show how the joint angles and torques can identify which joint is moving more and which one is under the most strain during a task. It was also possible to compare the performance of a task with the average performance and identify deviations that may imply improper execution. Even though the RULA is a very fast and concise assessment tool, it leaves little room for further analyses. However, the proposed work suggests a richer alternative without sacrificing the benefit of a quick evaluation. The biggest limitation of this work is that a pool of proper executions needs to be recorded for each task before individual comparisons can be done.																	1868-5137	1868-5145															10.1007/s12652-020-01926-y		APR 2020											
J								Performance investigation of grid integrated photovoltaic/wind energy systems using ANFIS based hybrid MPPT controller	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Hybrid system; MPPT; PV system; PMSG; Voltage source inverter (VSI)	POWER-POINT TRACKING; PV SYSTEM	The solar energy and wind energy are two natural, renewable energy resources for the mankind, for the energy supply without hassles that provided an avenue for its utilization. For the efficient of the resources, wind power generation is one of the options in association with a photovoltaic system for preserving solar energy. Hence the current system requires technology to be involved to control the operation to satisfy the major concern as initially a combinational hybrid system to keep track maximum power, DC voltage input and trade-off between renewable energy delivered for interconnections. The fluctuation in power for interconnection can be minimized by adaptive neuro fuzzy interference method (ANFIS) dependent maximum power point tracking (MPPT) system and microscopically trusted ANFIS based system. In this paper the hybrid MPPT controllers are compared that is with perturb-observe (P&O) and tip speed ratio (TSR) as one set of MPPT controller and another side incremental conductance method (ICM) and hill climb search (HCS) both the controllers are tested with the ANFIS. The proposed system to satisfy the defined objectives is modeled using MATLAB/SIMULINK software by connecting power grid system to inverter influenced by ANFIS controller.																	1868-5137	1868-5145															10.1007/s12652-020-01967-3		APR 2020											
J								Capital-constrained supply chain with altruism and reciprocity	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Capital constraint; Altruism and reciprocity; Loan limit; Supply chain management	TRADE CREDIT; VS. BANK; FAIRNESS; FINANCE; MODEL; MANUFACTURER; PREFERENCES; MANAGEMENT; ECONOMICS; TRUST	Recently, supply chain finance has attracted increasing attention from the scientific community. However, it lacks serious consideration of social preference in supply chain finance. In this paper, we examine the effects of altruism and reciprocity on the efficiency of a capital-constrained supply chain consisting of a manufacturer with no original capital and a retailer with limited capital in a perishable product market with uncertain demand. The retailer may be in need of short-term financing. We show that, when the retailer has not enough money for his ideal inventory, financing will significantly improve his utility and supply chain efficiency, and global altruism will improve his financing willingness. Moreover, when the retailer borrows a full loan or has more than enough capital, both the global altruistic behavior and the intrinsic altruistic behavior allow sustaining more efficiency of supply chain than with selfish ones, but a more reciprocal behavior improves efficiency if and only if the manufacturer is intrinsically more altruistic than the retailer. Nevertheless, it is different from what may be expected that when the retailer borrows a limited loan or does not borrow but use up all the capital, altruism and reciprocity have no effect on supply chain efficiency. This article thus effectively encourages capital-constrained supply chains to incorporate financial consideration into production decisions and properly guides the firms in the chains either to act more altruistically or to reciprocate more the attitude of their partners.																	1868-5137	1868-5145															10.1007/s12652-020-01927-x		APR 2020											
J								Interval Analysis Technique for Versatile and Parallel Multi-Agent Collision Detection and Avoidance	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Collision detection; Collision avoidance; Dynamic obstacle; Interval analysis; Multi-agent; Embedded vehicle	OBSTACLE AVOIDANCE; NAVIGATION; COVERAGE; ALGORITHMS; TRACKING; SYSTEMS	Collision detection and avoidance are vital sub-tasks in any autonomous robotic task. This work presents a technique that guarantees collision avoidance in an environment containing multiple agents. These agents can be of different types such as static, dynamic obstacles and other robots. The technique supports parallel implementation and is appropriate for real-time applications. In this work, the theory of interval arithmetic is used to represent the pose of agents as intervals in a fixed time period. Geometrically, the intervals correspond to finite-length arcs and line-segments. Theoretical results on the inclusion of one interval, in another interval, in terms of sub-intervals are derived. Breaking down the problem in sub-intervals supports parallelism in performing multiple interval inclusion tests and in handling multiple agents. The proposed interval-arithmetic based framework leads directly to a hardware-efficient collision detection scheme. In particular, the proposed strategy admits a solution even for a dynamic environment using just shift and add capability, an important aspect for embedded implementation. The solution of interval inclusion is also used to find a set of solutions for guaranteed collision avoidance with multiple agents with known or unknown trajectories. Simulation results in MATLAB and experiments with an FPGA-driven differential drive mobile robot demonstrate the versatility of the proposed approach.																	0921-0296	1573-0409				JUN	2020	98	3-4					705	720		10.1007/s10846-019-01091-1		APR 2020											
J								Incremental one-class classifier based on convex-concave hull	PATTERN ANALYSIS AND APPLICATIONS										One-class classification; Data stream; Online learning; Convex hull; Convex-concave hull	DATA STREAMS; FRAUD DETECTION; SUPPORT; ALGORITHM; MACHINE; ENSEMBLE; SELECTION; MODEL; SET; UNCERTAINTY	One subject that has been considered less is a binary classification on data streams with concept drifting in which only information of one class (target class) is available for learning. Well-known methods such as SVDD and convex hull have tried to find the enclosed boundary around target class, but their high complexity makes them unsuitable for large data sets and also online tasks. This paper presents a novel online one-class classifier adapted to the streaming data. Considering time complexity, an incremental convex-concave hull classification method, called ICCHC, is proposed which can significantly reduce the computational time and expand the target class boundary. Also, it can be adapted to the gradual concept drift. Evaluations have been conducted on seventeen real-world data sets by hold-out validation. Also, noise analysis has been carried out. The results of the experiments have been compared with the state-of-the-art methods, which show the superiority of ICCHC regarding the accuracy, precision, and recall metrics.																	1433-7541	1433-755X				NOV	2020	23	4					1523	1549		10.1007/s10044-020-00876-7		APR 2020											
J								Empirical evaluation and study of text stemming algorithms	ARTIFICIAL INTELLIGENCE REVIEW										Natural language processing; Information retrieval; Text mining; Stemming algorithms; Stemmer evaluation methods; Urdu stemming	LIGHT STEMMER; LANGUAGE; URDU; MORPHOLOGY; IMPACT	Text stemming is one of the basic preprocessing step for Natural Language Processing applications which is used to transform different word forms into a standard root form. For Arabic script based languages, adequate analysis of text by stemmers is a challenging task due to large number of ambigious structures of the language. In literature, multiple performance evaluation metrics exist for stemmers, each describing the performance from particular aspect. In this work, we review and analyze the text stemming evaluation methods in order to devise criteria for better measurement of stemmer performance. Role of different aspects of stemmer performance measurement like main features, merits and shortcomings are discussed using a resource scarce language i.e. Urdu. Through our experiments we conclude that the current evaluation metrics can only measure an average conflation of words regardless of the correctness of the stem. Moreover, some evaluation metrics favor some type of languages only. None of the existing evaluation metrics can perfectly measure the stemmer performance for all kind of languages. This study will help researchers to evaluate their stemmer using right methods.																	0269-2821	1573-7462				DEC	2020	53	8					5559	5588		10.1007/s10462-020-09828-3		APR 2020											
J								Compacting frequent star patterns in RDF graphs	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Semantic Web; RDF compaction; Linked data; Knowledge graph		Knowledge graphs have become a popular formalism for representing entities and their properties using a graph data model, e.g., the Resource Description Framework (RDF). An RDF graph comprises entities of the same type connected to objects or other entities using labeled edges annotated with properties. RDF graphs usually contain entities that share the same objects in a certain group of properties, i.e., they match star patterns composed of these properties and objects. In case the number of these entities or properties in these star patterns is large, the size of the RDF graph and query processing are negatively impacted; we refer these star patterns as frequent star patterns. We address the problem of identifying frequent star patterns in RDF graphs and devise the concept of factorized RDF graphs, which denote compact representations of RDF graphs where the number of frequent star patterns is minimized. We also develop computational methods to identify frequent star patterns and generate a factorized RDF graph, where compact RDF molecules replace frequent star patterns. A compact RDF molecule of a frequent star pattern denotes an RDF subgraph that instantiates the corresponding star pattern. Instead of having all the entities matching the original frequent star pattern, a surrogate entity is added and related to the properties of the frequent star pattern; it is linked to the entities that originally match the frequent star pattern. Since the edges between the entities and the objects in the frequent star pattern are replaced by edges between these entities and the surrogate entity of the compact RDF molecule, the size of the RDF graph is reduced. We evaluate the performance of our factorization techniques on several RDF graph benchmarks and compare with a baseline built on top gSpan, a state-of-the-art algorithm to detect frequent patterns. The outcomes evidence the efficiency of proposed approach and show that our techniques are able to reduce execution time of the baseline approach in at least three orders of magnitude. Additionally, RDF graph size can be reduced by up to 66.56% while data represented in the original RDF graph is preserved.																	0925-9902	1573-7675				DEC	2020	55	3					561	585		10.1007/s10844-020-00595-9		APR 2020											
J								"Just-in-time" generation of datasets by considering structured representations of given consent for GDPR compliance	KNOWLEDGE AND INFORMATION SYSTEMS										GDPR; Consent; Data integration		Data processing is increasingly becoming the subject of various policies and regulations, such as the European General Data Protection Regulation (GDPR) that came into effect in May 2018. One important aspect of GDPR is informed consent, which captures one's permission for using one's personal information for specific data processing purposes. Organizations must demonstrate that they comply with these policies. The fines that come with non-compliance are of such importance that it has driven research in facilitating compliance verification. The state-of-the-art primarily focuses on, for instance, the analysis of prescriptive models and posthoc analysis on logs to check whether data processing is compliant to GDPR. We argue that GDPR compliance can be facilitated by ensuring datasets used in processing activities are compliant with consent from the very start. The problem addressed in this paper is how we can generate datasets that comply with given consent "just-in-time". We propose RDF and OWL ontologies to represent the consent that an organization has collected and its relationship with data processing purposes. We use this ontology to annotate schemas, allowing us to generate declarative mappings that transform (relational) data into RDF driven by the annotations. We furthermore demonstrate how we can create compliant datasets by altering the results of the mapping. The use of RDF and OWL allows us to implement the entire process in a declarative manner using SPARQL. We have integrated all components in a service that furthermore captures provenance information for each step, further contributing to the transparency that is needed towards facilitating compliance verification. We demonstrate the approach with a synthetic dataset simulating users (re-)giving, withdrawing, and rejecting their consent on data processing purposes of systems. In summary, it is argued that the approach facilitates transparency and compliance verification from the start, reducing the need for posthoc compliance analysis common in the state-of-the-art.																	0219-1377	0219-3116				SEP	2020	62	9					3615	3640		10.1007/s10115-020-01468-x		APR 2020											
J								BinEHO: a new binary variant based on elephant herding optimization algorithm	NEURAL COMPUTING & APPLICATIONS										Binary optimization; Elephant herding optimization; Knapsack problem; Uncapacitated facility location	BEE COLONY ALGORITHM; DIFFERENTIAL EVOLUTION ALGORITHM; PARTICLE SWARM OPTIMIZATION; ARTIFICIAL ALGAE ALGORITHM; WIND TURBINE; KNAPSACK-PROBLEM; SEARCH; PLACEMENT; PERFORMANCE	One of the new optimization techniques proposed in recent years is elephant herding optimization (EHO) algorithm. Despite its short history, EHO has been used to solve many engineering and real-world problems by attracting researcher attention with its advantages such as efficient global search ability, having fewer control parameters and ease of implementation. However, there is no remarkable binary variant of EHO algorithm in the literature. A new binary approach based on EHO algorithm is proposed in this study. The newer binary variant of EHO named as BinEHO is binarized with preserving the search ability of basic EHO. The main purpose of the study is to present a simple, efficient and robust binary variant which copes with different binary problems. Therefore, the proposed method is tested on three important binary optimization problems, 0-1 knapsack, uncapacitated facility location and wind turbine placement, in order to show its performance and accuracy. In addition, the BinEHO is compared with various binary variants on these problems. Experimental results and comparisons show that the BinEHO algorithm is a robust and efficient tool for binary optimization.																	0941-0643	1433-3058				NOV	2020	32	22			SI		16971	16991		10.1007/s00521-020-04917-4		APR 2020											
J								An intelligent heuristic-clustering algorithm to determine the most probable reservoir model from pressure-time series in underground reservoirs	SOFT COMPUTING										Well testing; Reservoir model; Genetic algorithm (GA); Density-based spatial clustering of applications with noise (DBSCAN); KMEANS; Monte Carlo simulation	ARTIFICIAL NEURAL-NETWORK; GENETIC ALGORITHMS; OPTIMIZATION; PARAMETERS; DESIGN	Precise characterization of underground reservoirs requires accurate calculations of the reservoir's petrophysical data and accurate selection of the mathematical model governing the reservoir's dynamic. In this study, we develop a novel heuristic-clustering algorithm, namely GA-DBSCAN-KMEANS, that can be applied over pressure transient data to assess the true reservoir model out of a pool of candidates. In this algorithm, each specific reservoir model is considered a subpopulation in the GA (genetic algorithm). Then, the simultaneous optimization of all the reservoir models is sought using the proposed hybrid algorithm. During the optimization process, the population size of different models will be either decreased, increased, or unchanged based on the average quality match obtained for each model. A combined DBSCAN (density-based spatial clustering of applications with noise)-KMEANS clustering scheme is used to increase the population size for the best reservoir model in each iteration of the GA. The accuracy of the proposed algorithm was verified using several synthetic data and a real field case obtained from the open literature. The tested data were collected from different types of reservoir models, including homogeneous reservoirs, matrix-fracture dual-porosity reservoirs, and fault-limited reservoirs. For uncertainty analyses and to test the performance of the algorithm under large numbers of initializations, Monte Carlo simulations were conducted. Results of the Monte Carlo simulations unveiled high values of P10, P50, and P90 for the probability of the true reservoir model and low values of these statistics for the false reservoir models. This shows that the outcome of the proposed algorithm is not affected by the initial randomization of the solution subspaces; hence, the developed algorithm is a reliable tool in determining the most probable reservoir model from transient well testing data.																	1432-7643	1433-7479				OCT	2020	24	20					15773	15794		10.1007/s00500-020-04908-6		APR 2020											
J								A new bi-objective integrated dynamic cell formation and AGVs' dwell point location problem on the inter-cell unidirectional single loop	SOFT COMPUTING										Dynamic cell formation; Automated guided vehicle; Unidirectional single loop; Bi-objective optimization; NSGA-II	MATHEMATICAL-MODEL; MANUFACTURING SYSTEMS; GENETIC-ALGORITHM; OPTIMIZATION ALGORITHM; OPERATOR ASSIGNMENT; DESIGN; LAYOUT	In order to increase the flexibility, space utilization, product quality and safety, transferring the semi-manufactured parts between machines is performed by automated guided vehicles (AGVs). AGVs should be properly managed to maintain their efficiency as a material handling equipment in production systems. This AGVs' management will be more important where the circumstances will be dynamic (i.e., where the volume and variety of demands are different one period to another). To determine one or several homes for idle AGVs (AGVs' dwell point), the assignment of AGVs to manufacturing cell and to prevent the AGVs collisions are three basic aspects in their management. This paper proposes a new nonlinear mixed-integer bi-objective mathematical model of dynamic cell formation and dwell point location problem (DCFDPLP) for AGVs on unidirectional single loop in which AGVs assignment, location of dwell points for AGVs and transportation time of AGVs are considered besides cell reconfiguration, part families and machine groups formation in a dynamic environment. The first objective is to minimize the related costs, and the second one is to minimize the maximum response time for all AGVs. The nonlinear model is transformed to linear one. Due to the NP-hardness of DCFDPLP, a non-dominated sorting genetic algorithm (NSGA-II) is developed to solve the problem. Finally, randomly generated test problems are generated to demonstrate the performance of NSGA-II as a solution procedure.																	1432-7643	1433-7479				NOV	2020	24	21					16021	16042		10.1007/s00500-020-04921-9		APR 2020											
J								A hybrid fuzzy multi-attribute decision making model to select road pavement type	SOFT COMPUTING										Road pavement type; Life-cycle cost analysis (LCCA); Multi-attribute decision making (MADM); Fuzzy logic	ANP; TOPSIS; MANAGEMENT; DEMATEL	Pavement is an important part of a road structure. Selection of road pavement type is one of the important challenges of decision making by highway administrators. The life-cycle cost analysis is one of the common tools to select the pavement type which considers economic criteria such as costs of construction, maintenance and rehabilitation, and user costs; however, noneconomic criteria are neglected. The inadequate selection of road pavement type tends to increase the pavement life-cycle cost and impacts both environmental and social parameters. In this study, a three-step hybrid model is proposed to solve this problem. Initially, the causal relationships of main criteria affecting the pavement type selection are determined using decision making trial and evaluation laboratory. Then, the weight and importance of criteria and sub-criteria are determined using fuzzy analytic network process. Finally, the ranking of pavement alternatives is carried out using a fuzzy technique for order preference by similarity to ideal solution. A real case study consisting of five different types of pavements including hot mix asphalt, stone mastic asphalt, jointed plain concrete pavement, roller compacted concrete pavement with hot mix asphalt overlay and continuous reinforced concrete pavement is used to show the validity of the proposed model.																	1432-7643	1433-7479				NOV	2020	24	21					16135	16148		10.1007/s00500-020-04928-2		APR 2020											
J								Classification of noiseless corneal image using capsule networks	SOFT COMPUTING										Capsule network; Image classification; Corneal OCT		Classifying a particular image from a data set is a complex work for any image analyst. Generally, the output of medical image scan gives numerous images for analysis. In that, the image analyst has to manually predict a better noiseless image for computer-assisted image process program. Manual verification of all the output images from the scan device consumes a lot of time in predicting the abnormality of a patient. The proposed capsule network for noiseless image algorithm assists the image analyst by classifying the noiseless image from the data set for further computer-assisted image enhancement or segmentation program. The proposed algorithm performance is evaluated and compared with the existing algorithms in terms of accuracy, sensitivity, specificity, positive predictive value, and negative predictive value.																	1432-7643	1433-7479				NOV	2020	24	21					16201	16211		10.1007/s00500-020-04933-5		APR 2020											
J								Gaussian-kernel c-means clustering algorithms	SOFT COMPUTING										Clustering; Hard c-means (HCM); Fuzzy c-means (FCM); Gaussian-kernel HCM (GK-HCM); Gaussian-kernel FCM (GK-FCM); MRI segmentation		Partitional clustering is the most used in cluster analysis. In partitional clustering, hard c-means (HCM) (or called k-means) and fuzzy c-means (FCM) are the most known clustering algorithms. However, these HCM and FCM algorithms work worse for data sets in a noisy environment and get inaccuracy when the data set has different shape clusters. For solving these drawbacks in HCM and FCM, Wu and Yang (Pattern Recognit 35:2267-2278, 2002) proposed the alternative c-means clustering with an exponential-type distance that extends HCM and FCM into alternative HCM (AHCM) and alternative FCM (AFCM). In this paper, we construct a more generalization of AHCM and AFCM with Gaussian-kernel c-means clustering, called GK-HCM and GK-FCM. For theoretical behaviors of GK-FCM, we analyze the bordered Hessian matrix and then give the theoretical properties of the GK-FCM algorithm. Some numerical and real data sets are used to compare the proposed GK-HCM and GK-FCM with AHCM and AFCM methods. Experimental results and comparisons actually demonstrate these good aspects of the proposed GK-HCM and GK-FCM algorithms with its effectiveness and usefulness. Finally, we apply the GK-FCM algorithm to MRI segmentation.																	1432-7643	1433-7479															10.1007/s00500-020-04924-6		APR 2020											
J								Leveraging deep graph-based text representation for sentiment polarity applications	EXPERT SYSTEMS WITH APPLICATIONS										Sentiment analysis; Graph representation; Representation learning; Feature learning; Deep neural networks	NEURAL-NETWORKS; MACHINE; SYSTEM	Over the last few years, machine learning over graph structures has manifested a significant enhancement in text mining applications such as event detection, opinion mining, and news recommendation. One of the primary challenges in this regard is structuring a graph that encodes and encompasses the features of textual data for the effective machine learning algorithm. Besides, exploration and exploiting of semantic relations is regarded as a principal step in text mining applications. However, most of the traditional text mining methods perform somewhat poor in terms of employing such relations. In this paper, we propose a sentence-level graph-based text representation which includes stop words to consider semantic and term relations. Then, we employ a representation learning approach on the combined graphs of sentences to extract the latent and continuous features of the documents. Eventually, the learned features of the documents are fed into a deep neural network for the sentiment classification task. The experimental results demonstrate that the proposed method substantially outperforms the related sentiment analysis approaches based on several benchmark datasets. Furthermore, our method can be generalized on different datasets without any dependency on pre-trained word embeddings. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113090	10.1016/j.eswa.2019.113090													
J								Incremental and dynamic graph construction with application to image classification	EXPERT SYSTEMS WITH APPLICATIONS										Incremental graph construction; Locality-constrained linear coding; Graph-based label propagation; Graph-based linear manifold learning; Semi-supervised learning; Face recognition	DIMENSIONALITY REDUCTION; FACE RECOGNITION; LAPLACIAN	In this paper, we propose a dynamic graph construction technique that inserts new samples into a previously constructed graph, which reduces the computational time and complexity of the classic batch construction schemes. The basic assumption of the proposed method is that by adding one sample into a graph, only its close nodes will be affected, hence, it is not necessary to update the whole graph but only the weights of close nodes. The proposed method has two steps of insertion and updating. In the insertion phase, the similarity between the new sample and the available data is calculated. The similarity vector is retrieved either from a distance function or a coding scheme. Then, in the update phase, by evaluating the similarity vector of the new sample, close nodes which will be affected are identified and the graph weights of these close (similar) nodes are updated. By adopting this scenario, in each insertion of a node (or a set of nodes), only the weights of very few nodes have to be updated. It is worthy to mention that since the proposed method does not invoke labels of the samples, it can be adopted by any unsupervised, semi-supervised or supervised technique. A set of extensive experiments for the task of classification on different image datasets show that, in various post-graph learning tasks (i.e., Label propagation and Manifold learning), the graph which is constructed by the proposed method, even after hundreds of insertions and updates, has a similar performance (in some cases it can be better) compared to the graph which is constructed from scratch. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113117	10.1016/j.eswa.2019.113117													
J								Advanced orthogonal learning-driven multi-swarm sine cosine optimization: Framework and case studies	EXPERT SYSTEMS WITH APPLICATIONS										Sine cosine algorithm; Orthogonal learning; Multi-swarm; Greedy selection	PARTICLE SWARM; DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; ENGINEERING OPTIMIZATION; INSPIRED OPTIMIZER; GENETIC ALGORITHM; SEARCH ALGORITHM; LOCAL SEARCH; DESIGN; COLONY	Sine cosine algorithm (SCA) is a widely used nature-inspired algorithm that is simple in structure and involves only a few parameters. For some complex tasks, especially high-dimensional problems and multimodal problems, the basic method may have problems in harmonic convergence or trapped into local optima. To efficiently alleviate this deficiency, an improved variant of basic SCA is proposed in this paper. The orthogonal learning, multi-swarm, and greedy selection mechanisms are utilized to improve the global exploration and local exploitation powers of SCA. In preference, the orthogonal learning procedure is introduced into the conventional method to expand its neighborhood searching capabilities. Next, the multi-swarm scheme with three sub-strategies is adopted to enhance the global exploration capabilities of the algorithm. Also, a greedy selection strategy is applied to the conventional approach to improve the qualities of the search agents. Based on these three strategies, we called the improved SCA as OMGSCA. The proposed OMGSCA is compared with a comprehensive set of meta-heuristic algorithms including six other improved SCA variants, basic version, and ten advanced meta-heuristic algorithms. We employed thirty IEEE CEC2014 benchmark functions, and eight advanced meta-heuristic algorithms on seventeen real-world benchmark problems from IEEE CEC2011. Also, non-parametric statistical Wilcoxon sign rank and the Friedman tests are performed to monitor the performance of the proposed method. The obtained experimental results demonstrate that the introduced strategies can significantly improve the exploratory and exploitative inclinations of the basic algorithm. The convergence speed of the original method has also been improved, substantially. The results suggest the proposed OMGSCA can be used as an effective and efficient auxiliary tool for solving complex optimization problems. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113113	10.1016/j.eswa.2019.113113													
J								Personalized itinerary recommendation: Deep and collaborative learning with textual information	EXPERT SYSTEMS WITH APPLICATIONS										Personalized itinerary recommendation; Textual information; Deep learning; Collaborative learning; Iterated local search	ORIENTEERING PROBLEM; DISTANCE	Personalized itinerary recommendation is a complicated and challenging task, which aims to construct and recommend a visit sequence consists of multiple Points of Interest (POIs) with the constraints that maximizing user satisfaction while adhering time budget. User interests, therefore, becomes the most crucial element in the recommendation task, determining the POIs and their visit durations in the itinerary. In this paper, we propose a novel framework named DCC-PersIRE to infer the user interests and recommend personalized itinerary consists of POIs, visit durations and visit sequence. Specifically, we employ an unsupervised deep learning model to embed the POI textual contents, and then propose a DCC model, which seamlessly integrates the embedded POI textual contents with the traditional and widely used user-POI visits and POI categories, to predict the user interests as well as the visit durations. Then, after formulating itinerary construction as a variant of the Orienteering Problem, an Iterated Local Search based algorithm is proposed to calculate the visit sequence with maximized satisfaction consists of multiple POIs and personalized POI visit durations, i.e., the optimal itinerary. Extensive experiments on eight real-world datasets validate the effectiveness of DCC-PersIRE. The experimental results show that our algorithm delivers a more fine-grained prediction of user interests and outperforms various state-of-the-art baselines in tour itinerary recommendation. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113070	10.1016/j.eswa.2019.113070													
J								Discriminative globality and locality preserving graph embedding for dimensionality reduction	EXPERT SYSTEMS WITH APPLICATIONS										Dimensionality reduction; Graph embedding; Graph construction; Pattern classification	FEATURE-SELECTION; FACE RECOGNITION; PROJECTIONS; CLASSIFICATION; EXTENSIONS; EIGENMAPS; FRAMEWORK	Graph embedding in dimensionality reduction has attracted much attention in the high-dimensional data analysis. Graph construction in graph embedding plays an important role in the quality of dimensionality reduction. However, the discrimination information and the geometrical distributions of data samples are not fully exploited for discovering the essential geometrical and discriminant structures of data and strengthening the pattern discrimination in graph constructions of graph embedding. To overcome the limitations of graph constructions, in this article we propose a novel graph-based dimensionality reduction method entitled discriminative globality and locality preserving graph embedding (DGLPGE) by designing the informative globality and locality preserving graph constructions. In the constructed graphs, bidirectional weights of edges are newly defined by considering both the geometrical distributions of each point of edges and the class discrimination. Using the adjacent weights of graphs, we characterize the intra-class globality preserving scatter, the inter-class globality preserving scatter and the locality preserving scatter to formulate the objective function of DGLPGE in order to optimize the projection of dimensionality reduction. Extensive experiments demonstrate that the proposed DGLPGE often outperforms the state-of-the-art dimensionality reduction methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113079	10.1016/j.eswa.2019.113079													
J								SCGSA: A sine chaotic gravitational search algorithm for continuous optimization problems	EXPERT SYSTEMS WITH APPLICATIONS										Gravitational search algorithm; Chaotic maps; Sine cosine algorithm; Continuous optimization problem		Gravitational search algorithm (GSA), as one of the novel meta-heuristic optimization algorithms inspired by the law of gravity and mass interactions, is however prone to local optima stagnation due to heavier gravity. Hence, an enhanced version, chaotic gravitational constants for the gravitational search algorithm (CGSA), was proposed to improve the exploration ability through various chaotic maps. In this paper, with insightful utilization of sine cosine algorithm, we put forward sine chaotic gravitational search algorithm (SCGSA) as a further step of CGSA to escape from its local optima stagnation. The experiments show remarkable results in both the speed of convergence and the ability of finding global optima in 30 benchmark functions (CEC 2014), thus proving a better balance between exploration and exploitation in SCGSA compared with CGSA. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113118	10.1016/j.eswa.2019.113118													
J								Characterizing Complexity and Self-Similarity Based on Fractal and Entropy Analyses for Stock Market Forecast Modelling	EXPERT SYSTEMS WITH APPLICATIONS										Hurst exponent; Shannon entropy; Renyi entropy; Artificial neural network; Regression algorithms; Fractal analysis; Complexity and Self-Similarity; Stock indices forecasting	HURST EXPONENT; RANKING EFFICIENCY; MUTUAL INFORMATION; FINANCIAL CRISIS; NEURAL-NETWORKS; MAX-RELEVANCE; SUPPORT; PREDICTION; INDEX; VOLATILITY	Complex systems constitute components that interact with one another and involve phenomena which are not always easy to understand in terms of their components and interactions. Alternative mathematical models have been developed so that the users' tasks can be facilitated and an actual assistance can be provided for decision-making processes in case of every encountered incident which requires critical decision-making. Within this framework, financial systems can be regarded as complex systems with their volatile and vulnerable nature along with various parameters and interactions involved. Forecasting shifts in stock indices is crucial to validate the potential strategies of monetary mechanisms. Therefore, forecasting is an essential step in financial decision-making to manage data selection and attain robust prediction. Our purpose is to optimize the stock indices' forecasting model in the stock indices dataset, constructed from the daily values. The following steps were applied to demonstrate the critical significance of Hurst exponent (HE) computed by Rescaled Range (10) fractal analysis when used as indicator in conjunction with Shannon entropy (SE) and Renyi entropy (RE) for the future forecasting ability of the stock indices. With this aim, the following stages were performed with indicators obtained from the applications and added into the dataset in the respective order. The first stage consists of: i) HE indicator, ii) Entropy based SE and RE indicators, iii) HE, SE and RE indicators. As the second stage, stock indices day-to-day valuation was evaluated using Multi Layer Regression (MLR), Support Vector Regression (SVR) and Feed Forward Back Propagation (FFBP) algorithms, applied for each indicator for comparative analysis. When compared with earlier works, no relevant work exists in the literature in which the algorithms and above-mentioned indicators have been used in conjunction with one another. This paper, through the multistage methodology and proposed model, demonstrates that HE is obviously a significant and critical determining indicator compared to RE and SE indicators for forecasting purposes. Consequently, experimental results demonstrate the accuracy and applicability of the proposed method. Thus, this study attempts to illustrate a new frontier in domains concerning critical decision-making processes in nonlinear, dynamic, and volatile environments. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113098	10.1016/j.eswa.2019.113098													
J								DNNRec: A novel deep learning based hybrid recommender system	EXPERT SYSTEMS WITH APPLICATIONS										Deep learning; Recommender systems; Embeddings; Side information; Cyclical learning rates; Deep neural network; Cold start problem	SENTIMENT ANALYSIS	We propose a novel deep learning hybrid recommender system to address the gaps in Collaborative Filtering systems and achieve the state-of-the-art predictive accuracy using deep learning. While collaborative filtering systems are popular with many state-of-the-art achievements in recommender systems, they suffer from the cold start problem, when there is no history about the users and items. Further, the latent factors learned by these methods are linear in nature. To address these gaps, we describe a novel hybrid recommender system using deep learning. The solution uses embeddings for representing users and items to learn non-linear latent factors. The solution alleviates the cold start problem by integrating side information about users and items into a very deep neural network. The proposed solution uses a decreasing learning rate in conjunction with increasing weight decay, the values cyclically varied across epochs to further improve accuracy. The proposed solution is benchmarked against existing methods on both predictive accuracy and running time. Predictive Accuracy is measured by Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and R-squared. Running time is measured by the mean and standard deviation across seven runs. Comprehensive experiments are conducted on several datasets such as the MovieLens 100 K, FilmTrust, Book-Crossing and MovieLens 1 M. The results show that the proposed technique outperforms existing methods in both non-cold start and cold start cases. The proposed solution framework is generic from the outperformance on four different datasets and can be leveraged for other ratings prediction datasets in recommender systems. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113054	10.1016/j.eswa.2019.113054													
J								Intelligent traffic control for autonomous vehicle systems based on machine learning	EXPERT SYSTEMS WITH APPLICATIONS										Intelligent traffic control; Machine learning; Autonomous vehicle systems; Material handling; Vehicle routing	MATERIAL HANDLING-SYSTEM; SEQUENTIAL DETECTION; OPTIMIZATION METHOD; ROUTING PROBLEM; CONGESTION; DESIGN; HEURISTICS; SIMULATION; MANAGEMENT; ALGORITHM	This study aimed to resolve a real-world traffic problem in a large-scale plant. Autonomous vehicle systems (AVSs), which are designed to use multiple vehicles to transfer materials, are widely used to transfer wafers in semiconductor manufacturing. Traffic control is a significant challenge with AVSs because all vehicles must be monitored and controlled in real time, to cope with uncertainties such as congestion. However, existing traffic control systems, which are primarily designed and controlled by human experts, are insufficient to prevent heavy congestion that impedes production. In this study, we developed a traffic control system based on machine learning predictions, and a routing method that dynamically determines AVS routes with reduced congestion rates. We predicted congestion for critical bottleneck areas, and utilized the predictions for adaptive routing control of all vehicles to avoid congestion. We conducted an experimental evaluation to compare the predictive performance of four popular algorithms. We performed a simulation study based on data from semiconductor fabrication to demonstrate the utility and superiority of the proposed method. The experimental results showed that AVSs with the proposed approach outperformed the existing approach in terms of delivery time, transfer time, and queuing time. We found that adopting machine learning-based traffic control can enhance the performance of existing AVSs and reduce the burden on the human experts who monitor and control AVSs. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113074	10.1016/j.eswa.2019.113074													
J								A hybrid recommendation system for Q&A documents	EXPERT SYSTEMS WITH APPLICATIONS										Q&A documents; Recommendation systems; Knowledge management	KNOWLEDGE MANAGEMENT; QUESTIONS; RETRIEVAL; TRUST	Question and answer (Q&A) documents are a new type of knowledge document composed of a question part and an answer part. The questions represent knowledge needs, and the answers contain the knowledge that meets these knowledge needs. An overload of accumulated Q&A documents decreases the reuse of valuable knowledge. In this paper, we propose a novel hybrid system to recommend Q&A documents to alleviate overload. First, knowledge needs are partitioned, and current knowledge needs are identified by sequentially clustering the Q&A documents. Second, a content-based (CB) recommendation method, a collaborative filtering (CF) recommendation method and a complementarity-based recommendation method are used to find the Q&A documents that are potentially helpful for the user. Third, the three initial recommendation lists of Q&A documents derived from the three recommendation methods are combined to form a more comprehensive recommendation based on the Fermat point. Because reading all Q&A documents in the recommendation list consumes an enormous amount of time and users prefer to read Q&A documents one by one starting from the top, a novel ranking mechanism is proposed to ensure that users obtain comprehensive knowledge to the greatest extent possible from the limited number of Q&A documents at the top of the list. The proposed approach is evaluated and compared based on an experimental dataset. Our experimental results show that the approach is feasible, performs well, and provides a more effective way to recommend Q&A documents. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113088	10.1016/j.eswa.2019.113088													
J								M(3)DNet: A manifold-based discriminant feature learning network for hyperspectral imagery	EXPERT SYSTEMS WITH APPLICATIONS										Hyperspectral imagery; Feature extraction; Manifold learning; Graph embedding; Adaptive optimization	FEATURE-EXTRACTION; NEURAL-NETWORK; FEATURE FUSION; CLASSIFICATION; IMPLEMENTATION; FRAMEWORK	Feature extraction (FE) is an effective method for learning discriminant features from hyperspectral image (HSI). Recently, graph embedding (GE) framework has been widely applied in FE of HSI data. GE unifies many classical FE methods and explores the low-dimensional embedding of high-dimensional data by a projection matrix generated from undirected weighted graphs. However, GE is unable to adaptively optimize projection matrix due to the absence of an iterative strategy in a single mapping process. To address this issue, a unified optimization method termed manifold-based maximization margin discriminant network (M(3)DNet) was proposed to improve the performance of traditional FE methods. In M(3)DNet, an initial projection matrix is obtained from original FE method, and then a maximal manifold margin criterion ((MC)-C-3) is proposed to maximize the margins among different classes, which enhances the discriminative ability of embedding features. After that, an iterative strategy is designed to optimize the projection matrix. Experiments on real-world HSI data sets indicate that the proposed M(3)DNet performs significantly better than some state-of-the-art methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113089	10.1016/j.eswa.2019.113089													
J								Influential spreaders identification in complex networks with improved k-shell hybrid method	EXPERT SYSTEMS WITH APPLICATIONS										Influential spreader identification; Centrality measures; K-shell hybrid; Improved k-shell hybrid; Kendall rank correlation	SOCIAL NETWORKS; INFLUENCE MAXIMIZATION; NODE RANKING; DYNAMICS; MODEL; EMERGENCE; ALGORITHM; COMMUNITY; INDEX; USERS	Identifying influential spreaders in a complex network has practical and theoretical significance. In applications such as disease spreading, virus infection in computer networks, viral marketing, immunization, rumor containment, among others, the main strategy is to identify the influential nodes in the network. Hence many different centrality measures evolved to identify central nodes in a complex network. The degree centrality is the most simple and easy to compute whereas closeness and betweenness centrality are complex and more time-consuming. The k-shell centrality has the problem of placing too many nodes in a single shell. Over the time many improvements over k-shell have been proposed with pros and cons. The k-shell hybrid (ksh) method has been recently proposed with promising results but with a free parameter that is set empirically which may cause some constraints to the performance of the method. This paper presents an improvement of the ksh method by providing a mathematical model for the free parameter based on standard network parameters. Experiments on real and artificially generated networks show that the proposed method outperforms the ksh method and most of the state-of-the-art node indexing methods. It has a better performance in terms of ranking performance as measured by the Kendall's rank correlation, and in terms of ranking efficiency as measured by the monotonicity value. Due to the absence of any empirically set free parameter, no time-consuming preprocessing is required for optimal parameter value selection prior to actual ranking of nodes in a large network. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113092	10.1016/j.eswa.2019.113092													
J								The bi-objective periodic closed loop network design problem	EXPERT SYSTEMS WITH APPLICATIONS										Network design; Closed loop supply chain; Periodic location-routing problem; Simultaneous pickup and delivery; Time window; Bi-objective	LOCATION-ROUTING PROBLEM; SIMULTANEOUS PICKUP; EVOLUTIONARY ALGORITHM; HEURISTIC ALGORITHM; GENETIC ALGORITHM; NSGA-II; HYBRID; DELIVERY; SEARCH; MODEL	Reverse supply chains are becoming a crucial part of retail supply chains given the recent reforms in the consumers' rights and the regulations by governments. This has motivated companies around the world to adopt zero-landfill goals and move towards circular economy to retain the product's value during its whole life cycle. However, designing an efficient closed loop supply chain is a challenging undertaking as it presents a set of unique challenges, mainly owing to the need to handle pickups and deliveries at the same time and the necessity to meet the customer requirements within a certain time limit. In this paper, we model this problem as a bi-objective periodic location routing problem with simultaneous pickup and delivery as well as time windows and examine the performance of two procedures, namely NSGA-II and NRGA, to solve it. The goal is to find the best locations for a set of depots, allocation of customers to these depots, allocation of customers to service days and the optimal routes to be taken by a set of homogeneous vehicles to minimise the total cost and to minimise the overall violation from the customers' defined time limits. Our results show that while there is not a significant difference between the two algorithms in terms of diversity and number of solutions generated, NSGA-II outperforms NRGA when it comes to spacing and runtime. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113068	10.1016/j.eswa.2019.113068													
J								Distributed version of hybrid swarm intelligence-Nelder Mead algorithm for DOA estimation in WSN	EXPERT SYSTEMS WITH APPLICATIONS										Swarm intelligence; Grey wolf optimization; Nelder-Mead simplex search method; Distributed DOA estimation; Wireless sensor networks	GENETIC ALGORITHM; SIMPLEX SEARCH; OPTIMIZATION; LOCALIZATION	Distributed direction of arrival (DOA) estimation based on maximum likelihood (ML) is an energy-efficient source localization technique that is vital to systems such as wireless sensor networks (WSN). Due to the multimodal nature of the ML function, the distributed approach uses swarm intelligence (SI) algorithms. However, this approach has slow convergence and incurs significant communication overhead for more than two sources. Hence, to obtain accurate DOA estimates with faster convergence, this paper proposes a distributed hybrid version of SI and Nelder-Mead (NM) simplex algorithm. In this approach, a modified evolutionary population dynamics based distributed Grey Wolf optimization SI algorithm is proposed to obtain initial DOA estimates. Then NM provides accurate DOA estimates with faster convergence and thereby reduces the communication overhead. Detailed simulation analysis shows that by using Quasi-Opposition based population initialization and sensor node degree combiner coefficients, the proposed distributed hybrid algorithm converges to theoretical Cramer-Rao lower bound with small communication overhead. Thus DOA estimation can be performed in an energy-efficient way by incorporating a computationally intelligent distributed hybrid approach. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113112	10.1016/j.eswa.2019.113112													
J								A Music Classification model based on metric learning applied to MP3 audio files	EXPERT SYSTEMS WITH APPLICATIONS										Music similarity; Metric learning; Feature extraction; Mel frequency cepstral coefficient; Principal components analysis	GENRE	The development of models for learning music similarity from audio media files is an increasingly important task for the entertainment industry. This work proposes a novel music classification model based on metric learning whose main objective is to learn a personalized metric for each customer. The metric learning process considers the learning of a set of parameterized distances employing a structured prediction approach from a set of MP3 audio files containing several music genres according to the users taste. The structured prediction solution aims to maximize the separation margin between genre centroids and to minimize the overall intra-cluster distances. To extract the acoustic information we use the Mel-Frequency Cepstral Coecient (MFCC) and made a dimensionality reduction using Principal Components Analysis (PCA). We attest the model validity performing a set of experiments and comparing the training and testing results with baseline algorithms, such as K-means and Soft Margin Linear Support Vector Machine (SVM). Also, to prove the prediction capacity, we compare our results with two recent works with good prediction results on the GTZAN dataset. Experiments show promising results and encourage the future development of an online version of the learning model to be applied in streaming platforms. Published by Elsevier Ltd.																	0957-4174	1873-6793				APR 15	2020	144								113071	10.1016/j.eswa.2019.113071													
J								A new methodology for multi-period portfolio selection based on the risk measure of lower partial moments	EXPERT SYSTEMS WITH APPLICATIONS										Finance; Multi-period planning; Lower partial moment measure; Portfolio-driven method	OPTIMIZATION MODEL; SKEWNESS; STOCK; LPM	This study aims to optimize the multi-period investment portfolio model with lower partial moments (LPM) as a measure of risk under transaction costs constraint. A new method is used to calculate the LPM model. The meta-heuristic algorithm Non-dominated Sorting Genetic Algorithm II is used to solve the multi-period optimization problem. To show efficiency of the proposed method some quantitative performance measures such as skewness, conditional Sharpe ratio, modified Sharpe ratio and Jensen measure are used. The results show that in comparison to the regular method of computing LPM, the proposed method works better and improves the efficiency of portfolio optimization, especially in terms of the processing time. (C) 2019 Published by Elsevier Ltd.																	0957-4174	1873-6793				APR 15	2020	144								113032	10.1016/j.eswa.2019.113032													
J								Mathematically optimized, recursive prepartitioning strategies for k-anonymous microaggregation of large-scale datasets	EXPERT SYSTEMS WITH APPLICATIONS										Data privacy; Statistical disclosure control; k-anonymity; Microaggregation; Optimized prepartitioning; Large-scale datasets	DATA-ORIENTED MICROAGGREGATION; NUMERICAL DATA; T-CLOSENESS; PRIVACY; ALGORITHM; ANONYMIZATION	The technical contents of this work fall within the statistical disclosure control (SDC) field, which concerns the postprocessing of the demographic portion of the statistical results of surveys containing sensitive personal information, in order to effectively safeguard the anonymity of the participating respondents. A widely known technique to solve the problem of protecting the privacy of the respondents involved beyond the mere suppression of their identifiers is the k-anonymous microaggregation. Unfortunately, most microaggregation algorithms that produce competitively low levels of distortions exhibit a superlinear running time, typically scaling with the square of the number of records in the dataset. This work proposes and analyzes an optimized prepartitioning strategy to reduce significantly the running time for the k-anonymous microaggregation algorithm operating on large datasets, with mild loss in data utility with respect to that of MDAV, the underlying method. The optimization strategy is based on prepartitioning a dataset recursively until the desired k-anonymity parameter is achieved. Traditional microaggregation algorithms have quadratic computational complexity in the form Theta(n(2)). By using the proposed method and fixing the number of recurrent prepartitions we obtain subquadratic complexity in the form Theta(n(3/2)), Theta(n(4/3)), ..., depending on the number of prepartitions. Alternatively, fixing the ratio between the size of the microcell and the macrocell on each prepartition, quasilinear complexity in the form Theta(nlogn) is achieved. Our method is readily applicable to large-scale datasets with numerical demographic attributes. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113086	10.1016/j.eswa.2019.113086													
J								BPDET: An effective software bug prediction model using deep representation and ensemble learning techniques	EXPERT SYSTEMS WITH APPLICATIONS										Software bug prediction; Classification technique; Software metrics; Deep representation; Boosting; Staked denoising auto-encoder; Heterogeneous Ensemble learning technique	DEFECT PREDICTION; FAULT-PRONENESS; NEURAL-NETWORKS; EMPIRICAL-ANALYSIS; CLASSIFICATION; REGRESSION; NUMBER; FOREST; PHASE	In software fault prediction systems, there are many hindrances for detecting faulty modules, such as missing values or samples, data redundancy, irrelevance features, and correlation. Many researchers have built a software bug prediction (SBP) model, which classify faulty and non-faulty module which are associated with software metrics. Till now very few works has been done which addresses the class imbalance problem in SBP. The main objective of this paper is to reveal the favorable result by feature selection and machine learning methods to detect defective and non-defective software modules. We propose a rudimentary classification based framework Bug Prediction using Deep representation and Ensemble learning (BPDET) techniques for SBP. It combinedly applies by ensemble learning (EL) and deep representation(DR). The software metrics which are used for SBP are mostly conventional. Staked denoising auto-encoder (SDA) is used for the deep representation of software metrics, which is a robust feature learning method. Propose model is mainly divided into two stages: deep learning stage and two layers of EL stage (TEL). The extraction of the feature from SDA in the very first step of the model then applied TEL in the second stage. TEL is also dealing with the class imbalance problem. The experiment mainly performed NASA (12) datasets, to reveal the efficiency of DR, SDA, and TEL. The performance is analyzed in terms of Mathew co-relation coefficient (MCC), the area under the curve (AUC), precision-recall area (PRC), F-measure and Time. Out of 12 dataset MCC values over 11 datasets, ROC values over 6 datasets, PRC values overall 12 datasets and F-measure over 8 datasets surpass the existing state of the art bug prediction methods. We have tested BPDET using Wilcoxon rank sum test which rejects the null hypothesis at alpha = 0.025. We have also tested the stability of the model over 5, 8, 10, 12, and 15 fold cross-validation and got similar results. Finally, we conclude that BPDET is a stable and outperformed on most of the datasets compared with EL and another state of the art techniques. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113085	10.1016/j.eswa.2019.113085													
J								Globally-biased BIRECT algorithm with local accelerators for expensive global optimization	EXPERT SYSTEMS WITH APPLICATIONS										Nonlinear global optimization; DIRECT-Type algorithms; BIRECT Algorithm; Hybrid optimization algorithms; Nonlinear regression	LIPSCHITZIAN OPTIMIZATION; SOFTWARE; SET; PARTITIONS; RECTANGLES; STRATEGIES; SELECTION; DESIGN; SCHEME	In this paper, black-box global optimization problem with expensive function evaluations is considered. This problem is challenging for numerical methods due to the practical limits on computational budget often required by intelligent systems. For its efficient solution, a new DIRECT-type hybrid technique is proposed. The new algorithm incorporates a novel sampling on diagonals and bisection strategy (instead of a trisection which is commonly used in the existing DIRECT-type algorithms), embedded into the globally-biased framework, and enriched with three different local minimization strategies. The numerical results on a test set of almost 900 problems from the literature and on a real-life application regarding nonlinear regression show that the new approach effectively addresses well-known DIRECT weaknesses, has beneficial effects on the overall performance, and on average, gives significantly better results compared to several DIRECT-type methods widely used in decision-making expert systems. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113052	10.1016/j.eswa.2019.113052													
J								Transform domain representation-driven convolutional neural networks for skin lesion segmentation	EXPERT SYSTEMS WITH APPLICATIONS										Convolutional neural network; Dermoscopic features; Melanoma; Skin lesion segmentation; Transform domain	IMAGE SEGMENTATION; DERMOSCOPY IMAGES; DIAGNOSIS; ALGORITHM; MODEL	Automated diagnosis systems provide a huge improvement in early detection of skin cancer, and consequently, contribute to successful treatment. Recent research on convolutional neural network has achieved enormous success in segmentation and object detection tasks. However, these networks require large amount of data that is a big challenge in medical domain where often have insufficient data and even a pretrained model on medical images can be hardly found. Lesion segmentation as the initial step of skin cancer analysis remains a challenging issue since datasets are small and include a variety of images in terms of light, color, scale, and marks which have led researchers to use extensive augmentation and preprocessing techniques or fine tuning the network with a pretrained model on irrelevant images. A segmentation model based on convolutional neural networks is proposed in this study for the tasks of skin lesion segmentation and dermoscopic feature segmentation. The network is trained from scratch and despite the small size of datasets neither excessive data augmentation nor any preprocessing to remove artifacts or enhance the images are applied. Alternatively, we investigated incorporating image representations of the transform domain to the convolutional neural network and compared to a model with more convolutional layers that resulted in 6% higher Jaccard index and has shorter training time. The model improved by applying CIELAB color space and the performance of the final proposed architecture is evaluated on publicly available datasets from ISBI challenges in 2016 and 2017. The proposed model has resulted in an improvement of as much as 7% for the segmentation metrics and 17% for the feature segmentation, which demonstrates the robustness of this unique hybrid framework and its future applications as well as further improvement. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113129	10.1016/j.eswa.2019.113129													
J								An expert system gap analysis and empirical triangulation of individual differences, interventions, and information technology applications in alertness of railroad workers	EXPERT SYSTEMS WITH APPLICATIONS										Sleep deprivation; Fatigue; Stress; Expert system; Alertness; Empirical analysis	SLEEP-DEPRIVATION; COGNITIVE PERFORMANCE; PERCEIVED USEFULNESS; DECISION-MAKING; USER ACCEPTANCE; 24 H; EASE; IMPAIRMENT; WAKING; TRAIT	In this abstract we would like to provide some exciting concrete information including the article's main impact and significance on expert and intelligent systems. The main impact is that the PTC expert intelligent system fills in the gaps between the human and software decision making processes. This gap analysis is analyzed via empirical triangulation of rail worker data collected from its groups, individuals and the rail industry itself. We utilize an expert intelligent system PTC information technology application to both measure and to improve the alertness of the groups and workers in order to improve the overall safety of the railways through reduced human errors and failures to prevent accidents. Many individual differences in alertness among military, railroad, and other industry workers stem from a lack of sufficient sleep. This continues to be a concern in the railroad industry, even with the implementation of positive train control (PTC) expert system technology. Information technology aids such as PTC cannot prevent all accidents, and errors and failures with PTC may occur. Furthermore, drug interventions are a short-term solution for improving alertness. This study investigated the effect of sleep deprivation on the alertness of railroad signalmen at work, individual differences in alertness, and the information technology available to improve alertness. We investigated various information and communication technology control systems that can be used to maintain operational safety in the railroad industry in the face of incompatible circadian rhythms due to irregular hours, weekend work, and night operations. To fully explain individual differences after the adoption of technology, our approach posits the necessary parameters that one must consider for reason-oriented action, sequential updating, feedback, and technology acceptance in a unified model. This triangulation can help manage workers by efficiently increasing their productivity and improving their health. In our analysis we used R statistical software and Tableau. To test our theory, we issued an Apple watch to a locomotive engineer. The perceived usefulness, perceived ease of use, and actual use he reported led to an analysis of his sleep patterns that eventually ended in his adoption of a sleep apnea device and an improvement in his alertness and effectiveness. His adoption of the technology also resulted in a decrease in his use of chemical interventions to increase his alertness. Our model shows that the alertness of signalmen can be predicted. Therefore, we recommend that the alertness of all railroad workers be predicted given the safety limitations of PTC. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113081	10.1016/j.eswa.2019.113081													
J								An explainable AI decision-support-system to automate loan underwriting	EXPERT SYSTEMS WITH APPLICATIONS										Explainable artificial intelligence; Interpretable machine learning; Loan underwriting; Evidential reasoning; Belief-rule-base; Automated decision making	KNOWLEDGE ACQUISITION; NEURAL-NETWORKS; VECTOR MACHINE; CREDIT; ENSEMBLE; MODEL; CLASSIFIERS; PERFORMANCE	Widespread adoption of automated decision making by artificial intelligence (AI) is witnessed due to specular advances in computation power and improvements in optimization algorithms especially in machine learning (ML). Complex ML models provide good prediction accuracy; however, the opacity of ML models does not provide sufficient assurance for their adoption in the automation of lending decisions. This paper presents an explainable AI decision-support-system to automate the loan underwriting process by belief-rule-base (BRB). This system can accommodate human knowledge and can also learn from historical data by supervised learning. The hierarchical structure of BRB can accommodates factual and heuristic rules. The system can explain the chain of events leading to a decision for a loan application by the importance of an activated rule and the contribution of antecedent attributes in the rule. A business case study on automation of mortgage underwriting is demonstrated to show that the BRB system can provide a good trade-off between accuracy and explainability. The textual explanation produced by the activation of rules could be used as a reason for denial of a loan. The decision-making process for an application can be comprehended by the significance of rules in providing the decision and contribution of its antecedent attributes. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113100	10.1016/j.eswa.2019.113100													
J								Geometric probabilistic evolutionary algorithm	EXPERT SYSTEMS WITH APPLICATIONS										Real parameter optimisation; Spherical inversion; Evolutionary algorithm; Multivariate distribution	OPTIMIZATION	In this paper we introduce a crossover operator and a mutation operator, called Bernoulli Reflection Search Operator (BRSO) and Cauchy Distributed Inversion Search Operator (CDISO) respectively, in order to define the search mechanism of a new evolutionary algorithm for global continuous optimisation, namely the Geometric Probabilistic Evolutionary Algorithm (GPEA). Both operators have been motivated by geometric transformations, namely inversions with respect to hyperspheres and reflections with respect to a hyperplanes, but are implemented stochastically. The design of the new operators follows statistical analyses of the search mechanisms (Inversion Search Operator (ISO) and Reflection Search Operator (RSO)) of the Spherical Evolutionary Algorithm (SEA). From the statistical analyses, we concluded that the non-linearity of the ISO can be imitated stochastically, avoiding the calculation of several parameters such as the radius of hypersphere and acceptable regions of application. In addition, a new mutation based on a normal distribution is included in CDISO in order to guide the exploration. On the other hand, the BRSO imitates the mutation of individuals using reflections with respect to hyperplanes and complements the CDISO. In order to evaluate the proposed method, we use the benchmark functions of the special session on real-parameter optimisation of the CEC 2013 competition. We compare GPEA against 12 state-of-the-art methods, and present a statistical analysis using the Wilcoxon signed rank and the Friedman tests. According to the numerical experiments, GPEA exhibits a competitive performance against a variety of sophisticated contemporary algorithms, particularly in higher dimensions. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113080	10.1016/j.eswa.2019.113080													
J								The solution of the concurrent layout scheduling problem in the job-shop environment through a local neighborhood search algorithm	EXPERT SYSTEMS WITH APPLICATIONS										Job-shop scheduling problem; Facility layout problem; Local neighborhood search algorithm	GENETIC ALGORITHM; OPTIMIZATION APPROACH; INJECTION PROCESS; OPTIMAL-DESIGN; TABOO SEARCH; HEURISTICS; SIMULATION; FACILITIES	The concurrent layout and scheduling problem is an extension of the well-known job-shop scheduling problem with transport delays, in which, in addition to the decisions taken in the classic problem, the location of machines must be selected from a set of possible sites. The aim of this study was to solve this problem by using a local neighborhood search algorithm (LNSA). This algorithm used a random local neighborhood, where neighbors were produced by simple operators commonly used in metaheuristics for scheduling problems. The solution coding used in the LNSA enabled all the calculated solutions to be valid schedules for the problem. The findings of the study were compared with those obtained by Ranjbar, who proposed the benchmark problems. Our method obtained a minor average relative percentage compared to the best-known results. Furthermore, it achieved a smaller makespan in the problems with higher computational complexity, which may aid companies that require customized manufacturing. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113096	10.1016/j.eswa.2019.113096													
J								Unusual customer response identification and visualization based on text mining and anomaly detection	EXPERT SYSTEMS WITH APPLICATIONS										Voice of customers; Keyword network; Local outlier factor; TF-IDF	VOICE; SATISFACTION; DISCOVERY; LOF	The Vehicle Dependability Study (VDS) is a survey study on customer satisfaction for vehicles that have been sold for three years. VDS data analytics plays an important role in the vehicle development process because it can contribute to enhancing the brand image and sales of an automobile company by properly reflecting customer requirements retrieved from the analysis results when developing the vehicle's next model. Conventional approaches to analyzing the voice of customers (VOC) data, such as VDS, have focused on finding the mainstream of customer responses, many of which are already known to the enterprise. However, detecting and visualizing notable opinions from a large amount of VOC data are important in responding to customer complaints. In this study, we propose a framework for identifying unusual but significant customer responses and frequently used words therein based on distributed document representation, local outlier factor, and TF-IDF methods. We also propose a procedure that can provide useful information to vehicle engineers by visualizing the main results of the framework. This unusual customer response detection and visualization framework can accelerate the efficiency and effectiveness of many VOC data analytics. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113111	10.1016/j.eswa.2019.113111													
J								A new approach using fuzzy DEA models to reduce search space and eliminate replications in simulation optimization problems	EXPERT SYSTEMS WITH APPLICATIONS										Simulation optimization; Discrete event simulation; Fuzzy-Data Envelopment Analysis	DATA ENVELOPMENT ANALYSIS; EFFICIENCY; RANKING	This article proposes a new combination of methods to increase optimization simulation efficiency and reliability, utilizing orthogonal arrays, fuzzy-data envelopment analysis (FDEA) with linear membership function, and discrete event simulation (DES). Considering a simulation optimization problem, experimental matrices are generated using orthogonal arrays and which simulation runs (scenarios) will be executed are defined, followed by FDEA to analyze and rank the scenarios in terms of their efficiency (considering occurrence of uncertainty). In this way, it is possible to reduce the search space of scenarios to be simulated, and avoid the need for replications in DES, without impairing the quality of the final solution. Six real cases that were solved by the proposed approach are presented. In order to highlight the efficiency of the proposed method, in Cases 5 and 6, all viable solutions of each of these problems were tested, ie, 100% of the search space was analyzed, and it was found that the solution obtained by the new method was statistically equal to the overall optimal solution. Note that for the other real cases solved, the solutions obtained by the proposed method were also statistically equal to those obtained from the original search space, and that analyzing 100% of the viable solutions space would be computationally impossible or impractical. These results confirmed the reliability and applicability of the proposed method, since it enabled a significant reduction in the search space for the simulation application compared to conventional simulation optimization techniques. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113137	10.1016/j.eswa.2019.113137													
J								Privacy-preserving in association rule mining using an improved discrete binary artificial bee colony	EXPERT SYSTEMS WITH APPLICATIONS										Data mining; Privacy preserving in data mining; Association rule mining; Association rule hiding; Artificial bee colony	SENSITIVE KNOWLEDGE; ALGORITHM; SANITIZATION; PRESERVATION; OPTIMIZATION; PATTERNS	Association Rule Hiding (ARH) is the process of protecting sensitive knowledge using data transformation. Although there are some evolutionary-based ARH algorithms, they mostly focus on the itemset hiding instead of the rule hiding. Besides, unstable convergence to the global optimum solution and designing long solutions make them inappropriate in reducing side effects. They use the basic versions of evolutionary approaches, resulting in inappropriate performance in ARH domain where the search space is large and the algorithms easily get trapped in local optima. To deal with these problems, we propose a new rule hiding algorithm based on a binary Artificial Bee Colony (ABC) approach which has good exploration. However, we improve the binary ABC algorithm to enhance its poor exploitation by designing a new neighborhood generation mechanism to balance between exploration and exploitation. We called this algorithm Improved Binary ABC (IBABC). IBABC approach is coupled with our proposed rule hiding algorithm, called ABC4ARH, to select sensitive transactions for modification. To choose victim items, ABC4ARH formulates a heuristic. The performance of ABC4ARH algorithm on the side effects is demonstrated using extensive experiments conducted on five real datasets. Furthermore, the effectiveness of IBABC is verified using the uncapacitated facility location problem and 0-1 knapsack problem. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113097	10.1016/j.eswa.2019.113097													
J								A batch informed sampling-based algorithm for fast anytime asymptotically-optimal motion planning in cluttered environments	EXPERT SYSTEMS WITH APPLICATIONS										Motion planning; Anytime algorithm; Asymptotic optimality; Optimal path planning	RRT-ASTERISK	Practical applications favor anytime asymptotically-optimal algorithms that find and improve an initial solution toward the optimal solution as quickly as possible due to the algorithms may be terminated at any time. We present Batch-to-batch Informed Fast Marching Tree (BBI-FMT*), an anytime asymptotically-optimal sampling-based algorithm that is designed for solving complex motion planning problems. The proposed algorithm has the ability to fast find an initial low-cost solution by the batch sampling-based incremental search and the "lazy" optimal search, then it employs the batch informed sampling-based incremental search and the anytime optimal search to quickly improve the tree and achieve the optimal solution. The proposed anytime optimal search strategy integrates the "lazy" and "non-lazy" optimal search to efficiently improve the tree to the minimum-cost spanning tree in cluttered environments. This paper theoretically analyzes the proposed algorithm in depth and evaluates it by numerical experiments under a few challenging scenarios. The experimental results show that BBI-FMT* outperforms the state-of-the-art algorithms in the self-adaptability, robustness, convergence rate, and success rate of the planning. The proposed algorithm can be widely applied to intelligent robots with expert systems to improve the efficiency and stability of the motion planning and navigation modules which are the core modules in the expert systems. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113124	10.1016/j.eswa.2019.113124													
J								Drawing openness to experience from user generated contents: An interpretable data-driven topic modeling approach	EXPERT SYSTEMS WITH APPLICATIONS										Openness to experience; Interpretability; Topic modeling; Maximum-A-Posteriori estimation; Data-driven	SENTIMENT ANALYSIS; ONLINE REVIEWS; PERSONALITY; PREFERENCES; IMPACT	Openness to experience, one of the essential individual characteristics, is of great theoretical and practical value in psychological and behavioral domains. Although typical machine learning methods can be utilized to extract individuals' openness to experience from the large-scale textual data like the unprecedented massive user generated contents (UGCs), they are often regarded as "black boxes" because they are unable to provide knowledge about the influential factors of openness to experience. This is of no help for us to investigate why a particular level of openness to experience is predicted for an individual. In addition, high dimensionality and sparseness of textual data impairs the performance of the typical machine learning method in extracting individuals' characteristics. In this study, we propose an interpretable data-driven mixture method for qualified modeling and predicting individuals' openness to experience. The proposed method extends the latent Dirichlet allocation (LDA) to overcome the problem of high dimensionality and sparseness in modeling the textual data, and can effectively extract two influential variables, namely, the topic preference and the expressed emotional intensity, to make an accurate prediction and to help us fully understand individuals' openness to experience lurking in the textual data. Experimental results indicate the effectiveness of the proposed method in drawing individuals' openness to experience, and also validate the predictive ability of topic preference and expressed emotional intensity which are indicated in psychological literature to be influential factors of openness to experience. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				APR 15	2020	144								113073	10.1016/j.eswa.2019.113073													
J								A team of pursuit learning automata for solving deterministic optimization problems	APPLIED INTELLIGENCE										Distributed learning; Learning automata; Deterministic optimization	LOCAL SEARCH; ALGORITHMS; SIMULATION; SCHEMES; SET	Learning Automata (LA) is a popular decision-making mechanism to "determine the optimal action out of a set of allowable actions" [1]. The distinguishing characteristic of automata-based learning is that the search for an optimal parameter (or decision) is conducted in the space of probability distributions defined over the parameter space, rather than in the parameter space itself [2]. In this paper, we propose a novel LA paradigm that can solve a large class of deterministic optimization problems. Although many LA algorithms have been devised in the literature, those LA schemes are not able to solve deterministic optimization problems as they suppose that the environment is stochastic. In this paper, our proposed scheme can be seen as the counterpart of the family of pursuit LA developed for stochastic environments [3]. While classical pursuit LAs can pursue the action with the highest reward estimate, our pursuit LA rather pursues the collection of actions that yield the highest performance by invoking a team of LA. The theoretical analysis of the pursuit scheme does not follow classical LA proofs, and can pave the way towards more schemes where LA can be applied to solve deterministic optimization problems. Furthermore, we analyze the scheme under both a constant learning parameter and a time-decaying learning parameter. We provide some experimental results that show how our Pursuit-LA scheme can be used to solve the Maximum Satisfiability (Max-SAT) problem. To avoid premature convergence and better explore the search space, we enhance our scheme with the concept of artificial barriers recently introduced in [4]. Interestingly, although our scheme is simple by design, we observe that it performs well compared to sophisticated state-of-the-art approaches.																	0924-669X	1573-7497				SEP	2020	50	9					2916	2931		10.1007/s10489-020-01657-9		APR 2020											
J								AUV Docking Method in a Confined Reservoir with Good Visibility	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Autonomous underwater vehicle (AUV); Docking technology; Data fusion; Visual guidance	UNDERWATER DOCKING; DESIGN	Underwater docking technology enables autonomous underwater vehicles (AUVs) to execute long-term observation missions by periodically recovering and recharging AUVs. The conventional AUV homing and docking operations utilize acoustic and optical sensors at different ranges relative to the docking station. However, this method cannot perform perfectly in confined water regions because of the acoustic reflection and multipath effect. Thus, this paper proposes a novel navigation system, which fuses downward-looking visual odometry and model-based velocity for homing, and recognizes and tracks the light marker for terminal docking, in order to overcome the defects of the conventional navigation method. The reservoir experiment result verifies the effectiveness of the proposed method and shows good potential to extended applications in underwater routine cruise.																	0921-0296	1573-0409				OCT	2020	100	1					349	361		10.1007/s10846-020-01175-3		APR 2020											
J								Material hermeneutics and Heelan's philosophy of technoscience	AI & SOCIETY										Wittgenstein; Electron microscopy; Galileo's telescopes; Fleck; Heelan; Feyerabend; Latour; Pasteur		This essay raises the question of material hermeneutics in Heelan's philosophy of techno-science. For Heelan, a continental philosophy of technoscience, referring to Husserl and Heidegger and especially to Merleau-Ponty, features hermeneutic contexts of mathematics and measurement as well as laboratory observation, including what the later Heelan spoke of as "portable laboratories," for the sake of objectivity and "meaning making." For Paul Feyerabend, this material practice corresponded to the use of both techniques of observation and instrumentation, and not less "propaganda" in the case of Galileo which practice for Heelan included the ontological status of measures and numbers as well as apprenticeship in what Heelan called "contingent local practical cultural milieus." The essay includes a discussion of Heidegger on mathematics and Bruno Latour on pasteurization.																	0951-5666	1435-5655															10.1007/s00146-020-00963-7		APR 2020											
J								Controller Optimization for Multirate Systems Based on Reinforcement Learning	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Multirate system; reinforcement learning; policy iteration; optimal control; controller optimization	SAMPLED-DATA SYSTEMS; H-INFINITY CONTROL; FEEDBACK-CONTROL; STABILIZATION; DESIGN	The goal of this paper is to design a model-free optimal controller for the multirate system based on reinforcement learning. Sampled-data control systems are widely used in the industrial production process and multirate sampling has attracted much attention in the study of the sampled-data control theory. In this paper, we assume the sampling periods for state variables are different from periods for system inputs. Under this condition, we can obtain an equivalent discrete-time system using the lifting technique. Then, we provide an algorithm to solve the linear quadratic regulator (LQR) control problem of multirate systems with the utilization of matrix substitutions. Based on a reinforcement learning method, we use online policy iteration and off-policy algorithms to optimize the controller for multirate systems. By using the least squares method, we convert the off-policy algorithm into a model-free reinforcement learning algorithm, which only requires the input and output data of the system. Finally, we use an example to illustrate the applicability and efficiency of the model-free algorithm above mentioned.																	1476-8186	1751-8520				JUN	2020	17	3					417	427		10.1007/s11633-020-1229-0		APR 2020											
J								A Survey on Modelling and Compensation for Hysteresis in High Speed Nanopositioning of AFMs: Observation and Future Recommendation	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Hysteresis; nonlinearities; piezoelectric actuator; smart actuator; atomic force microscope	RATE-DEPENDENT HYSTERESIS; PRANDTL-ISHLINSKII MODEL; DYNAMIC SURFACE CONTROL; ROBUST ADAPTIVE-CONTROL; MOTION TRACKING CONTROL; NEURAL-NETWORK CONTROL; PIEZOELECTRIC ACTUATORS; NONLINEAR-SYSTEMS; INVERSE-FEEDFORWARD; NEGATIVE-IMAGINARY	This paper surveys the recent advances on the modeling and control of hysteresis of piezoelectric actuators (PTAs) in the context of high precision applications of atomic force microscopes (AFMs). The current states, findings, and outcomes on hysteresis modeling and control in terms of achievable bandwidth and accuracy are discussed in detailed. Future challenges and the scope of possible research are presented to pave the way to video rate atomic force microscopy.																	1476-8186	1751-8520				AUG	2020	17	4					479	501		10.1007/s11633-020-1225-4		APR 2020											
J								Parabolic Trough Collector and Central Receiver Coupled with Fresnel Lens: Experimental Tests	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Solar collector; parabolic trough collector; Fresnel lens; thermal conductivity; solar power	SOLAR; PERFORMANCE; SIMULATION; SYSTEMS	Renewable energies have a high impact on power energy production and reduction of environmental pollution worldwide, so high efforts have been made to improve renewable technologies and research about them. This paper presents the thermal performance results obtained by simulation and experimental tests of a parabolic trough collector with central receiver coupled to Fresnel lens, under different configurations on the pipe. The simulation method was computational fluid dynamics (CFD) analysis in Solid Works (R) soft-ware tool, which works with Naiver-Stokes equations to converge on a solution. Experimental tests were formed with all configurations proposed and three observations for each one, a total of 12 observations were performed in all research. As a result, the best thermal performance in simulation was achieved with the Fresnel lens and black pipe collector, with a maximum temperature of 116 degrees C under 1 000 W/m(2) radiation, the same system achieved in experimental tests a maximum temperature of 96 degrees C with a radiation of 983 W/m(2).																	1476-8186	1751-8520				AUG	2020	17	4					572	587		10.1007/s11633-019-1220-9		APR 2020											
J								Novel Non-monotonic Lyapunov-Krasovskii Based Stability Analysis and Stabilization of Discrete State-delay System	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Lyapunov-Krasovskii functional; discrete state-delay systems; non-monotonic Lyapunov function; robust stability; stabilization	H-INFINITY-CONTROL; ROBUST STABILITY; TIME-DELAY; FUZZY-SYSTEMS; CONTROLLER; DESIGN; INEQUALITY; CRITERIA	This paper proposes a novel less-conservative non-monotonic Lyapunov-Krasovskii stability approach for stability analysis of discrete time-delay systems. In this method, monotonically decreasing requirements of the Lyapunov-Krasovskii method are replaced with non-monotonic ones. The Lyapunov-Krasovskii functional is allowed to increase in some steps, but the overall trend should be decreasing. The model of practical systems used for stability analysis usually contain uncertainty. Therefore, firstly a non-monotonic stability condition is derived for certain discrete time-delay systems, then robust non-monotonic stability conditions are proposed for uncertain systems. Finally, a novel stabilization algorithm is derived based on the introduced non-monotonic stability condition. The Lyapunov-Krasovskii functional and the controller are obtained by solving a set of linear matrix inequalities (LMI) or iterative LMI based nonlinear minimization. The proposed theorems are first evaluated by some numerical examples, and then by simulation and implementation on the pH neutralizing process plant.																	1476-8186	1751-8520				OCT	2020	17	5					713	732		10.1007/s11633-020-1222-7		APR 2020											
J								Intelligent traffic monitoring and traffic diagnosis analysis based on neural network algorithm	NEURAL COMPUTING & APPLICATIONS										Neural network algorithm; Intelligent transportation; Intelligent diagnosis; Intelligent detection; Abnormal recognition	IDENTIFICATION; SENSOR	Traffic sign recognition and lane detection play an important role in traffic flow planning, avoiding traffic accidents, and alleviating traffic chaos. At present, the traffic intelligent recognition rate still needs to be improved. In view of this, based on the neural network algorithm, this study constructs an intelligent transportation system based on neural network algorithm, and combines machine vision technology to carry out intelligent monitoring and intelligent diagnosis of traffic system. In addition, this study discusses in detail the core of the monitoring system: multi-target tracking algorithm, and introduces the complete implementation process and details of the system, and highlights the implementation and tracking effect of the multi-target tracker. Finally, this study uses case identification to analyze the effectiveness of the algorithm proposed by this paper. The research results show that the proposed method has certain practical effects and can be used as a reference for subsequent system construction.																	0941-0643	1433-3058															10.1007/s00521-020-04899-3		APR 2020											
J								The impact of alphabet size on pattern complexity of maxmin-omega cellular automata	NATURAL COMPUTING										Max-plus algebra; Cellular automaton; Alphabet; Entropy; Complexity	MODEL; ROBUSTNESS	We present an analysis of an additive cellular automaton (CA) under asynchronous dynamics. The asynchronous scheme is maxmin-x, a deterministic system, introduced in our previous work with a binary alphabet. Extending this work, we study the impact of a larger alphabet, which also allows a meaningful inference of the behaviour of the resultant CA from the asymptotic behaviour of the maxmin-x update system. Far from being a straightforward positive correlation between complexity and alphabet size, we show that there is a region of x and alphabet size where complexity of CA is maximal. Thus, despite employing a fixed CA rule, the complexity of this CA can be controlled by x and alphabet size. The main message is that the effect of maxmin-x updating on the state of a network can be well understood, especially if the state alphabet is counter-intuitively large.																	1567-7818	1572-9796				JUN	2020	19	2			SI		273	285		10.1007/s11047-020-09787-2		APR 2020											
J								Distance dynamics based overlapping semantic community detection for node-attributed networks	COMPUTATIONAL INTELLIGENCE										community detection; distance dynamics; interaction model; node-attributed network; semantic community		In recent years, due to the rise of social, biological, and other rich content graphs, several novel community detection methods using structure and node attributes have been proposed. Moreover, nodes in a network are naturally characterized by multiple community memberships and there is growing interest in overlapping community detection algorithms. In this paper, we design a weighted vertex interaction model based on distance dynamics to divide the network, furthermore, we propose a distance Dynamics-based Overlapping Semantic Community detection algorithm(DOSC) for node-attribute networks. The method is divided into three phases: Firstly, we detect local single-attribute subcommunities in each attribute-induced graph based on the weighted vertex interaction model. Then, a hypergraph is constructed by using the subcommunities obtained in the previous step. Finally, the weighted vertex interaction model is used in the hypergraph to get global semantic communities. Experimental results in real-world networks demonstrate that DOSC is a more effective semantic community detection method compared with state-of-the-art methods.																	0824-7935	1467-8640															10.1111/coin.12324		APR 2020											
J								TNAM: A tag-aware neural attention model for Top-N recommendation	NEUROCOMPUTING										Recommender systems; Tag information; Deep learning; Attention networks		Recent work shows that incorporating tag information to recommender systems is promising for improving the recommendation accuracy in social systems. However, existing approaches suffer from less reasonable assignment of tag weights when constructing the user profiles and item characteristics in real-world scenarios, resulting in decreased accuracy in making recommendations. The above issue is specifically summarized into two aspects: 1) the weight of a target item is mainly determined by number of one certain type of tags, and 2) users place equal focus on the same tag for different items. To tackle these problems, we propose a novel model named TNAM, a Tag-aware Neural Attention Model, which accurately captures users' special attention to tags of items. In the proposed model, we design a tag-based neural attention network by extracting potential tag information to overcome the difficulty of assigning tag weights for personalized users. We combine user-item interactions with tag information to map sparse data to dense vectors in higher-order space. In this way, TNAM acquires more interrelations between users and items to make recommendations more accurate. Extensive experiments of our model on three publicly implicit feedback datasets reveal significant improvements on the metrics of HR and NDCG in Top-N recommendation tasks over several state-of-the-art approaches. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				APR 14	2020	385						1	12		10.1016/j.neucom.2019.11.095													
J								Distributed set-membership filtering for nonlinear systems subject to round-robin protocol and stochastic communication protocol over sensor networks	NEUROCOMPUTING										Distributed set-membership filtering; Sensor networks; Nonlinear system; Round-Robin protocol; Stochastic communication protocol; Unknown but bounded noise	STATE ESTIMATION	In this paper, the distributed set-membership filtering problem is investigated for general nonlinear systems subject to Round-Robin and stochastic communication protocols over sensor networks. The considered disturbances are assumed to be unknown but bounded within certain known ellipsoidal regions. Two types of frequently used protocols, i.e., the Round-Robin and stochastic communication protocols are adopted separately to regulate the data transmission among sensor nodes. According to the Round-Robin protocol, each node is able to receive information from only one of its neighboring nodes in a circular order at each time step, whereas the stochastic communication protocol allows each node randomly receive the information sent from one of the neighboring nodes. By resorting to the recursive linear matrix inequalities approach, sufficient conditions are established for the existence of the desired distributed filter, guaranteeing that the state estimates are constrained within certain pre-determined ellipsoidal regions at each time step. Then two optimization problems aiming to minimize the ellipsoids (in the sense of matrix trace) are put forward to seek the locally optimal filtering performance. Finally, simulation examples are given to illustrate the effectiveness of the proposed algorithms. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						13	21		10.1016/j.neucom.2019.11.056													
J								HDS-SP: A novel descriptor for skeleton-based human action recognition	NEUROCOMPUTING										Human action recognition; Skeleton joints; Histogram; PSO	HISTOGRAM	3D skeleton-based human action recognition has attracted much attention due to a wide spectrum of promising applications in terms of depth sensors. This paper, based on the principle of the better viewpoint the better action recognition performance, devises a HDS-SP descriptor for skeleton-based human action, i.e., the histogram of distributed sectors based on their specific projections. The HDS-SP descriptor consists of both spatial and temporal information from specific viewpoint, in which projecting 3D trajectories on specific planes and creating reasonable histograms using proposed way are two primary contributions. Inspired by the nature of human action, the spatial information is incorporated into the histogram of the displacement of one joint between two successive frames voting for corresponding bins following weight-based rules over an undefined plane while the specific projection planes are optimized by both local search algorithm and Particle Swarm Optimization (PSO); the temporal information is captured by temporal hierarchical construction. The proposed method is evaluated in five widely researched datasets for skeleton-based human action recognition with results significantly outperforming the state-of-the-art. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						22	32		10.1016/j.neucom.2019.11.048													
J								Periodic event-triggered consensus of multi-agent systems under directed topology	NEUROCOMPUTING										Consensus; Multi-agent systems; Periodic sampling; Event-triggered control; Digraph; Delay	SYNCHRONIZATION; PROTOCOLS; NETWORKS	The event-triggered consensus problems of multi-agent systems without and with time delay under directed topology are investigated, respectively. The event-checkings are only implemented at periodic time instants. Under the designed consensus algorithm, the sampling period is permitted to be arbitrarily large. Another advantage of the designed consensus algorithm is that, for systems with time delay, consensus can be achieved for any finite delay only with the requirement that it is bounded by the sampling period. The case of strongly connected topology is first investigated. Then, the result is extended to the most general topology which only needs to contain a spanning tree. A novel method based on positive series is introduced to analyze the convergence of the closed-loop systems. A numerical example is provided to illustrate the effectiveness of the obtained theoretical results. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						33	41		10.1016/j.neucom.2019.12.081													
J								Containment control of linear discrete-time fractional-order multi-agent systems with time-delays	NEUROCOMPUTING										Containment control; Fractional order systems; Multi-agent systems; Discrete-time fractional order systems; Time delays	SUFFICIENT CONDITIONS; COOPERATIVE CONTROL; ALGORITHMS; STABILITY	This work concerns with containment control of discrete-time fractional-order multi-agent systems with time-delay. A directed fixed topology is considered for the agents. At first, necessary and sufficient conditions to ensure the asymptotic stability of a general linear discrete-time fractional-order system with input time-delay are extracted. Then, a multi-agent system that the dynamics of its agents are represented with discrete-time fractional-order integrators with input time-delay is considered. Now, the obtained stability conditions are employed to derive the required conditions in terms of the control gains to realize the containment control of the mentioned multi-agent system. Numerical simulations demonstrate that the positions of the followers tend to the convex hull constructed by the leaders. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						42	47		10.1016/j.neucom.2019.12.067													
J								Finite-time sliding mode control for networked singular Markovian jump systems with packet losses: A delay-fractioning scheme	NEUROCOMPUTING										Networked systems; Singular Markovian jump systems; Packet losses; Sliding mode control; Stochastic finite-time boundedness	RECURSIVE STATE ESTIMATION; VARYING NONLINEAR-SYSTEMS; H-INFINITY; STOCHASTIC-SYSTEMS; MISSING MEASUREMENTS; ACTUATOR SATURATION; DISSIPATIVE CONTROL; COMPLEX NETWORKS; FAULT ESTIMATION; NEURAL-NETWORKS	The paper is concerned with the finite time sliding mode control (SMC) problem for a class of discrete networked singular Markovian jump systems (SMJSs) subject to packet losses (PLs) via the delay-fractioning approach. A random variable obeying the Bernoulli distribution is used to depict the phenomenon of PLs, which is frequently encountered in the practical engineering especially during the networked transmission. A key issue to be discussed is how to guarantee both the singular stochastic finite-time boundedness (SSFTB) of the resultant sliding mode dynamic systems and the reachability of the pre-selected sliding surface. Accordingly, a sufficient condition is obtained to ensure the SSFTB of sliding motion based on delay-fractioning method and linear matrix inequality technique. Furthermore, a new SMC mechanism is proposed to compensate the influence induced by LPs and ensure the reachability condition of pre-selected sliding surface simultaneously. Finally, the effectiveness of the proposed SMC approach is tested by a simulation example. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						48	62		10.1016/j.neucom.2019.12.064													
J								High-order fuzzy clustering algorithm based on multikernel mean shift	NEUROCOMPUTING										Multikernel; Mean shift; High-order; Commensurability; Fuzzy clustering; Hyper-plane	KERNEL; SUBSPACE	This study proposes a method of constructing multikernel space to ensure the integrity of the original data in which the multikernel space aims to reduce the computational complexity of multidimensional data and is suitable for the processing of relational data. The high-dimensional samples of the original space are therefore mapped into a high-dimensional kernel feature space to obtain the inner product. However, when the dimensions of the feature space for multikernel is extremely high or even infinite, the inner product is difficult to calculate directly. To overcome these limitations, this study further proposes a high-order fuzzy clustering (HoFC) algorithm called multikernel mean shift (MKMS-HoFC), which incorporates mean shift based on multikernel space to divide the data and expand the original dimension into multiple new dimensions in the high-dimensional kernel feature space. The MKMS-HoFC initially maps the input points into a high-dimensional feature space of the multikernel and constructs a separating hyper-plane that maximizes the margin among multiple clusters in this space. The multikernel then finds the optimal hyper-plane by HoFC. This method iteratively searches for the densest regions of the sample points in the feature space and improves the clustering performance by using the multidimensional commensurability of HoFC. Real datasets are used to analyze the quality of clustering. Experimental results and comparisons demonstrate the excellent performances of MKMS-HoFC with its effectiveness in practice. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						63	79		10.1016/j.neucom.2019.12.030													
J								Randomized sketches for sparse additive models	NEUROCOMPUTING										Convergence rate; Kernel method; Random projection; Sparse additive models	DIVIDE-AND-CONQUER; VARIABLE SELECTION; ALGORITHMS; FRAMEWORK	Sparse additive models in the setting of reproducing kernel Hilbert spaces have theoretically optimal properties even in high dimensions. However, its computational cost is heavy, especially considering that it contains two regularization parameters associated with different norms. In the big data setting, fitting of the model becomes infeasible. As our first attempt to address this issue, we herein consider fixed-dimensional setting and propose a randomized sketches approach for sparse additive models. It is shown that the sketched estimator has the same optimal convergence rate as the standard estimator. Some Monte Carlo examples are presented to illustrate the performance of the estimators. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						80	87		10.1016/j.neucom.2019.12.012													
J								Multi-scale local LSSVM based spatiotemporal modeling and optimal control for the goethite process	NEUROCOMPUTING										Distributed parameter system; Iron removal process; Least squares; Multi-scale kernel learning; Spatiotemporal modeling; Support vector machine	MULTIOBJECTIVE OPTIMIZATION; IRON PRECIPITATION; KERNEL; REMOVAL; SERIES	The iron removal process by goethite is an important part of zinc hydrometallurgy. In existing works, the goethite process is often modeled as a lumped parameter system, where the spatial distribution information of reactants is not involved. In this paper, the spatiotemporal modeling of the goethite process and its optimal control problem are studied. To make the infinite-dimensional distributed parameter system easier to solve, space-time separation is adopted to transform it into a finite-dimensional system. Then, a multi-scale local least squares support vector machine is proposed to establish the temporal model. This method uses multi-scale kernel learning to deal with different trends of the process and establish a local model to track the state change of the system. Through space-time synthesis, the established spatiotemporal model can approximate the distributed parameter system of the goethite process. Moreover, an optimal control strategy based on the spatiotemporal model is designed to reduce the cost of oxygen and zinc oxide consumed in the process. Finally, simulation experiments on the goethite process demonstrate the effectiveness of the proposed modeling method and optimal control strategy. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						88	99		10.1016/j.neucom.2019.12.008													
J								Deep learning based software defect prediction	NEUROCOMPUTING										Software defect prediction; Deep learning; Software quality; Software metrics; Robustness evaluation	SUPPORT VECTOR REGRESSION; MEAN SQUARED ERROR; QUANTITATIVE-ANALYSIS; NEURAL-NETWORKS; MODELS; FAULTS	Software systems have become larger and more complex than ever. Such characteristics make it very challengeable to prevent software defects. Therefore, automatically predicting the number of defects in software modules is necessary and may help developers efficiently to allocate limited resources. Various approaches have been proposed to identify and fix such defects at minimal cost. However, the performance of these approaches require significant improvement. Therefore, in this paper, we propose a novel approach that leverages deep learning techniques to predict the number of defects in software systems. First, we preprocess a publicly available dataset, including log transformation and data normalization. Second, we perform data modeling to prepare the data input for the deep learning model. Third, we pass the modeled data to a specially designed deep neural network-based model to predict the number of defects. We also evaluate the proposed approach on two well-known datasets. The evaluation results illustrate that the proposed approach is accurate and can improve upon the state-of-the-art approaches. On average, the proposed method significantly reduces the mean square error by more than 14% and increases the squared correlation coefficient by more than 8%. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						100	110		10.1016/j.neucom.2019.11.067													
J								Deep discrete hashing with pairwise correlation learning	NEUROCOMPUTING										Image retrieval; Discrete hashing; Pairwise correlation learning	IMAGE RETRIEVAL; QUANTIZATION; CODES; NETWORK	Hashing technology plays an important role in large-scale visual search due to its low memory and fast retrieval speed. Most existing deep hashing approaches first leverage the continuous relaxation strategy to learn continuous approximate codes, and then transform them into discrete hash codes by separating quantization operations, which results in the suboptimal problem of hash codes and ultimately affects the performance of image retrieval. To solve this problem, we propose a novel deep discrete hashing approach with pairwise labels, namely Pairwise Correlation Discrete Hashing (PCDH), to leverage the pairwise correlation of deep features and semantic supervised information to directly guide discrete hashing codes learning. Firstly, we integrate discrete hash code learning and deep features learning in a unified network framework, which can utilize the semantic supervision to guide discrete hash codes learning. Secondly, we design a novel pairwise correlation constraint to perform pairwise correlation learning of deep features. Thirdly, we develop a novel pairwise construction module to mine good pairwise samples for discrete hash codes learning. Extensive experimental results show that the proposed PCDH approach achieves superior performance over other recent state-of-the-art hashing approaches. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						111	121		10.1016/j.neucom.2019.12.078													
J								Label embedded dictionary learning for image classification	NEUROCOMPUTING										Dictionary learning; Sparse representation; Label embedded dictionary learning; Image classification	CONSISTENT K-SVD; FACE RECOGNITION; SPARSE; SUPERRESOLUTION; REPRESENTATION; FUSION; MODELS; SET	Recently, label consistent k-svd (LC-KSVD) algorithm has been successfully applied in image classification. The objective function of LC-KSVD is consisted of reconstruction error, classification error and discriminative sparse codes error with l(0)-norm sparse regularization term. The l(0)-norm, however, leads to NP-hard problem. Despite some methods such as orthogonal matching pursuit can help solve this problem to some extent, it is quite difficult to find the optimum sparse solution. To overcome this limitation, we propose a method named label embedded dictionary learning (LEDL), which embeds the label information into l(1) regularized dictionary learning algorithm to improve the performance of image classification tasks. Specifically, (i) compared to LC-KSVD, we utilise the l(1)-norm to transfer the sparse constraint problem to convex optimization problem; (ii) alternating direction method of multipliers (ADMM) is adopted to solve the sparse constraint problem to improve the optimization speed; (iii) extensive experimental results on six benchmark datasets illustrate that the classification rate of our proposed algorithm exceeds the LC-KSVD algorithm and our proposed algorithm has achieved state-of-the-art performance. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						122	131		10.1016/j.neucom.2019.12.071													
J								Matrix cofactorization for joint representation learning and supervised classification - Application to hyperspectral image analysis	NEUROCOMPUTING										Image interpretation; Supervised learning; Representation learning; Hyperspectral images; Non-convex optimization; Matrix cofactorization	SPARSE REGRESSION; FACTORIZATION; DECOMPOSITION; SEGMENTATION; DICTIONARY; ALGORITHMS; ERROR; MODEL	Supervised classification and representation learning are two widely used classes of methods to analyze multivariate images. Although complementary, these methods have been scarcely considered jointly in a hierarchical modeling. In this paper, a method coupling these two approaches is designed using a matrix cofactorization formulation. Each task is modeled as a factorization matrix problem and a term relating both coding matrices is then introduced to drive an appropriate coupling. The link can be interpreted as a clustering operation over the low-dimensional representation vectors. The attribution vectors of the clustering are then used as features vectors for the classification task, i.e., the coding vectors of the corresponding factorization problem. A proximal gradient descent algorithm, ensuring convergence to a critical point of the objective function, is then derived to solve the resulting non-convex non-smooth optimization problem. An evaluation of the proposed method is finally conducted both on synthetic and real data in the specific context of hyperspectral image interpretation, unifying two standard analysis techniques, namely unmixing and classification. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				APR 14	2020	385						132	147		10.1016/j.neucom.2019.12.068													
J								Adaptive OFDM underwater acoustic transmission: An adversarial bandit approach	NEUROCOMPUTING										Adaptive OFDM; UASNs; Adversarial multi-armed bandit; Orthogonal learning strategy; Dynamic exploration mechanism	POWER ALLOCATION; ALGORITHM; OPTIMIZATION; ADAPTATION; NETWORKS	Adaptive orthogonal frequency-division multiplexing (OFDM) is a promising technology for underwater acoustic sensor networks (UASNs) to facilitate robust and reliable transmission. This paper deals with an adaptive UASN-OFDM multi-parameter allocation problem in a strongly incomplete information scenario. Specifically, an adversarial multi-armed bandit (MAB) formalism is first proposed, whereby no prior knowledge about channel conditions is required and the reward sequences are not restrained by any statistical assumptions. Second, considering the curse of dimensionality caused by exponentially large number of feasible strategies, we tailor orthogonal learning strategy to reinforce learning for initial decision set and achieve filtration by abandoning some inferior levels. Third, under strictly limited prior information, we design a time-based dynamic exploration mechanism to adjust exploration factor adaptively, which improves algorithm learning ability effectively. Thank to aforementioned efforts, a low-complexity, high-efficiency OD-Exp3 algorithm is presented to handle the complex adaptive OFDM problem in UASNs. Lastly, we show the upper regret bound and the convergence of OD-Exp3 algorithm. Comparative results demonstrate that the proposed algorithm is superior to the existing algorithms. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						148	159		10.1016/j.neucom.2019.12.063													
J								Learning and encoding motor primitives for limb actions in a brain-like computation approach	NEUROCOMPUTING										Motor learning and encoding; Motor primitive; Motor cortex; Brain-like computation; Arm action	CORTICAL CONTROL; ARM MOVEMENTS; DYNAMICS; FEEDBACK; CORTEX; NETWORK; MODELS	Recent neurophysiological studies discovered the sparse rotational patterns in the dynamics of neural population during motor control. In this work, we show that a computational model guided by the dynamical system theory of motor coding can successfully generate the similar network behaviors as found in the electrophysiological studies. The RNN-based model learns the arm reaching control policy from self-generated movements. Essential biomechanical and neural properties including multiphasic neural response and the sparse rotation naturally emerge after training for the movement control tasks. The temporal dynamics in the trained network is analyzed to illustrate how the sparse rotational patterns correlate to the generalization capability of the control policy. We find that the trial-and-error motor learning, which naturally brings in the generalization capability, lead to the existence of low-dimensional manifold in the population dynamics of the motor network. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						160	168		10.1016/j.neucom.2019.12.051													
J								Unsupervised framework for depth estimation and camera motion prediction from video	NEUROCOMPUTING										Unsupervised deep learning; Depth estimation; Camera motion prediction; Convolutional neural network	NETWORK	Depth estimation from monocular video plays a crucial role in scene perception. The significant drawback of supervised learning models is the need for vast amounts of manually labeled data (ground truth) for training. To overcome this limitation, unsupervised learning strategies without the requirement for ground truth have achieved extensive attention from researchers in the past few years. This paper presents a novel unsupervised framework for estimating single-view depth and predicting camera motion jointly. Stereo image sequences are used to train the model while monocular images are required for inference. The presented framework is composed of two CNNs (depth CNN and pose CNN) which are trained concurrently and tested independently. The objective function is constructed on the basis of the epipolar geometry constraints between stereo image sequences. To improve the accuracy of the model, a left-right consistency loss is added to the objective function. The use of stereo image sequences enables us to utilize both spatial information between stereo images and temporal photometric warp error from image sequences. Experimental results on the KITTI and Cityscapes datasets show that our model not only outperforms prior unsupervised approaches but also achieving better results comparable with several supervised methods. Moreover, we also train our model on the Euroc dataset which is captured in an indoor environment. Experiments in indoor and outdoor scenes are conducted to test the generalization capability of the model. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						169	185		10.1016/j.neucom.2019.12.049													
J								Leader-following bipartite consensus of second-order time-delay nonlinear multi-agent systems with event-triggered pinning control under signed digraph	NEUROCOMPUTING										Multi-agent systems; Bipartite leader-following consensus; Time-delay; Event-triggered control; Pinning control	MEMRISTIVE NEURAL-NETWORKS; NONIDENTICAL CHAOTIC SYSTEMS; SYNCHRONIZATION; STABILITY	This paper investigates the leader-following bipartite consensus problem for second-order multi-agent systems with nonlinear dynamics, node-delay and signed digraph topology. By using pinning control strategies, an event-triggered controller is designed to achieve bipartite consensus in multi-agent systems, where a novel event-triggered function is proposed. Two cases are discussed in this paper according to whether the node-delay is differentiable or not. When the node-delay is differentiable, an LMI (Linear Matrix Inequality) condition is derived for reaching bipartite consensus in multi-agent systems by using M-matrix theory. When the node-delay is non-differentiable, some bipartite consensus conditions are developed based on Jensen inequality and the reciprocally convex approach. Moreover, it is shown that Zeno-behavior can be avoided for the designed event-triggered controller. Finally, some numerical examples are provided to illustrate the effectiveness and correctness of the theoretical analysis. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						186	196		10.1016/j.neucom.2019.12.043													
J								Locality-aware group sparse coding on Grassmann manifolds for image set classification	NEUROCOMPUTING										Grassmann manifolds; Group sparse coding; Locality preserving; Image set classification	REPRESENTATION; RECOGNITION	Riemannian sparse coding methods are attracting increasing interest in many computer vision applications, relying on its non-Euclidean structure. One such recently successful task is image set classification by the aid of Grassmann Manifolds, where an image set can be seen as a point. However, due to irrelevant information and outliers, the probe set may be represented by misleading sets with large sparse coefficients. Meanwhile, it is difficult for a single subspace to cover changes within an image set and the hidden structure among samples is relaxed. In this paper, we propose a novel Grassmann Locality-Aware Group Sparse Coding model (GLGSC) that attempts to preserve locality information and take advantage of the relationship among image sets to capture the inter and intra-set variations simultaneously. Since the contributions of different gallery subspaces to the probe subspace should vary in importance, we then introduce a novel representation adaption term. In addition, a kernelised version of GLGSC is proposed to handle non-linearity in data. To reveal the effectiveness of our algorithm over state-of-the-art, several classification tasks are conducted, including face recognition, object recognition and gesture recognition. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						197	210		10.1016/j.neucom.2019.12.026													
J								Global synchronization of fractional-order quaternion-valued neural networks with leakage and discrete delays	NEUROCOMPUTING										Global synchronization; Fractional-order; Quaternion-valued neural networks; Leakage and discrete delays	ROBUST STABILITY ANALYSIS	In this paper, without transforming the quaternion-valued neural networks into two equivalent complex-valued systems or four equivalent real-valued systems, a novel class of fractional-order quaternion-valued neural networks with leakage and discrete delays is investigated. To this end, two novel inequalities are established with the aid of properties of quaternion and Caputo fractional derivative. By exploiting Lyapunov method, our established inequalities, fractional-order Razumikhin theorem and some analysis techniques, some criteria ensuring the global asymptotical synchronization and the global Mittag-Leffler synchronization of the considered networks are obtained through designing suitable controllers. Finally, two numerical examples are utilized to show the derived theoretical results. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						211	219		10.1016/j.neucom.2019.12.018													
J								Robust multi-view clustering via inter-and-intra-view low rank fusion	NEUROCOMPUTING										Multi-view clustering; Low rank matrices; Unsupervised learning	SPARSE; ALGORITHM	Multi-view Clustering has become a vital task with the rapidly growing amount of data in multiple representations. Under the context of unsupervised learning, the existing clustering methods show limited capability in leveraging the discriminative information from multiple views. View-independent noises and distribution differences among views are two main causes that may degrade the clustering performances. By introducing the inter-and-intra-view low rank decomposition, we propose Multi-view Clustering via Intra and Inter-view Low Rank Fusion (MCIIF). The proposed method facilitates clustering with the discriminative intra-view low rank structures that complement the shared inter-view transition. By recovering complementary low rank transitions from the differences between the shared inter-view transition and individual-view transitions, we pursue a more accurate transition probability matrix by combining the complementary intra-view transitions with the shared inter-view transition in an additive manner. To solve the corresponding optimization problem, we propose a procedure based on the Alternating Direction Method of Multipliers (ADMM) scheme with convergence guarantees. Experimental results on various real-world datasets verify the effectiveness of the proposed method over the state-of-art multi-view clustering methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						220	230		10.1016/j.neucom.2019.11.058													
J								How efficient deep-learning object detectors are?	NEUROCOMPUTING										Neural networks; Deep learning; Object detection; Efficiency analysis; Data envelopment analysis	DATA ENVELOPMENT ANALYSIS; ENVIRONMENTAL EFFICIENCY; LATIN-AMERICA; DEA; MODELS; PERFORMANCE; SYSTEMS	Deep-learning object-detection architectures are gaining attraction, as they are used for critical tasks in relevant environments such as health, self-driving, industry, security, and robots. Notwithstanding, the available architectures provide variable performance results depending on the scenario under consideration. Challenges are usually used to evaluate such performance only in terms of accuracy. In this work, instead of proposing a new architecture, we overcome the limitations of those challenges by proposing a computationally undemanding comparative model based on several Data Envelopment Analysis (DEA) strategies, not only for the comparison of deep-learning architectures, but also to detect which parameters are the most relevant features for achieving efficiency. In addition, the proposed model provides with a set of recommendations to improve object-detection frameworks. Those measures may be applied in future high-performance meta-architectures, since this model requires lower computational and temporal requirements compared to the traditional strategy based on training neural networks - based on the trial-error method - for each configurable parameter. To this aim, the presented model evaluates 16 parameters of 139 configurations of well-known detectors present in the Google data set [1]. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						231	257		10.1016/j.neucom.2019.10.094													
J								Multi-facet user preference learning for fine-grained item recommendation	NEUROCOMPUTING										Multi-facet user preference; Behavior feedback; Recommender system; Item recommendation	BAYESIAN PERSONALIZED RANKING	Existing recommendation methods mainly learn user preference from historical user-item interaction data, while ignoring the extent of interactions, i.e., diverse user experience and user intention. To be more specific, for new users with little or no experience, they may turn to popular items preferred by majority users. In terms of those medium-level users who already have interacted with some items, they may require and expect items to meet their personal preferences. As pro-active users are likely to leave rich behavior (action) feedback (e.g., view, like) on the items they interacted with, the system will have good chance to better interpret users' intention, and thus generate more accurate and elaborate recommendations to hit their preferences. In this paper, we propose a generic Multi-facet User Preference Learning (MUPL) framework for fine-grained item recommendation. By considering diverse user experience and intention, MUPL captures user preference in the level of group-, individual- and action-facet. Besides, the importance of user preference in different facets can be automatically learnt by MUPL. Extensive experiments on two real-world datasets (Xing, Sobazaar) demonstrate the superiority of our proposed approach over other state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						258	268		10.1016/j.neucom.2019.12.089													
J								Conditional restricted Boltzmann machine for item recommendation	NEUROCOMPUTING										Conditional restricted Boltzmann machine; Item recommendation; Explicit feedback		Recommender systems provide an excellent solution to the issue of information overload by generating item recommendation from a huge collection of items based on users' preferences. In terms of modeling users' rating data, existing methods are mainly neighborhood- and factorization-based methods, most of which are rating oriented. Among network-based methods, the restricted Boltzmann machine (RBM) model is also applied to rating prediction tasks. However, item recommendation tasks play a more important role in the real world, due to the large item space as well as users' limited attention. In this paper, we treat users' rating behaviors from a new perspective and study the effectiveness of conditional RBM (CRBM) in modeling users' rating preferences for top-k recommendation. We conduct extensive empirical studies on four real-world datasets and find that our proposed CRBM-IR is very competitive in exploiting users' explicit rating feedback in comparison with the closely related works. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						269	277		10.1016/j.neucom.2019.12.088													
J								Collaborative deep recommendation with global and local item correlations	NEUROCOMPUTING										Matrix factorization; Deep neural networks; Item correlations; Manifold regularization	NONNEGATIVE MATRIX-FACTORIZATION; NEURAL-NETWORKS; REGULARIZATION	Many recommendation methods have introduced item correlation information to alleviate the data sparsity and cold-start problems. However, existing approaches exploit either global or local item correlations, rarely consider both global and local item correlations, and thus they cannot provide advanced recommendation performance. Inspired by this, we propose a novel collaborative deep framework called GLICR to simultaneously incorporate the global and local item correlations into the model. More specifically, our proposed GLICR model tightly couples deep neural network with matrix factorization (MF), and jointly learns the deep feature representations of item content information in deep neural network and the rating matrix in MF. In addition, we introduce manifold regularization to learn the global and local item correlations directly from data. We conduct comprehensive experiments on real-world datasets at three different degrees of sparsity to confirm that our approach can effectively alleviate data sparsity problem and is superior to existing state-of-the-art recommendation techniques. This work is the first attempt that considers the global and local item correlations by manifold regularization in recommendation scenario. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						278	291		10.1016/j.neucom.2019.12.087													
J								Adaptive optimal controller design for a class of LDI-based neural network systems with input time-delays	NEUROCOMPUTING										Input delays; Nonlinear systems; Policy iteration; Delayed state feedback; Linear differential inclusion (LDI)	POLICY ITERATION; TRACKING CONTROL; LINEAR-SYSTEMS; OPTIMIZATION	In this paper, a new online adaptive optimal controller design scheme is studied for a class of nonlinear systems with input time-delays. First, we linearize the original nonlinear systems by means of linear differential inclusion technique. Then the adaptive optimal controller of the linearized systems with input time-delays is obtained by online policy iteration algorithm. It also proves the convergence of the designed adaptive optimal control algorithm. Finally, the effectiveness of the proposed method is verified by two simulation examples. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						292	299		10.1016/j.neucom.2019.12.084													
J								Fusing convolutional neural network features with hand-crafted features for osteoporosis diagnoses	NEUROCOMPUTING										Osteoporosis; Fusion; CNN features; Hand-crafted features; Encoded features	HYPERSPECTRAL IMAGE CLASSIFICATION; LOCAL BINARY PATTERNS; TEXTURE	Osteoporosis makes bones weak and brittle, increasing the risk of fracture. In this paper, we designed a hybrid model to diagnose osteoporosis based on bone radiograph images. Two types of features were used to distinguish between the "healthy" and the "sick". One type of features was obtained from deep convolutional neural networks (CNNs), named CNN features, and the other was hand-crafted features containing a group of standard texture features such as local binary pattern and gray level co-occurrence matrix and a group of "encoded features" that have shown impressive discriminative capabilities. We used a minimum-redundancy maximum-relevance algorithm to reduce the high dimensionality of the features and a support vector machine was used as the recognizer. This is the first study to fuse the CNNs features with the state-of-the-art osteoporotic texture features for osteoporosis diagnosis. We explore if the fusion of the two types of powerful features will increase the performance or not. Comparative experiments show that considerable performance improvements can be made through the fusion of both types of features, and the fusion of AlexNet with encoded features or all the hand-crafted features achieved the highest accuracy among all the fusions. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						300	309		10.1016/j.neucom.2019.12.083													
J								Echo state network optimization using binary grey wolf algorithm	NEUROCOMPUTING										Echo state network; Binary grey wolf optimization; Time series; Network structure optimization	GENETIC ALGORITHM; MODEL; RESERVOIR	The echo state network (ESN) is a powerful recurrent neural network for time series modelling. ESN inherits the simplified structure and relatively straightforward training process of conventional neural networks, and shows strong computational capabilities to solve nonlinear problems. It is able to map low-dimensional input signals to high-dimensional space for information extraction, but it is found that not every dimension of the reservoir output directly contributes to the model generalization. This work aims to improve the generalization capabilities of the ESN model by reducing the redundant reservoir output features. A novel hybrid model, namely binary grey wolf echo state network (BGWO-ESN), is proposed which optimises the ESN output connection by the feature selection scheme. Specially, the feature selection scheme of BGWO is developed to improve the ESN output connection structure. The proposed method is evaluated using synthetic and financial data sets. Experimental results demonstrate that the proposed BGWO-ESN model is more effective than other benchmarks, and obtains the lowest generalization error. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						310	318		10.1016/j.neucom.2019.12.069													
J								Observer-based event-triggered sliding mode control for uncertain descriptor systems with a neural-network event-triggering sampling scheme	NEUROCOMPUTING										Sliding mode observer; Descriptor systems; Event-triggering; Integral-type switching surface; LMIs	H-INFINITY CONTROL; STOCHASTIC-SYSTEMS; COMMUNICATION; STABILIZATION	This paper investigates the problem of the observer-based event-triggered sliding mode control for uncertain descriptor systems with exogenous disturbance via a new neural-network event-triggering strategy. Firstly, a new neural-network event-triggering communication scheme is proposed in order to decide whether sampled data is to be transmitted for the observer. Secondly, the existence conditions and the stability conditions of the sliding mode dynamics are presented in terms of linear matrix inequalities. An adaptive event-triggered sliding mode controller is designed to ensure the retainability to the sliding surface from beginning almost surely. Thirdly, an effective strategy is developed to avoid the Zeno behavior without decomposing the descriptor system, and the positive lower-bound of the inter-execution time intervals is provided. The existing method can not directly get the positive lower-bound. Thus, the parameters used for scaling corresponding inequalities are introduced to solve the problem. Finally, simulation results are presented to illustrate the effectiveness of the proposed method and are superior to the existing event triggering strategies in the literature. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				APR 14	2020	385						319	328		10.1016/j.neucom.2019.12.066													
J								Optimising topic coherence with Weighted Polya Urn scheme	NEUROCOMPUTING										Polya urn scheme; Unsupervised learning; Topic model; Sentiment analysis		Topic models have been widely used to mine hidden topics from documents. However, one limitation of such topic models is that they are prone to generate incoherent topics. To address this limitation, many approaches have been proposed to incorporate the prior knowledge of word semantic relatedness into the topic inference process. One example is the Generalized Polya Urn (GPU) scheme. However, GPU-based topic models often require sophisticated algorithms to acquire domain-specific knowledge from data. Moreover, prior knowledge is incorporated into the topic inference process without considering its impact on the intermediate topic sampling results. In this paper, we propose a novel Weighted Polya Urn scheme and incorporate it into Latent Dirichlet Allocation framework to build the self-enhancement topic model and generate coherent topics. In specific, semantic prior knowledge based on word embedding is employed to measure the semantic coherence of a word to different topics, which is incorporated into the Weighted Polya Urn scheme. Moreover, semantic coherence is updated dynamically based on the semantic similarity between a word and the representative words in different topics. Experiments have been conducted on seven public corpora from different domains to evaluate the effectiveness of the proposed approach. Experimental results show that compared to the state-of-the-art baselines, the proposed approach can generate more coherent topics. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						329	339		10.1016/j.neucom.2019.12.013													
J								Attention augmentation with multi-residual in bidirectional LSTM	NEUROCOMPUTING										Long Short-Term Memory; Attention augmentation; Natural language processing; Multi-residual network		Recurrent neural networks (RNNs) have been proven to be efficient in processing sequential data. However, the traditional RNNs have suffered from the gradient diminishing problem until the advent of Long Short-Term Memory (LSTM). However, LSTM is weak in capturing long-time dependency in sequential data due to the inadequacy of memory capacity in LSTM cells. To address this challenge, we propose an Attention-augmentation Bidirectional Multi-residual Recurrent Neural Network (ABMRNN) to overcome the deficiency. We propose an algorithm which integrates both past and future information at every time step with omniscient attention model. The multi-residual mechanism has also been leveraged in the proposed model targeting the pattern of the relationship between current time step and further distant time steps instead of only one previous time step. The results of experiments show that our model outperforms the traditional statistical classifiers and other existing RNN architectures. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						340	347		10.1016/j.neucom.2019.10.068													
J								Joint learning based deep supervised hashing for large-scale image retrieval	NEUROCOMPUTING										Large-scale image retrieval; Deep supervised hashing; Combined loss function; Joint learning	QUANTIZATION	Hashing has been widely used for large-scale image retrieval due to its high storage efficiency and fast calculation speed. Recent works have found that deep-supervised hashing methods are superior to nondeep-supervised hashing methods and unsupervised hashing methods in many applications. However, the previous deep-supervised hashing only uses the training set to learn the hash function, which results in that the discrete hash codes corresponding to the retrieval image set and the query image set are derived from the trained hash function. The retrieval image set did not participate in the training and learning of the network. Hence, it is difficult to acquire the real discrete hash code of the retrieval image. In addition, the traditional deep-supervised hashing methods failed to make full use of the supervised information. In this paper, we propose a novel deep supervised hashing method called Joint Learning Based Deep Supervised Hashing (JLDSH). It joints the image classification and hash function learning into the same end-to-end neural network framework. In the training process, we randomly select a subset from the retrieval image set to learn the hash function. The hash code of the entire retrieval image set is directly calculated by the trained hash code of the subset. Meanwhile, JLDSH sets a hyper-parameter on the supervised information to make the output of the network closer to the real discrete hash code. Furthermore, by adding a new loss function, an extended JLDSH-A model is proposed to make the image representation obtained by network training more discrete. Experimental results demonstrate that the proposed method achieves the state-of-the-art performance on benchmark datasets. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				APR 14	2020	385						348	357		10.1016/j.neucom.2019.12.096													
J								Efficient discrete supervised hashing for large-scale cross-modal retrieval	NEUROCOMPUTING										Cross-modal retrieval; Matrix factorization; Discrete optimization; Semantic embedding; Hashing	CODES	Supervised cross-modal hashing has gained increasing research interest on large-scale retrieval task owning to its satisfactory performance and efficiency. However, there are still some issues to be further addressed: (1) most of them fail to capture the inherent data structure effectively due to the complex correlations among heterogeneous data points; (2) most of them obtain continuous solutions firstly and then quantize the continuous solutions to generate hash codes directly, which causes large quantization error and consequent suboptimal retrieval performance; (3) most of them suffer from relatively high memory cost and computational complexity during training procedure, which makes them unscalable. In this paper, to address above issues, we propose a supervised hashing method for cross-modal retrieval dubbed Efficient Discrete Supervised Hashing (EDSH). Specifically, the sharing space learning with collective matrix factorization and semantic embedding with class labels are seamlessly integrated to learn hash codes. Therefore, the feature based similarities and semantic correlations are both preserved in hash codes, which makes the learned hash codes more discriminative. Then an efficient discrete optimal scheme is designed to handle the scalable issue. Instead of learning hash codes bit-by-bit, hash codes matrix can be obtained directly which is more efficient. Extensive experimental results on three public datasets show that our EDSH produces a superior performance in both accuracy and scalability over several existing cross-modal hashing approaches. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						358	367		10.1016/j.neucom.2019.12.086													
J								Reachable set bounding for a class of memristive complex-valued neural networks with disturbances	NEUROCOMPUTING										Complex-valued neural networks; Memristor; Reachable set; Bounded disturbances	GLOBAL EXPONENTIAL STABILITY; TIME-VARYING DELAYS; LINEAR-SYSTEMS; FINITE-TIME; SYNCHRONIZATION; STABILIZATION; DEVICES	This study focus on reachable set bounding for memristive complex-valued neural networks (MCVNNs) with bounded input disturbances. In order to find the region such that all the solutions of MCVNNs with bounded input disturbances converge within it, we utilize a approach based on Lyapunov-Krasovskii functional (LKF). From this, we derive some improved conditions to get the bounding ellipsoid in accordance with linear matrix inequalities (LMIs), the stability criteria are also obtained. At last, two illustrative simulations are also provided to show the availability and validity of obtained conclusions. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 14	2020	385						368	377		10.1016/j.neucom.2019.12.085													
J								Considering Slip-Track for Energy-Efficient Paths of Skid-Steer Rovers	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Energy-efficient paths; Skid-steer rovers; Optimization; Power modeling	MOBILE; KINEMATICS; ROBOT; MODEL; CAR	Skid-steer rovers consume a lot more power in point turns compared to straight line motion. As energy is the integral of power over time, the turning radius should be considered explicitly for this type of rover. Lower instantaneous power consumption for wider arcs must be traded off against shorter traversal distance for tighter arcs by evaluating the total energy consumed when following different paths. This research seeks to find the most energy-efficient path from among Circular arc - Line - Circular arc (CLC) paths, a generalization of Point turn - Line - Point turn (PLP) paths which are the simplest path to execute for a skid-steer rover traversing between general start and end poses. The optimally energy-efficient CLC path on hard ground is found to have circular arcs of radius R-', the turning radius at which a skid-steer rover's inner wheels are not commanded to turn. The radius corresponds to exactly half the rover's slip-track. Theoretical, numerical, and experimental evidenceis presented to support this result. Further, important features of the R-' turning radius are explored.																	0921-0296	1573-0409				OCT	2020	100	1					335	348		10.1007/s10846-020-01173-5		APR 2020											
J								Boosting a Reference Model-Based Controller Using Active Disturbance Rejection Principle for 3D Trajectory Tracking of Quadrotors: Experimental Validation	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Reference model-based control; Nonlinear control; Robust Control; IDA-PBC; ADRC; ESO; Quadrotor	ATTITUDE-CONTROL; UAV; INTERCONNECTION; STABILIZATION; PASSIVITY; DESIGN	It is relevant to develop an adequate control algorithm for quadrotors that guarantees a good compromise robustness/ performance. This compromise should be ensured with or without external disturbances. In this paper, we investigate and apply a revisited formulation of a reference model-based control strategy by introducing a boosting mechanism. This mechanism uses an Extended State-based Observer (ESO) to estimate the uncertainties and variety of disturbances. The estimation is continually updated and rejected from the main control loop. The reinforcement principle is inspired from the popular Active Disturbance Rejection Control (ADRC) technique in order to enhance the robustness ability of a nonlinear reference model-based control strategy (i.e. Interconnection and Damping Assignment-Passivity Based Control (IDA-PBC)). The obtained controller is augmented by an additional input, which is derived via sliding modes framework to handle the estimation errors and ensure asymptotic stability. This combination leads to promising results by improving the nominal control technique. The primary results are shown through numerical simulations and are confirmed, experimentally, with several scenarios.																	0921-0296	1573-0409				NOV	2020	100	2					597	614		10.1007/s10846-020-01182-4		APR 2020											
J								Multi-view visual Bayesian personalized ranking for restaurant recommendation	APPLIED INTELLIGENCE										Recommendation system; Multi-view visual features; Bayesian personalized ranking; Restaurant recommendation	SYSTEM	In recent recommendation systems, the image information of items is often used in conjunction with deep convolution network to directly learn the visual features of items. However, the existing approaches usually use only one image to represent an item. These approaches are inadequate for an item with multi-view related images. For a restaurant, it has visual information of food, drink, environment, and so on. Each view of an item can be represented by multiple images. In this paper, we propose a new factorization model that combines multi-view visual information with the implicit feedback data for restaurant prediction and ranking. The visual features (visual information) of images are extracted by using a deep convolution network and are integrated into a collaborative filtering framework. In order to conduct personalized recommendation better, the multi-view visual features are fused through user related weights. User related weights reflect the personalized visual preference for restaurants and the weights are different and independent between users. We applied this model to make personalized recommendations for users on two real-world restaurant review datasets. Experimental results show that our model with multi-view visual information achieves better performance than models without or with only single-view visual information.																	0924-669X	1573-7497				SEP	2020	50	9					2901	2915		10.1007/s10489-020-01703-6		APR 2020											
J								Automated mammogram breast cancer detection using the optimized combination of convolutional and recurrent neural network	EVOLUTIONARY INTELLIGENCE										Mammography; Breast cancer diagnosis; Optimized region growing; Deep hybrid learning; Firefly updated chick-based chicken swarm optimization	LEVEL SET METHOD; DIAGNOSIS SYSTEM; ALGORITHM; CLASSIFICATION; FRAMEWORK; WAVELET	The objective of this study is to frame mammogram breast detection model using the optimized hybrid classifier. Image pre-processing, tumor segmentation, feature extraction, and detection are the functional phases of the proposed breast cancer detection. A median filter eliminates the noise of the input mammogram. Further, the optimized region growing segmentation is carried out for segmenting the tumor from the image and the optimized region growing depends on a hybrid meta-heuristic algorithm termed as firefly updated chicken based CSO (FC-CSO). To the next of tumor segmentation, feature extraction is done, which intends to extract the features like grey level co-occurrence matrix (GLCM), and gray level run-length matrix (GRLM). The two deep learning architectures termed as convolutional neural network (CNN), and recurrent neural network (RNN). Moreover, both GLCM and GLRM are considered as input to RNN, and the tumor segmented binary image is considered as input to CNN. The result of this study shows that the AND operation of two classifier output will tend to yield the overall diagnostic accuracy, which outperforms the conventional models.																	1864-5909	1864-5917															10.1007/s12065-020-00403-x		APR 2020											
J								3D path planning for a robot based on improved ant colony algorithm	EVOLUTIONARY INTELLIGENCE										Robot; Ant colony algorithm; Path planning; Three-dimensional	PARTICLE SWARM OPTIMIZATION; GENETIC ALGORITHM; VEHICLES	Path planning is an important issue in the field of robotics research. Compared with traditional two-dimensional (2D) path planning, three-dimensional (3D) path planning is closer to practical applications. In this paper, a new improved ant colony algorithm is proposed to solve the problem of slow convergence speed, low efficiency and the tendency of falling into the local optimal solution of the traditional ant colony algorithm for the 3D path planning. There are three main improved steps in the novel ant colony algorithm in 3D path planning. In the first step, a pseudo-random state transition strategy is adopted to ensure the global search ability of the algorithm. In the second step, the pheromone update and the pheromone increment calculation method are used to accelerate the convergence speed of the algorithm which can ensure the quality of the solution. In the end step, a security value function of the heuristic function is used to ensure the security of path. In addition, the conditional fallback method is used to ensure the global search ability of the algorithm. Simulation results show that the new improved ant colony algorithm can find a feasible three-dimensional path quickly and efficiently.																	1864-5909	1864-5917															10.1007/s12065-020-00397-6		APR 2020											
J								Enhanced query processing over semantic cache for cloud based relational databases	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Amending query; Semantic cache; Query matching; Query processing; Cloud based relational databases	OPTIMIZATION	Semantic cache diminishes the expectancies of data retrieval over distributed system like cloud-based systems, by reusing already extracted data. It improves the performance of the retrieval system with limited bandwidth which is a key requirement in cloud, fog, edge and other mobile computing technologies. Cache management and query processing over semantic cache are two key activities which should be handled carefully. Query dispensation performs a significant part in terms of reusability in already retrieved data efficiently. Competent query dispensation algorithms made the semantic cache more efficient. This paper identifies four issues that cause decreasing the efficiency of query processing algorithm. The solution is provided to the identified issues to improve the efficiency of query processing. Improvement in efficiency is discussed along each of the issues. Efficiency analysis for enhanced algorithm is also provided and compared with state-of-the-art algorithms in the field. The proposed approach significantly improves the efficiency in many ways.																	1868-5137	1868-5145															10.1007/s12652-020-01943-x		APR 2020											
J								A hybrid algorithm based on artificial bat and backpropagation algorithms for multiplicative neuron model artificial neural networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Artificial bat algorithm; Back propagation algorithm; Hybrid algorithm; Multiplicative neuron model; Artificial neural networks; Forecasting	FUZZY LOGICAL RELATIONSHIPS; TIME-SERIES PREDICTION; TEMPERATURE PREDICTION	In the literature, the multiplicative neuron model artificial neural networks are trained by gradient-based or some artificial intelligence optimization algorithms. It is well known that the hybrid algorithms give successful results than classical algorithms in the literature and the use of hybrid systems increase day by day. From this point of view, different from other studies contribute to multiplicative neuron model artificial neural networks, the properties of an artificial intelligence optimization technique, artificial bat algorithm, and a gradient-based algorithm, backpropagation learning algorithm, is used together firstly by using the proposed method in this study. Thus, both a derivative and a heuristic algorithm were used together firstly for multiplicative neuron model artificial neural networks. The proposed method is applied to three well-known different real-world time series data. The performance of the proposed method is both compared with gradient-based optimization algorithms, some artificial optimization algorithms used for the training of artificial neural networks and some popular analyze methods. The analysis results show that the proposed hybrid method has superior performance than other methods.																	1868-5137	1868-5145															10.1007/s12652-020-01950-y		APR 2020											
J								Predictive composition of pictogram messages for users with autism	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Augmentative and alternative communication (AAC); Autism; Ergonomics; Interaction design; Personalization; Pictograms; Prediction	WORD-PREDICTION; DESIGN; PERSPECTIVES; CHILDREN	Communication is a basic need for every person. However, there are many people who present disabilities that prevent communication through natural language. Augmentative and alternative communication (AAC) systems, including those based on pictograms, attempt to facilitate the communication for people with this kind of difficulties. In this paper we present PictoEditor, an augmentative and alternative communication application for the composition of pictogram messages for users with autism that incorporates prediction functionalities. Although such functionalities have been widely studied in text-based augmentative and alternative communication tools, they have not been applied to pictogram based ones. The results show that prediction based on frequency of use of specific pictograms improves the immediate availability of the desired pictograms, but the improvement with prediction based on sequencing of pseudo-syntactic types of pictogram is not as clear.																	1868-5137	1868-5145															10.1007/s12652-020-01925-z		APR 2020											
J								Participation of senior citizens in somatosensory games: a correlation between the willingness to exercise and happiness	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Somatosensory games; Willingness to exercise; Sense of happiness; Elderly; Game intervention; Nursing homes	WELL; EXERGAMES; PERSONALITY; PERFORMANCE; VALIDATION; RESIDENTS	In this study, the correlation between the willingness to exercise and happiness of senior citizens living in nursing homes was determined. For this purpose, 30 people older than 65 who were capable of performing various activities in Taiwan's nursing homes were recruited and divided into experimental and control groups. The experimental group participated in the somatosensory game "Old-age sitting posture and health exercises", which includes intervention strategies and was designed by physiotherapists, social workers, and functional therapists. and 16 different activities for eight weeks, while the control group was not involved in them. Both groups filled out "Growing Happiness in the Old Age: Chinese Scale" and "Sports Participation Intention Scale" questionnaires before and after the intervention to evaluate the levels of happiness and willingness to exercise of the elderly. It was found that somatosensory game interventions significantly increased the degree of social interaction and maintenance of the willingness to exercise of the elderly living in nursing homes over time. Moreover, happiness indicators were significantly improved after the games, especially health, autonomy, and social aspects, suggesting that the willingness to exercise was positively correlated with happiness. Therefore, somatosensory games can promote the physical and mental health of the elderly and may be used by senior citizens living in nursing homes as part of a multi-activity project.																	1868-5137	1868-5145															10.1007/s12652-020-01918-y		APR 2020											
J								Study on metal mine detection from underwater sonar images using data mining and machine learning techniques	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Uwcns; UWSNs; Mine detection; Machine learning; Data mining; KNN classifier; Gradient booster; Decision tree; SVM		Ocean mines are the major threat to the safety of great vessels and other living beings in the marine life. It is a self-contained explosive device placed in water to destroy ships or submarines. Due to various factors like variations in operating and target shapes, environmental conditions, presence of spatially varying clutter, compositions and orientation, detection and classification of sonar imagery with respect to underwater objects is a complicated problem. It is well known that many post processing techniques in image processing have done to receive high resolution images to distinguish the objects. However the mentioned technique needs a special method to detect the metal from the usual sub bottom materials mainly rocks. Hence the data collection made in simulated environment locating metals in rock bed and collected with the sonar and the distinguished features of metals from rock have been identified with the totally different approach called intruder detection technique using data mining/machine learning. This paper proposes a novel approach for discriminating and detection of objects in underwater environment with accuracy of 90% (full feature set) and 86% (selected feature set). Hence, it is quite revealing that the new technique is better in classification of mine like objects in underwater, justified with samples of sonar data sets.																	1868-5137	1868-5145															10.1007/s12652-020-01958-4		APR 2020											
J								Despeckling of polarimetric SAR images using optimized bandelet transform and GCV thresholding	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										PolSAR; Bandelet; Despeckling; Directional variance; GCV threshold		Polarimetric synthetic aperture radar (PolSAR) imaging opened up a new era in remote sensing by exploiting the advantages of electromagnetic signals over optical signals. Much hype and hope exists over optical imaging due to its all weather imaging capability and capturing finer target details. However as in SAR imaging, PolSAR is also affected by speckle noise which is generally treated as multiplicative in nature. In order to extract target features, it is inevitable to filter out the speckle noise as part of preprocessing step in any image processing application. Transform domain filtering is desirable over spatial domain techniques due to the energy compaction property and easiness of filtering using thresholding. A novel method of despeckling using bandelet transform is proposed here as bandelets retain the geometrical features of the target effectively in comparison with other transforms post processing. Since the computational complexity involved with bandelet transform is relatively high compared to the state of the art techniques, a new approach using optimized bandelet transform based on directional variance coupled with optimal GCV threshold is proposed here. Performance comparison using both air borne and space borne real PolSAR images show that the proposed technique is much more effective and computationally efficient.																	1868-5137	1868-5145															10.1007/s12652-020-01929-9		APR 2020											
J								Quality of Service based Ad hoc On-demand Multipath Distance Vector Routing protocol in mobile ad hoc network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Mobile ad hoc networks (MANETs); Routing; Ad hoc On-demand Multipath Distance Vector (AOMDV); Intrusion detection; Grammatical Evolution (GE)		Mobile ad hoc networks (MANETs) are wireless networks that include many peer nodes. The node mobility in the MANETs leads to several issues like maintenance of paths, lifespan of the battery, safety, reliability and unpredictable link traits. All these in turn would adversely affect the network Quality of Service (QoS). In MANETs, a major role is played by the routing protocol for discovering as well as maintaining the paths. There are two types of routing: uni-path and multi-path. The MANET network can be made more reliable using the multipath routing protocol. The focus of this research is evaluating the multipath routing protocol for QoS. For better delivering of data, the Ad hoc On-demand Multipath Distance Vector (AOMDV) has improved methods. This maintains the QoS in terms of factors like MANET end-to-end delay, hop count and bandwidth. This work explores the evolutionary computation schemes for optimizing the routing. The discovery of QoS route in multi-constrained network is a complex problem, this is solved optimally using heuristic algorithms. In that, specifically used for intrusion detection programs in such challenging set ups would be Grammatical Evolution (GE). For finding out familiar threats in MANETs, the natural evolution-motivated GE scheme has been applied. The outcomes have shown that in MANETs, the proposed AOMDV-QoS schemes fulfill the Quality of Service requirements along with lesser delay and high reliability.																	1868-5137	1868-5145															10.1007/s12652-020-01935-x		APR 2020											
J								Modal parameter identification of general cutter based on milling stability theory	JOURNAL OF INTELLIGENT MANUFACTURING										General cutter; Modal parameter; Milling dynamics; Stability lobe diagram (SLD); In-cut cutting edge; Cutting force coefficient	CUTTING FORCE; DYNAMICS; CHATTER; COEFFICIENTS; MODEL	In the field of CNC milling, chatter has been a hot research topic, which is related to machining quality, precision and cost. Stability lobe diagram (SLD) reflects the vibration of the machining system under different process parameters and cutter axis vectors that is significant for optimization. The accurate dynamic characteristics of the machining system is the prerequisite for stability analysis. Finite element simulation is mainly aimed at small diameter cutter system, and accuracy is poor. The most widely used method is hammer test, but the equipment is expensive, the operation is too professional and it cannot reflect the dynamic characteristics of the machining system in working status. This paper proposes an undetermined coefficient method for the general cutter system to identify the modal parameters, that are the natural frequency, stiffness and damping ratio, just based on the very simple experiment of three-axis half-immersion milling in horizontal plane. Firstly, considering the exact in-cut cutting edge and the instantaneous cutting force coefficient corresponding to the axial factor and the chip thickness, the dynamic model of three-axis milling machining for the general cutter is established. Secondly, two implicit conditions of stable critical speed and cutting depth are derived based on feedback control theory in the frequency domain. Thirdly, the two sets of critical cutting depth and the chatter frequency under arbitrary speeds are obtained by using the dichotomy. With the method proposed in this paper, one of the two is used to solve a series of modal parameter sets, and the other of the two is used to extract the optimal modal parameters in the modal parameter sets. Finally, taking the identified modal parameters as known conditions to search the points one by one in the two-dimensional space composed of the rotational speed and the cutting depth, and judge whether it meets the critical conditions. SLD can be obtained by connecting the points that satisfy critical conditions together. Based on the previous experiment of flat-end cutter, it verified the feasibility of the modal parameter identification method in the paper. In the designed three-axis milling experiment of the ball-end cutter, the in-cut cutting edge simulation and the cutting force coefficient identification were carried out, and the modal parameters of the cutter system were also obtained successfully. The plotted lobe diagram was verified by the spectrum analysis result of the vibration signal collected by the acceleration sensor.																	0956-5515	1572-8145															10.1007/s10845-020-01569-y		APR 2020											
J								Fast incremental structure from motion based on parallel bundle adjustment	JOURNAL OF REAL-TIME IMAGE PROCESSING										3D reconstruction; K nearest neighbor; Feature matching; Structure from motion; Parallel computing	SFM	Structure from motion has attracted a lot of research in recent years, with new state-of-the-art approaches coming almost every year. One of its advantages over 3D reconstruction is that it can be used for any cameras (UAVs, depth sensor, light field) and produces relatively accurate point clouds and camera parameters. One of its disadvantages compared to other approaches is that it is computationally expensive. In this paper, we design a novel structure-from-motion framework to reduce the computational cost and implement a parallel bundle adjustment on GPU device for large-scale optimization. In our framework, the local bundle adjustment is added into the architecture of the incremental structure from motion; namely, the point clouds and camera's parameters are optimized when an additional number of images was added. Then, the purpose is not only to improve the quality of the produced point clouds but also to reduce computation time via parallel bundle adjustment. We conduct extensively experiments on several challenging datasets and make comparison with the state-of-the-art methods. Experimental results show that the proposed method has the best performance in terms of accuracy and efficiency.																	1861-8200	1861-8219															10.1007/s11554-020-00970-3		APR 2020											
J								Just-In-Time Constraint-Based Inference for Qualitative Spatial and Temporal Reasoning	KUNSTLICHE INTELLIGENZ										Qualitative constraints; Spatio-temporal reasoning; Just-in-time inference; Local consistencies; Singleton checks; Dynamic algorithms; Decomposability; Adaptivity; Parallelization	ALGORITHMS; HEURISTICS; COMPLEXITY; MODELS	We discuss a research roadmap for going beyond the state of the art in qualitative spatial and temporal reasoning (QSTR). Simply put, QSTR is a major field of study in Artificial Intelligence that abstracts from numerical quantities of space and time by using qualitative descriptions instead (e.g., precedes, contains, is left of); thus, it provides a concise framework that allows for rather inexpensive reasoning about entities located in space or time. Applications of QSTR can be found in a plethora of areas and domains such as smart environments, intelligent vehicles, and unmanned aircraft systems. Our discussion involves researching novel local consistencies in the aforementioned discipline, defining dynamic algorithms pertaining to these consistencies that can allow for efficient reasoning over changing spatio-temporal information, and leveraging the structures of the locally consistent related problems with regard to novel decomposability and theoretical tractability properties. Ultimately, we argue for pushing the envelope in QSTR via defining tools for tackling dynamic variants of the fundamental reasoning problems in this discipline, i.e., problems stated in terms of changing input data. Indeed, time is a continuous flow and spatial objects can change (e.g., in shape, size, or structure) as time passes; therefore, it is pertinent to be able to efficiently reason about dynamic spatio-temporal data. Finally, these tools are to be integrated into the larger context of highly active areas such as neuro-symbolic learning and reasoning, planning, data mining, and robotic applications. Our final goal is to inspire further discussion in the community about constraint-based QSTR in general, and the possible lines of future research that we outline here in particular.																	0933-1875	1610-1987				JUN	2020	34	2			SI		259	270		10.1007/s13218-020-00652-z		APR 2020											
J								LETHE: Forgetting and Uniform Interpolation for Expressive Description Logics	KUNSTLICHE INTELLIGENZ										Description logics; Non-classical reasoning; Uniform interpolation; Forgetting		Uniform interpolation and forgetting describe the task of projecting a given ontology into a user-specified vocabulary, that is, of computing a new ontology that only uses names from a specified set of names, while preserving all logical entailments that can be expressed with those names. This is useful for ontology analysis, ontology reuse and privacy. Lethe is a tool for performing uniform interpolation on ontologies in expressive description logics, and it can be used from the command line, using a graphical interface, and as a Java library. It furthermore implements methods for computing logical difference and performing abduction using uniform interpolation. We present the tool together with an evaluation on a varied corpus of realistic ontologies.																	0933-1875	1610-1987				SEP	2020	34	3			SI		381	387		10.1007/s13218-020-00655-w		APR 2020											
J								Research on covert communication channel based on modulation of common compressed speech codec	NEURAL COMPUTING & APPLICATIONS										G; 723; 1 and G; 729 speech codec; Information hiding; Covert communication channel; Codebook; Pulse position code	STEGANOGRAPHY	As is well known, multimedia has been widely used in VoIP and mobile communications. Research on how to establish covert communication channel over the above popular public applications has been flourishing in recent years. This paper tries to present a novel and effective method to construct a covert channel over common compressed speech stream by embedding sense information into it. In our method, after analysing the characteristic features of the excitation pulse positions of the ITU-T G.723.1 and G.729A speech codec, we design a novel and effective covert communication channel by finely modulating the codes of excitation pulse positions of the above two codecs in line with the secret information to be hidden. To improve the embedding capacity of the proposed method, we also use all the odd/even characteristics of pulse code positions to conduct information hiding. To test and verify the proposed approach, experiments are conducted on several different scenarios. Experimental results show that our methods and algorithms perform a higher degree of secrecy and sound information embedding efficacy compared with exiting similar methods.																	0941-0643	1433-3058															10.1007/s00521-020-04882-y		APR 2020											
J								A CNN-LSTM model for gold price time-series forecasting	NEURAL COMPUTING & APPLICATIONS										Deep learning; Convolutional neural networks; Time series; Gold price forecasting	MARKETS; MACHINE	Gold price volatilities have a significant impact on many financial activities of the world. The development of a reliable prediction model could offer insights in gold price fluctuations, behavior and dynamics and ultimately could provide the opportunity of gaining significant profits. In this work, we propose a new deep learning forecasting model for the accurate prediction of gold price and movement. The proposed model exploits the ability of convolutional layers for extracting useful knowledge and learning the internal representation of time-series data as well as the effectiveness of long short-term memory (LSTM) layers for identifying short-term and long-term dependencies. We conducted a series of experiments and evaluated the proposed model against state-of-the-art deep learning and machine learning models. The preliminary experimental analysis illustrated that the utilization of LSTM layers along with additional convolutional layers could provide a significant boost in increasing the forecasting performance.																	0941-0643	1433-3058															10.1007/s00521-020-04867-x		APR 2020											
J								A fuzzy generalized power series method under generalized Hukuhara differentiability for solving fuzzy Legendre differential equation	SOFT COMPUTING										Fuzzy analytic functions; Ordinary point of fuzzy equation; Fuzzy comparison test; Uniqueness fuzzy analytic solution; Fuzzy generalized power series method; Fuzzy Legendre differential equation	NUMERICAL-METHODS; VALUED FUNCTIONS; REAL; SEQUENCES; INTERVAL	In this paper, first the fuzzy generalized power series method, in which the coefficients are fuzzy numbers, is introduced, and then, the conditions of the uniqueness of the solution and its convergence for the fuzzy differential equation are investigated. Then, using the fuzzy generalized power series method, the fuzzy Legendre differential equation is considered as a case study, and finally, for further illustration some related examples are solved.																	1432-7643	1433-7479				JUN	2020	24	12			SI		8763	8779		10.1007/s00500-020-04913-9		APR 2020											
J								Fusion of deep-learned and hand-crafted features for cancelable recognition systems	SOFT COMPUTING										Deep learning; Feature fusion; Cancelable biometrics	BIOMETRICS; PCA	The recent years have witnessed a dramatic shift in the way of biometric identification, authentication, and security processes. Among the essential challenges that face these processes are the online verification and authentication. These challenges lie in the complexity of such processes, the necessity of the personal real-time identifiable information, and the methodology to capture temporal information. In this paper, we present an integrated biometric recognition method to jointly recognize face, iris, palm print, fingerprint and ear biometrics. The proposed method is based on the integration of the extracted deep-learned features together with the hand-crafted ones by using a fusion network. Also, we propose a novel convolutional neural network (CNN)-based model for deep feature extraction. In addition, several techniques are exploited to extract the hand-crafted features such as histogram of oriented gradients (HOG), oriented rotated brief (ORB), local binary patterns (LBPs), scale-invariant feature transform (SIFT), and speeded-up robust features (SURF). Furthermore, for dimensional consistency between the combined features, the dimensions of the hand-crafted features are reduced using independent component analysis (ICA) or principal component analysis (PCA). The core of this paper is the template protection via a cancelable biometric scheme without significantly affecting the recognition performance. Specifically, we have used the bio-convolving approach to enhance the user's privacy and ensure the robustness against spoof attacks. Additionally, various CNN hyper-parameters with their impact on the proposed model performance are studied. Our experiments on various datasets revealed that the proposed method achieves 96.69%, 95.59%, 97.34%, 96.11% and 99.22% recognition accuracies for face, iris, fingerprint, palm print and ear recognition, respectively.																	1432-7643	1433-7479				OCT	2020	24	20					15189	15208		10.1007/s00500-020-04856-1		APR 2020											
J								SIR-IM: SIR rumor spreading model with influence mechanism in social networks	SOFT COMPUTING										Social networks; SIR rumor spreading model; Influence mechanism; Numerical simulations		Rumor, with the fast speed of transmission, may bring us panic, even economic loss. Thus, it is significant for us to take effective steps to control the rumor spreading. Unfortunately, most of the existing works ignore that the spreading probability is not a constant, but depends on the number of spreaders currently. That is to say, the more spreaders, the larger spreading probability. In order to overcome this shortcoming, in this paper, we propose a novel susceptible-infected-removed (SIR) rumor spreading model with the influence mechanism, called SIR-IM, which first incorporates the number of current spreaders into the spreading probability. Then, it employs time function to describe the rate of people from spreader to stifler as time goes on. Moreover, we not only derive mean-field equations to describe the dynamics of our SIR model, but also give theoretical analysis. Numerical simulations are conducted on social networks, which show that the influence mechanism can accelerate the rumor spreading.																	1432-7643	1433-7479															10.1007/s00500-020-04915-7		APR 2020											
J								Passivity Analysis of Non-autonomous Discrete-Time Inertial Neural Networks with Time-Varying Delays	NEURAL PROCESSING LETTERS										Discrete-time inertial neural networks; Non-autonomous; Passivity; Maximum singular value; Time-varying delays	DISSIPATIVE DYNAMICAL-SYSTEMS; EXPONENTIAL STABILITY; HOPF-BIFURCATION; CHAOS; SYNCHRONIZATION; PASSIFICATION; STABILIZATION; INEQUALITY; MODELS	This paper addresses the passivity problem for delayed non-autonomous discrete-time inertial neural networks (NDINN), including the discrete-time switched inertial neural networks (DSINN) with state-dependent discontinuous right-hand side as its special case. First, we take a linear transformation to transform the original network into first-order difference equations. Second, by utilizing the Lyapunov direct method and with the help of the property of maximum singular value, we present a passivity criterion for the NDINN with delay-dependent linear matrix inequalities. Combining with the characteristic function method, the proposed analytical approach for NDINN is further extended to the DSINN. Finally, two simulation examples validate the efficacy of the analytical results.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2929	2944		10.1007/s11063-020-10235-6		APR 2020											
J								A machine learning approach for imputation and anomaly detection in IoT environment	EXPERT SYSTEMS										anomaly detection; dimensionality reduction; imputation; IoT; missing value; similarity	SIMILARITY MEASURE; SECURITY; INTERNET; CLASSIFICATION; TRENDS	The problem of anomaly and attack detection in IoT environment is one of the prime challenges in the domain of internet of things that requires an immediate concern. For example, anomalies and attacks in IoT environment such as scan, malicious operation, denial of service, spying, data type probing, wrong setup, malicious control can lead to failure of an IoT system. Datasets generated in an IoT environment usually have missing values. The presence of missing values makes the classifier unsuitable for classification task. This article introduces (a) a novel imputation technique for imputation of missing data values (b) a classifier which is based on feature transformation to perform classification (c) imputation measure for similarity computation between any two instances that can also be used as similarity measure. The performance of proposed classifier is studied by using imputed datasets obtained through applying Kmeans, F-Kmeans and proposed imputation methods. Experiments are also conducted by applying existing and proposed classifiers on the imputed dataset obtained using proposed imputation technique. For experimental study in this article, we have used an open source dataset named distributed smart space orchestration system publicly available from Kaggle. Experiment results obtained are also validated using Wilcoxon non-parametric statistical test. It is proved that the performance of proposed approach is better when compared to existing classifiers when the imputation process is performed using F-Kmeans and K-Means imputation techniques. It is also observed that accuracies for attack classes scan, malicious operation, denial of service, spying, data type probing, wrong setup are 100% while it is 99% for malicious control attack class when the proposed imputation and classification technique are applied.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12556	10.1111/exsy.12556		APR 2020											
J								A comparative study of machine learning and deep learning algorithms to classify cancer types based on microarray gene expression data	PEERJ COMPUTER SCIENCE										Machine Learning; Deep Learning; Cancer classification; Microarray gene expression; 11_tumor database; Bioinformatics	CLASSIFICATION	Cancer classification is a topic of major interest in medicine since it allows accurate and efficient diagnosis and facilitates a successful outcome in medical treatments. Previous studies have classified human tumors using a large-scale RNA profiling and supervised Machine Learning (ML) algorithms to construct a molecular-based classification of carcinoma cells from breast, bladder, adenocarcinoma, colorectal, gastro esophagus, kidney, liver, lung, ovarian, pancreas, and prostate tumors. These datasets are collectively known as the 11_tumor database, although this database has been used in several works in the ML field, no comparative studies of different algorithms can be found in the literature. On the other hand, advances in both hardware and software technologies have fostered considerable improvements in the precision of solutions that use ML, such as Deep Learning (DL). In this study, we compare the most widely used algorithms in classical ML and DL to classify the tumors described in the 11_tumor database. We obtained tumor identification accuracies between 90.6% (Logistic Regression) and 94.43% (Convolutional Neural Networks) using k-fold cross-validation. Also, we show how a tuning process may or may not significantly improve algorithms' accuracies. Our results demonstrate an efficient and accurate classification method based on gene expression (microarray data) and ML/DL algorithms, which facilitates tumor type prediction in a multi-cancer-type scenario.																	2376-5992					APR 13	2020									e270	10.7717/peerj-cs.270													
J								Training error and sensitivity-based ensemble feature selection	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Ensemble; Feature selection; Sensitivity; NSGA-III	NONDOMINATED SORTING APPROACH; CLASSIFICATION; ALGORITHM; OPTIMIZATION	Ensemble feature selection combines feature selection and ensemble learning to improve the generalization capability of ensemble systems. However, current methods minimizing only the training error may not generalize well on future unseen samples. In this paper, we propose a training error and sensitivity-based ensemble feature selection method. The NSGA-III is applied to find optimal feature subsets by minimizing two objective functions of the whole ensemble system simultaneously: the training error and the sensitivity of the ensemble. With this scheme, the ensemble system maintains both high accuracy and high stability which is expected to achieve a high generalization capability. Experimental results on 18 datasets show that the proposed method significantly outperforms state-of-the-art methods.																	1868-8071	1868-808X				OCT	2020	11	10					2313	2326		10.1007/s13042-020-01120-8		APR 2020											
J								Co-occurrence patterns in diagnostic data	COMPUTATIONAL INTELLIGENCE										clan decomposition; exploratory data analysis; Gaifman graphs	TRANSACTION DATA	We demonstrate how graph decomposition techniques can be employed for the visualization of hierarchical co-occurrence patterns between medical data items. Our research is based on Gaifman graphs (a mathematical concept introduced in Logic), on specific variants of this concept, and on existing graph decomposition notions, specifically, graph modules and the clan decomposition of so-called 2-structures. The construction of the Gaifman graphs from a dataset is based on co-occurrence, or lack of it, of items in the dataset. We may select a discretization on the edge labels to aim at one among several Gaifman graph variants. Then, the decomposition of the graph may provide us with visual information about the data co-occurrences, after which one can proceed to more traditional statistical analysis.																	0824-7935	1467-8640															10.1111/coin.12317		APR 2020											
J								An intelligent personalized web blog searching technique using fuzzy-based feedback recurrent neural network	SOFT COMPUTING										World Wide Web (WWW); CCSN; Web personalized; RNN	SOCIAL MEDIA; ALGORITHM; REGRESSION	In web information retrieval, every single user has a unique contextual objective when searching for information on web blogs. Based on the given query, the task of the web search engine is to fetch the most related information from the collection of web blogs. In order to enhance the searching ability, efficient semantic matching models, richer training, and evaluation resources are required. The conventional keyword-based search algorithms have a minimum efficiency in knowing users' intentions compared to machine learning algorithms. Recently, neural networks are well-recognized in information retrieval due to the ability of vector representation learning. This paper proposes an adaptive fuzzy feedback recurrent neural network-based web blog searching technique which follows inverse filtering (IF) algorithm using Word2Vec representation. Initially, the user query is pre-processed, and then given to the IF for accelerating the search process. Inside the IF, the web blog content is labelled according to their blog information and stored in the hash table, and then relevant contents are extracted by TF-IDF; this result is given to the similarity estimation to obtain the similarity score. Finally, the proposed technique considers the feedback got from users and performs re-ranking by fuzzy-based RNN to achieve rich user intention satisfaction. The implementation is performed in Python, and statistical measures such as accuracy, precision, and recall are utilized for performance evaluation. The results demonstrate that the proposed technique has enhanced accuracy (94%) compared to conventional techniques, namely deep auto-encoder, deep neural networks, and artificial neural networks.																	1432-7643	1433-7479				JUN	2020	24	12			SI		9321	9333		10.1007/s00500-020-04891-y		APR 2020											
J								A bi-objective function optimization approach for multiple sequence alignment using genetic algorithm	SOFT COMPUTING										Multiple sequence alignment; Genetic algorithm; Integer coding; Selection; Wilcoxon sign test; Experimental comparison; Bi-objective function	HIDDEN MARKOV-MODELS; PROTEIN; ACCURACY; IMPROVEMENT; BENCHMARK; COLONY; MAFFT	Multiple sequence alignment (MSA) is characterized as a very high computational complex problem. Therefore, MSA problem cannot be solved by exhaustive methods. Nowadays, MSA is being solved by optimizing more than one objective simultaneously. In this paper, we propose a new genetic algorithm based alignment technique, named bi-objective sequence alignment using genetic algorithm (BSAGA). The novelty of this approach is its selection process. One part of the population is selected based on the Sum of Pair, and rest is selected based on Total Conserve Columns. We applied integer-based chromosomal coding to represent only the gap positions in an alignment. Such representation improves the search technique to reach an optimum even for longer sequences. We tested and compared the alignment score of BSAGA with other relevant alignment techniques on BAliBASE and SABmark. The BSAGA shows better performance than others do, which was further proved by the Wilcoxon sign test.																	1432-7643	1433-7479				OCT	2020	24	20					15871	15888		10.1007/s00500-020-04917-5		APR 2020											
J								Automatic detection and classification of EEG artifacts using fuzzy kernel SVM and wavelet ICA (WICA)	SOFT COMPUTING										Wavelet ICA (WICA); Fuzzy kernel support vector machine (FKSVM); Aircraft; ECG signal	REMOVAL; ELECTROENCEPHALOGRAMS; POTENTIALS; EXTRACTION	Electroencephalography (EEG) is almost contaminated with many artifacts while recording the brain signal activity. Clinical diagnostic and brain computer interface applications frequently require the automated removal of artifacts. In digital signal processing and visual assessment, EEG artifact removal is considered to be the key analysis technique. Nowadays, a standard method of dimensionality reduction technique like independent component analysis (ICA) and wavelet transform combination can be explored for removing the EEG signal artifacts. Manual artifact removal is time-consuming; in order to avoid this, a novel method of wavelet ICA (WICA) using fuzzy kernel support vector machine (FKSVM) is proposed for removing and classifying the EEG artifacts automatically. Proposed method presents an efficient and robust system to adopt the robotic classification and artifact computation from EEG signal without explicitly providing the cutoff value. Furthermore, the target artifacts are removed successfully in combination with WICA and FKSVM. Additionally, proposes the various descriptive statistical features such as mean, standard deviation, variance, kurtosis and range provides the model creation technique in which the training and testing the data of FKSVM is used to classify the EEG signal artifacts. The future work to implement various machine learning algorithm to improve performance of the system.																	1432-7643	1433-7479				NOV	2020	24	21					16011	16019		10.1007/s00500-020-04920-w		APR 2020											
J								A human resource allocation method for business processes using team faultlines	APPLIED INTELLIGENCE										Resource allocation; Team faultlines; Process mining; Business process	DEMOGRAPHIC FAULTLINES; DIVERSITY FAULTLINES; WORK; CONFLICT; ASSIGNMENT	Equitable human resource allocation can maximize the efficiency of resources and optimize business performance. Despite numerous methods that have been suggested for solving the allocation problem, most of the existing methods focus on a single resource or task, and neglect the effects of team composition in business performance. In this paper, we introduce team faultlines to the human resource allocation problem. We first analyze resource characteristics from a demographic perspective and business process, then utilize the information value to select key characteristics and determine the corresponding weight. Second, we qualitatively identify team faultlines based on the clustering results of human resources and quantitatively measure the strength and distance of team faultlines. Multi-layer perceptron is utilized to build the base and ensemble performance prediction model. The allocation model and flow are designed subsequently. The reasonableness and effectiveness are evaluated with a real-world scenario, and the results show that our human resource allocation method using team faultlines can allocate human resources with high performance and optimize the business process.																	0924-669X	1573-7497				SEP	2020	50	9					2887	2900		10.1007/s10489-020-01686-4		APR 2020											
J								Comprehensive analysis for class imbalance data with concept drift using ensemble based classification	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Concept drift; Class imbalance; Ensemble classification; Datastream mining	EXTREME LEARNING-MACHINE; ALGORITHM; MAJORITY; PROOF	In many information system applications, the environment is dynamic and tremendous amount of streaming data is generated. This scenario enforces additional computational demand on the algorithm to process incoming instances incrementally using restricted memory and time compared to static data mining. Moreover, when the streams of data are collected from different sources, it may exhibit concept drift, which means the variation in the distribution of data and it can have a high degree of class imbalance. The problem of class imbalance occurs when there is a much lower number of an example representing one class than those of the other class. Concept drift and imbalanced streaming data are commonly found in real-world applications such as fraud detection, intrusion detection, decision support system and disease prediction. In this paper, the different concept drift detectors and handling approaches are analysed when dealing with imbalance data. A comparative analysis of concept drift is performed on various data sets like SEA synthetic data stream and real world datasets. Massive Online Analysis (MOA) tool is used to make the comparative study about different learners in a concept drifting environment. The performance measure such as Accuracy, Precision, Recall, F1-score and Kappa statistic has been used to evaluate the performance of the various learners on SEA synthetic data stream and real world dataset. Ensemble classifiers and single learners are employed and tested on the data samples of SEA synthetic data stream, electrical and KDD intrusion data set. The ensemble classifiers provide better accuracy when compared to the single classifier and ensemble based methods has shown good performance compared to strong single learners when dealing with concept drift and class imbalance data.																	1868-5137	1868-5145															10.1007/s12652-020-01934-y		APR 2020											
J								Different loading condition and angle measurement of human lumbar spine MRI image using ANSYS	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										MRI; Meshing; Stress; Strain; Total deformation; Finite element method		Every living organism is happiest and can do their daily routine when they are hale and hearty. Man is no exception. Health is his basic concern, apart from food, water, and shelter. To a human being, health is not only connected to the body, but it also connected to the mind. "A sound mind is in a sound body". To maintain one's health, various factors such as weather, mental well-being, proper rest, nutritious food, and a suitable living place contribute to a great extent. Apart from these, the workplace is also an important factor for consideration. Here also, blue-collar workers face a lot of health concerns due to the nature of their work, which involves carrying, lifting, loading, and unloading heavy objects every day. Industries and factories mostly do not adhere to any prescribed norms for extracting this kind of labor. This might be due to their target-based schedule and budget constraints. Most industries grow at the cost of the laborers' health. The laborers too ignore their health and risk their lives. As he or she endowed with the sixth sense, they can use it successfully so long as the body is healthy. In image processing, the 3D image generation models simulated. The stress test performed using ANSYS for the images generated from MATLAB. Initially, the performances of all the Edge Detection Algorithm (EDA) s were analyzed using the three parameters, namely average magnitude, initial position, and the threshold value. From the analysis, it was explicit that the canny EDA outperformed other EDAs. The Canny edge detection method showed remarkable results with average magnitude = 16.27, initial position = 8.59, and threshold = 8.61. The outcome of the above mentioned pre-processes converted into a 3D model using CAD and CATIA. Thus, the converted 3D model was put into analysis using ANSYS software to obtain the essential parameters, such as NS, MMPS, MMPES, EES, ES, NES, SES, SS, and TD, for identifying the maximum safest angle at which the lumbar spine remains with much less stress, strain, and TD of the worked-out 11 parameters, the 3 parameters such as TD, equivalent (Von Mises) stress and EES were selected to study and investigate the suitable degree at which the HLS remains unaffected and can function for a longer period in general. From the analysis, it is evident that the HLS of a person can bear the load weighing between 50 and 70 kg. The result is as follows: TD 8.0061e - 006 m, ES 3.3655e + 006 Pa and ESS 2.1051e - 004 m/m. This research has successfully proved that a 90 degrees angle is the best for lifting a load, thereby reducing stress, strain, and deformation in the long run.																	1868-5137	1868-5145															10.1007/s12652-020-01939-7		APR 2020											
J								RetrieveNet: a novel deep network for medical image retrieval	EVOLUTIONARY INTELLIGENCE										Content based image retrieval; Deep network; Feature extraction; Index matching	LOCAL TERNARY PATTERNS; FEATURE DESCRIPTOR; BINARY PATTERNS; TEXTURE; COLOR; WAVELET; CLASSIFICATION; DATABASE; DESIGN; MRI	Content-Based Image Retrieval is an accurate characterization of visual information used for medical and natural image classification, retrieval, face recognition, etc. In recent years, deep networks achieved state-of-the-art accuracy in various vision tasks. In this paper, we propose an end-to-end approach for medical image retrieval. The proposed approach comprises a novel deep network for classification of input query image followed by the retrieval module to retrieve the images belongs to the query image class. The proposed network comprises of multi-scale filter bank for robust feature extraction. We make use of skip connections to share the initially learned features across the network. The performance of the proposed network for medical image retrieval is validated in terms of precision and recall on three publicly available medical image databases. We validated the proposed approach for two different image modalities, namely CT scans and MRI scans. Performance evaluation depicts that proposed RetrieveNet outperforms other existing methods for medical image retrieval.																	1864-5909	1864-5917															10.1007/s12065-020-00401-z		APR 2020											
J								Compliant Finger Exoskeleton with Telescoping Super-elastic Transmissions	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Finger exoskeleton; Hand exoskeleton; Super-elastic transmission; Rehabilitation; Ni-Ti rod	VARIABLE STIFFNESS ACTUATORS; HAND EXOSKELETON; DESIGN; REHABILITATION; PERFORMANCE	This paper presents the concept, design, and experimental characterization of a compliant finger exoskeleton with telescoping super-elastic transmissions. Nickel-Titanium (Ni-Ti) rods with the super-elastic feature are adopted as transmission components for the concentric telescoping mechanism to be flexible and safe. The mechanism of this finger exoskeleton is characterized in detail, including the finger connector, the elastic-rod transmission system (ERTS), and the actuator. The performance of compliance is demonstrated by finite element analysis. Then, the finger motion mapping relationship is captured by the experiments for a two-finger prototype. Experiment results show that certain compliance (e.g., 16(circle)/2N for the flexion/extension motion of the index finger) can be achieved by the Ni-Ti rods, and reflects in the assisted motion.																	0921-0296	1573-0409				NOV	2020	100	2					435	444		10.1007/s10846-020-01186-0		APR 2020											
J								Supervised deep semantics-preserving hashing for real-time pulmonary nodule image retrieval	JOURNAL OF REAL-TIME IMAGE PROCESSING										Deep learning; Semantics-preserving hashing; Pulmonary nodule; Real-time image retrieval	LUNG NODULES; FEATURES	Hashing-based medical image retrieval has drawn extensive attention recently, which aims at providing effective aided diagnosis for medical personnel. In the paper, a novel deep hashing framework is proposed in the medical image retrieval, where the processes of deep feature extraction, binary code learning, and deep hash function learning are jointly carried out in supervised fashion. Particularly, the discrete constrained objective function in the hash code learning is optimized iteratively, where the binary code can be directly solved with no need for relaxation. In the meantime, the semantic similarity is maintained by fully exploring supervision information during the discrete optimization, where the neighborhood structure of training data is preserved by applying a graph regularization term. Additionally, to gain the fine-grained ranking of the returned medical images sharing the same Hamming distance, a novel image re-ranking scheme is proposed to refine the similarity measurement by jointly considering Euclidean distance between the real-valued feature descriptors and their category information between those images. Extensive experiments on the pulmonary nodule image dataset demonstrate that the proposed method can achieve better retrieval performance over the state of the arts.																	1861-8200	1861-8219															10.1007/s11554-020-00963-2		APR 2020											
J								Empower rumor events detection from Chinese microblogs with multi-type individual information	KNOWLEDGE AND INFORMATION SYSTEMS										Rumor events detection; Dynamic time series; Individual information; Sentiment dictionary; GRU; Chinese microblogs		Online social media has become an ideal place in spreading rumor events with its convenience in communication and information dissemination, which raises the difficulty in debunking rumor events automatically. To deal with such a challenge, traditional classification approaches relying on manually labeled features have to face a daunting number of human efforts. With the consideration of the realness of a rumor event, it will be verified and authenticated with multi-type individual information, especially with individuals' emotional expressions to events and their own credibility. This paper presents a novel two-layer GRU model for rumor events detection based on multi-type individual information (MII) and a dynamic time-series (DTS) algorithm, named as MII-DTS-GRU. Specifically, MII refers to adopt the sentiment dictionary to identify fine-grained human emotional expressions to events and fuse with the individual credibility. Besides, the DTS algorithm retains the time distribution of social events. Experimental results on Sina Weibo datasets show that our model achieves a high accuracy of 96.3% and demonstrate that our proposed MII-DTS-GRU model outperforms the state-of-the-art models on rumor events detection.																	0219-1377	0219-3116				SEP	2020	62	9					3585	3614		10.1007/s10115-020-01463-2		APR 2020											
J								Robust face tracking using multiple appearance models and graph relational learning	MACHINE VISION AND APPLICATIONS										Face tracking; Multiple appearance models; L2-subspace; Graph relational learning; Weighted fusion		This paper addresses the problem of appearance matching across different challenges while doing visual face tracking in real-world scenarios. In this paper, FaceTrack is proposed that utilizes multiple appearance models with its long-term and short-term appearance memory for efficient face tracking. It demonstrates robustness to deformation, in-plane and out-of-plane rotation, scale, distractors and background clutter. It integrates on the advantages of the tracking-by-detection by using a face detector that tackles the drastic scale appearance change of a face. A weighted score-level fusion strategy is proposed to obtain the face tracking output having the highest fusion score by generating candidates around possible face locations. FaceTrack showcases impressive performance when initiated automatically by outperforming several state-of-the-art trackers, except Struck by a very minute margin: 0.001 in precision and 0.017 in success, respectively.																	0932-8092	1432-1769				APR 11	2020	31	4							23	10.1007/s00138-020-01071-8													
J								Neural network-based design and evaluation of performance metrics using adaptive line enhancer with adaptive algorithms for auscultation analysis	NEURAL COMPUTING & APPLICATIONS										Auscultation; Normalized least mean square (NLMS); Adaptive line enhancer (ALE); Neural networks (NN); Performance metrics	NOISE; HEART	Auscultation is an important key for the physical (respiratory and circulatory) examination and is helpful in diagnosing various disorders. Auscultation is performed for the purposes of examining the circulatory and respiratory sounds and gastrointestinal system (bowel sounds). Besides inconsistencies in the propagation of the normal sounds, there are also several types of specific irregularities that can be heard in respiratory sounds: commonly known abnormal sounds in lung sound (wheezes, crackles, stridor, squawks, rhonchi and crackles) and heart sounds (heart murmurs). However, detection of abnormal sounds during auscultation needs extensive training and experience. Real-time separation of these heart sound signals from the lung sound signals is of great research interest and difficult to achieve. In this work, the authors proposed a novel adaptive line enhancer using nonlinear ANN design used for auscultation analysis. Our proposed designs are trained using different networks training algorithms which resulted in better mean square error reduction within compact time.																	0941-0643	1433-3058				SEP	2020	32	18			SI		15131	15153		10.1007/s00521-020-04864-0		APR 2020											
J								Robust hybrid data-level sampling approach to handle imbalanced data during classification	SOFT COMPUTING										Class imbalance; Outlier identification; Kernel approach; Hybrid approach; Firefly concept; Imbalanced data-set	CLUSTERING-ALGORITHM; DATA-SETS; SMOTE; SYSTEM; IDENTIFICATION; PREDICTION	Classification process is significant in finding different patterns from data. The performance of classifiers is highly affected with many data impurities like imbalance data, noise, class overlapping and different distributions of data within classes. The data in the real-world applications are often corrupted with multiple data impurities. To handle this issue, this paper proposed a hybrid data-level method to handle multiple data impurities like class imbalance, noise and different data distributions within classes. The proposed approach works in phases; in the first phase, it identifies and removes noise from the data, and then, it detects minority and majority cluster by using kernel-based fuzzy clustering approach. Radial basis kernel is used for clustering. In the next phase, minority and majority clusters are processed to balance the data. It uses radial basis kernel fuzzy membership and alpha-cut to reduce the data size of majority cluster- and firefly-based SMOTE method to intelligently produce synthetic data within minority cluster. After removing all the data impurities, a traditional classifier (Decision Tree) is used to classify the balanced data. Performance of proposed method is tested with 3 synthetic data-sets and 44 UCI real-world data-sets of different imbalance ratios (imbalance ratio varies from 1.82 to 129.44). Area under the ROC curve is used to assess and compare the performance of proposed method with 20 other data-level methods. Experimental results confirmed that proposed method outperformed every other method especially in the case of highly imbalanced data-set.																	1432-7643	1433-7479				OCT	2020	24	20					15715	15732		10.1007/s00500-020-04901-z		APR 2020											
J								A new ensemble feature selection approach based on genetic algorithm	SOFT COMPUTING										Ensemble feature selection; Optimization problem; Genetic algorithm	DEMAND; MODEL	In the ensemble feature selection method, if the weight adjustment is performed on each feature subset used, the ensemble effect can be significantly different; therefore, how to find the optimized weight vector is a key and challenging problem. Aiming at this optimization problem, this paper proposes an ensemble feature selection approach based on genetic algorithm (EFS-BGA). After each base feature selector generates a feature subset, the EFS-BGA method obtains the optimized weight of each feature subset through genetic algorithm, which is different from traditional genetic algorithm directly processing single features. We divide the EFS-BGA algorithm into two types. The first is a complete ensemble feature selection method; based on the first, we further propose the selective EFS-BGA model. After that, through mathematical analysis, we theoretically explain why weight adjustment is an optimization problem and how to optimize. Finally, through the comparative experiments on multiple data sets, the advantages of the EFS-BGA algorithm in this paper over the previous ensemble feature selection algorithms are explained in practice.																	1432-7643	1433-7479				OCT	2020	24	20					15811	15820		10.1007/s00500-020-04911-x		APR 2020											
J								Accurate quaternion radial harmonic Fourier moments for color image reconstruction and object recognition	PATTERN ANALYSIS AND APPLICATIONS										Radial harmonic Fourier moments; Geometric error; Numerical error; Quaternion; Object recognition	SCALE INVARIANTS; ZERNIKE MOMENTS; MELLIN MOMENTS; WATERMARKING; TRANSLATION	Orthogonal moments have become a powerful tool for object representation and image analysis. Radial harmonic Fourier moments (RHFMs) are one of such image descriptors based on a set of orthogonal projection bases, which outperform other moments because of their computational efficiency. However, the conventional computational framework of RHFMs produces geometric error and numerical integration error, which will affect the accuracy of RHFMs, thus degrading the image reconstruction performance. To overcome this shortcoming, we propose a new computational framework of RHFMs, namely accurate quaternion radial harmonic Fourier moments (AQRHFMs), for color image processing, and also analyze the properties of AQRHFMs. Firstly, we propose a precise computation method of RHFMs to reduce the geometric and numerical errors. Secondly, by using the algebra of quaternions, we extend the accurate RHFMs to AQRHFMs in order to deal with the color images in a holistic manner. Experimental results show the proposed AQRHFMs achieve promising performance in image reconstruction and object recognition in both noise-free and noisy conditions.																	1433-7541	1433-755X				NOV	2020	23	4					1551	1567		10.1007/s10044-020-00877-6		APR 2020											
J								Nature-inspired algorithm-based secure data dissemination framework for smart city networks	NEURAL COMPUTING & APPLICATIONS										Optimization; Algorithm; Security; Detection; Living; Mobility; Standards	SUSTAINABLE CITIES; INTERNET; SYSTEMS	Unceasing population growth and urbanization have intensified the traditional systems to deal with citizen lifestyle, environment, economic issues and good governess. New communication technologies have played a vital role in changes traditional urbanization into a smarter and comfort zone for the citizen. Due to various systems and integration of several new standards and systems, the smart cities have suffered from various open challenges related to technologies, system controlling and management, scalability and security concerns. The new concepts of nature-inspired solutions have implemented to deal with smart cities' challenges by more optimization and performance-oriented methods. Therefore, this paper aims to handle at least three areas of smart cities including smart mobility, smart living and security provision by developing three nature-inspired solutions. The three proposed solutions are dragon clustering mobility in IoV, moth flame electric management for smart living and ant colony-based intrusion detection system for security provision. These solutions are based on a dragonfly, moth flame and ant colony optimization techniques. The proposed solutions are evaluated in a simulation to check the performance. These solutions will help new researchers to explore the nature-inspired solutions to tackle the new and complex systems of smart cities.																	0941-0643	1433-3058															10.1007/s00521-020-04900-z		APR 2020											
J								Development of virtual reality rehabilitation games for children with attention-deficit hyperactivity disorder	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Attention-deficit hyperactivity disorder (ADHD); Virtual reality (VR); Serious game; Attention; Cognitive ability	PHYSICAL-ACTIVITY; EXECUTIVE FUNCTION; DEFICIT/HYPERACTIVITY DISORDER; ACADEMIC-ACHIEVEMENT; EXERCISE; ADHD; BRAIN; PERFORMANCE; DIAGNOSIS; FITNESS	Balance and coordination exercises improve the attention of children with attention-deficit hyperactivity disorder (ADHD), and exercise-based game treatments are effective in training children's balance control and coordination and improving their cognition and intelligence. This study used immersive virtual reality exercise games as an intervention in rehabilitation to improve the attention, cognitive ability, abstract reasoning, and complex information processing of children with ADHD. This study is comprised of two stages: In the first stage, we interviewed experts in rehabilitation, made observations of rehabilitation sessions for children with ADHD, and developed three games focusing on training body coordination. In the second stage, we used the HTC VIVE (HTC, Taiwan), a virtual reality game console, in a 3-month training program developed for children with ADHD. We compared children's attention, cognitive ability, abstract reasoning, and complex information processing before and after this program. The results revealed that children with ADHD improved their performance in attention, hyperactivity/impulsivity, and oppositional defiance. Our results may serve as a reference for the clinical use of technological assistive devices in rehabilitation as well as for families, schools, and rehabilitation institutes in implementing daily training and rehabilitation programs for children with ADHD.																	1868-5137	1868-5145															10.1007/s12652-020-01945-9		APR 2020											
J								A hybridization of deep learning techniques to predict and control traffic disturbances	ARTIFICIAL INTELLIGENCE REVIEW										Deep learning; Convolutional neural networks; Long short term memory; Immune memory algorithm; Reinforcement learning; Case-based reasoning; Preemptive LQF-MWM; Traffic signal control system	SIGNALIZED INTERSECTIONS; AGGREGATION; NETWORKS; PRIORITY; IMAGES	Predicting traffic disturbances is a challenging problem in urban cities. Emergency vehicles (EV) is one of the biggest disturbances that affect traffic fluidity. The goal of this paper is to provide a machine learning application to deal with emergency cases in traffic networks. Particularly, we investigate the use of deep learning techniques coupled with Artificial Immune System to tackle the issue of EV guidance at signalized intersections. To accomplish this goal, we develop a traffic signal control system capable to estimate traffic status, guide EV to reach their destinations while assuming better traffic condition, control traffic signals, and adapt to new disturbances. For traffic forecasting, the suggested system inherits the advantages of convolutional neural networks, classification, and long short term memory. To control traffic signals, the suggested system uses the immune memory algorithm. To enhance and adapt control decisions to traffic disturbances, the suggested system uses a continuous learning approach assumed by an adapted reinforcement learning algorithm. Assessments using well-known algorithms from the literature are detailed in this work. The benchmarking algorithms are the preemptive longest queue first matching weight matrix system, the pre-emptive immune memory algorithm inspired case-based reasoning, and the preemptive optimized stage based fixed time algorithm. Experiments show a competitive performance of the suggested system compared to benchmarking algorithms.																	0269-2821	1573-7462				DEC	2020	53	8					5675	5704		10.1007/s10462-020-09831-8		APR 2020											
J								Efficiencya Assessment and Target Setting Using a Fully Fuzzy DEA Approach	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Efficiency assessment; Fuzzy data; Fully fuzzy linear programming; Multiobjective optimization	DATA ENVELOPMENT ANALYSIS	Data envelopment analysis (DEA) is a non-parametric methodology for efficiency assessment. This paper proposes a new radial, input-oriented and fully fuzzy DEA approach, based on an LU-fuzzy partial order (L for lower, U for upper), for assessing the relative efficiency of a set of Decision-Making Units (DMUs). The proposed approach involves a radial input contraction, Phase I, and an additive slacks maximization, Phase II. Each phase is first formulated as a fully fuzzy linear programming (FFLP), and then it is transformed into a multiobjective optimization problem. The latter is solved using the lexicographic weighted Tchebycheff method. The proposed fully fuzzy DEA approach provides, for each unit, a fuzzy efficiency measure and a fuzzy target operating point. A classification of the efficiency status of the units is also presented. Computational experiences and comparison with other fuzzy DEA approaches are reported.																	1562-2479	2199-3211				JUN	2020	22	4					1056	1072		10.1007/s40815-020-00821-0		APR 2020											
J								Analysis of the Ebola Outbreak in 2014 and 2018 in West Africa and Congo by Using Artificial Adaptive Systems	APPLIED ARTIFICIAL INTELLIGENCE											EPIDEMIC	In this manuscript the Ebola outbreaks in 2014 and 2018 have been studied. On March 23, 2014, the World Health Organization announced the beginning of the Ebola outbreak in West Africa. The initial location was in a forested area in the south eastern portion of Guinea. We used three different methods to determine the origin of the outbreak. The first was a suite of artificial adaptive systems called Topological Weighted Centroid which located the outbreak origin at Longitude: -10.5337, Latitude: 8.1517. This area is 64 km from Guekedou, Guinea. We also used a Dynamic Naive Bayesian/Dynamic Networks Block Algorithm. The Bayesian algorithm shows the main source of the Ebola outbreak at Kissidougou. Both of these methods revealed the outbreak started in the forested area southeast of Guinea. The distance between Guekedou and Kissidougou is about 69 km. Furthermore, we used an artificial neural network (ANN) called Selfie to predict the outbreak diffusion. The Ebola outbreak in May 2018 in Democratic Republic of Congo was not as widespread as the outbreak in 2014. The outbreak was effecting the health zones of Bikoro and Iboko, and Wangata in Congo. We have used an ANN algorithm and predicted the origin of the outbreak at (Longitude: 18.3046, Latitude: -0.6865) in the Equator about 20 km at North-West of Bikoro.																	0883-9514	1087-6545				JUL 2	2020	34	8					597	617		10.1080/08839514.2020.1747770		APR 2020											
J								A consensus-based approach for multi-criteria decision making with probabilistic hesitant fuzzy information	SOFT COMPUTING										Multi-criteria decision making; Euclidean distance; Group consensus; Probabilistic hesitant fuzzy element	PREFERENCE RELATIONS; MODEL	As a generalized fuzzy number, probabilistic hesitant fuzzy element (PHFE) improves the flexibility for decision makers in expressing hesitant information, and it has been receiving increased attention. This study develops a multi-criteria decision-making (MCDM) approach that considers consensus reaching among decision makers with probabilistic hesitant fuzzy information. To obtain this aim, first, a new approach to derive normalized PHFE (NPHFE) is proposed to overcome the shortcomings in previous studies. Subsequently, a new Euclidean distance and some operations related to PHFEs are developed based on the new proposed NPHFEs. At the same time, the effectiveness and rationality of the new proposed approaches are discussed. Second, a consensus index of group with PHFEs is presented, which based on the proposed Euclidean distance of decision-makers' evaluation information on all the criteria. Third, if the consensus level of the group does not reach the expect threshold value, an iteration algorithm is designed to improve its consensus level. Moreover, the proof of the convergence of the proposed algorithm is provided to verify its effectiveness, and a MCDM approach based on group consensus is proposed. Finally, the most comprehensive candidate selection problems are provided to demonstrate the effectiveness of the proposed MCDM approach. And a comparative study with other methods is conducted with the same illustrative example.																	1432-7643	1433-7479				OCT	2020	24	20					15577	15594		10.1007/s00500-020-04886-9		APR 2020											
J								Optimum profit-driven churn decision making: innovative artificial neural networks in telecom industry	NEURAL COMPUTING & APPLICATIONS										Artificial neural networks (ANNs); Profit-driven churn prediction; Self-organizing map (SOM); Self-organizing error-driven ANN (SOEDANN)	CUSTOMER LIFETIME VALUE; CLASS IMBALANCE PROBLEM; TELECOMMUNICATION SECTOR; PREDICTION MODEL; RANDOM FOREST; CLASSIFICATION; INFORMATION; MACHINE; IMPACT	Knowledge-based churn prediction and decision making is invaluable for telecom companies due to highly competitive markets. The comprehensiveness and action ability of a data-driven churn prediction system depend on the effective extraction of hidden patterns from the data. Generally, data analytics is employed to extrapolate the extracted patterns from the training dataset to the test set. In this study, one more step is taken; the improved prediction performance is attained by capturing the individuality of each customer while discovering the hidden pattern from the train set and then applying all the knowledge to the test set. The proposed churn prediction system is developed using artificial neural networks that take advantage of both self-organizing and error-driven learning approaches (ChP-SOEDNN). We are introducing a new dimension to the study of churn prediction in telecom industry: a systematic and profit-driven churn decision-making framework. The comparison of the ChP-SOEDNN with other techniques shows its superiority regarding both accuracy and misclassification cost. Misclassification cost is a realistic criterion this article introduces to measure the success of a method in finding the best set of decisions that leads to the minimum possible loss of profit. Moreover, ChP-SOEDNN shows capability in devising a cost-efficient retention strategy for each cluster of customers, in addition to strength in dealing with the typical issue of imbalanced class distribution that is common in churn prediction problems.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14929	14962		10.1007/s00521-020-04850-6		APR 2020											
J								Using deep learning for short-term load forecasting	NEURAL COMPUTING & APPLICATIONS										Short-term load forecasting; Convolutional Neural Network; Deep learning; Artificial intelligence	FUNCTION APPROXIMATION; REGRESSION; MODELS	Electricity is the most important source of energy that is exploited nowadays; it is essential for the economic development and the social stability, and this implies the need to model systems that keeps a perfect balance between supply and demand. This task depends heavily on identifying the factors that affect power consumption and improving the precision of the forecasted model. This paper presents a novel convolutional neural network (CNN) for short-term load forecasting (STLF); studies have been conducted to identify the different factors that affect the power consumption in Algeria (North Africa), and these studies helped to determine the inputs to the model. The proposed CNN uses a two-dimensional input unlike the conventional one-dimensional input used for STLF, and the results given by the CNN were compared to other artificial intelligence methods and demonstrated good results for both: one-quarter-ahead and 24-h-ahead forecast.																	0941-0643	1433-3058				SEP	2020	32	18			SI		15029	15041		10.1007/s00521-020-04856-0		APR 2020											
J								Echo state network based on improved fruit fly optimization algorithm for chaotic time series prediction	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Chaotic time series; Prediction; Echo state network; Improved fruit fly optimization algorithm	MODEL; PARAMETERS; MACHINE; SYSTEMS	Chaos is a common phenomenon in nature and society. Chaotic system affects many fields. It is of great significance to find out the regularity of chaotic time series from chaotic system. Chaotic system has extremely complex dynamic characteristics and unpredictability. The traditional prediction methods for chaotic time series have some problems, such as low accuracy, slow convergence speed and complex model structure. In this paper, an echo state network prediction method based on improved fruit fly optimization algorithm for chaotic time series is proposed. The phase space reconstruction is introduced for the prediction of chaotic time series. The C-C method is used to determine the delay time. The embedding dimension is obtained by the G-P method. After reconstructing the phase space of the chaotic time series, an improved echo state network is proposed as the prediction model. In order to improve the prediction accuracy, an improved fruit fly optimization algorithm is proposed to optimize the parameters of the prediction model. Three typical chaotic time series, including Lorenz, Mackey-Glass, and short-term wind speed, are selected as simulation objects. The simulation results show that the prediction method proposed in this paper has good prediction indicators. At the same time, the results of the reliability and Pearson's test also show the better predictive effect.																	1868-5137	1868-5145															10.1007/s12652-020-01920-4		APR 2020											
J								TimeFly algorithm: a novel behavior-inspired movie recommendation paradigm	PATTERN ANALYSIS AND APPLICATIONS										Recommendation system; Collaborative filtering; Scalability; Time series analysis; E-commerce	MATRIX FACTORIZATION	This paper proposes a novel behavior-inspired recommendation algorithm named TimeFly algorithm, which works on the idea of altering behavior of the user with respect to time. The proposed model considers solving two recommendation problems (fluctuating user interest over time and high computation time when dataset shifts from scarcity to abundance) and presents a real application of the proposed method in the field of recommendation engine. It describes a system which enrolls the changing behavior of user to furnish personalization suggestions. The results obtained by TimeFly are compared with the results of other well-known algorithms. Simulation results on 100K, 1M, 10M, and 20M MovieLens dataset reveal that using TimeFly leads to high accurate predictions in less computation time.																	1433-7541	1433-755X				NOV	2020	23	4					1727	1734		10.1007/s10044-020-00883-8		APR 2020											
J								Optimal feature selection-based diabetic retinopathy detection using improved rider optimization algorithm enabled with deep learning	EVOLUTIONARY INTELLIGENCE										Diabetic retinopathy diagnosis; retinal abnormalities; Optimal feature selection; Deep belief network; Modified gear and steering-based rider optimization algorithm	LOCAL BINARY PATTERNS; AUTOMATED DETECTION; FEATURE-EXTRACTION; GENETIC ALGORITHM; CLASSIFICATION; LESIONS; MODEL	This proposal tempts to develop automated DR detection by analyzing the retinal abnormalities like hard exudates, haemorrhages, Microaneurysm, and soft exudates. The main processing phases of the developed DR detection model is Pre-processing, Optic Disk removal, Blood vessel removal, Segmentation of abnormalities, Feature extraction, Optimal feature selection, and Classification. At first, the pre-processing of the input retinal image is done by Contrast Limited Adaptive Histogram Equalization. The next phase performs the optic disc removal, which is carried out by open-close watershed transformation. Further, the Grey Level thresholding is done for segmenting the blood vessels and its removal. Once the optic disk and blood vessels are removed, segmentation of abnormalities is done by Top hat transformation and Gabor filtering. Further, the feature extraction phase is started, which tends to extract four sets of features like Local Binary Pattern, Texture Energy Measurement, Shanon's and Kapur's entropy. Since the length of the feature vector seems to be long, the feature selection process is done, which selects the unique features with less correlation. Moreover, the Deep Belief Network (DBN)-based classification algorithm performs the categorization of images into four classes normal, earlier, moderate, or severe stages. The optimal feature selection is done by the improved meta-heuristic algorithm called Modified Gear and Steering-based Rider Optimization Algorithm (MGS-ROA), and the same algorithm updates the weight in DBN. Finally, the effectual performance and comparative analysis prove the stable and reliable performance of the proposed model over existing models. The performance of the proposed model is compared with the existing classifiers, such as, NN, KNN, SVM, DBN and the conventional Heuristic-Based DBNs, such as PSO-DBN, GWO-DBN, WOA-DBN, and ROA-DBN for the evaluation metrics, accuracy, sensitivity, specificity, precision, FPR, FNR, NPV, FDR, F1 score, and MC. From the results, it is exposed that the accuracy of the proposed MGS-ROA-DBN is 30.1% higher than NN, 32.2% higher than KNN, and 17.1% higher than SVM and DBN. Similarly, the accuracy of the developed MGS-ROA-DBN is 13.8% superior to PSO, 5.1% superior to GWO, 10.8% superior to WOA, and 2.5% superior to ROA.																	1864-5909	1864-5917															10.1007/s12065-020-00400-0		APR 2020											
J								Study on the evolution of hot topics in the urban development	EVOLUTIONARY INTELLIGENCE										Urban studies; Hot topics; Topic model; Anomaly detection	ANOMALY DETECTION; WORD; TECHNOLOGY	Urbanization is crucially important for people to improve the quality of life. Thus, it is of importance to study the evolution of hot topics to explore the functions of cities for meeting the increasing demands of people. In this paper, we explored the semantic analysis of hot topics and trends in urban studies from the literature, which provides a research direction for future studies. Based on articles collected from the Science Citation Index Expanded and Conference Proceedings Citation Index-Science databases from 2000 to 2016, we found that the number of urban studies increased in stability during that time. Followed by England and China, USA was the largest contributor for studies in this field. Based on the keywords and abstracts of these articles, we extracted the topics of the study using a clustering method and topic model, and calculated the hot values of the topics. Finally, we obtained 15 hot topics in the field of urban studies, among which "city", "school", "regional economic", and "estate" were the hottest topics that indicated the focus of the research study. An anomaly detection method was used to analyze the change trend of topics' hot values, and we found that the hot value of these topics overall were on the rise, especially "urban education" and "urban planning" increased significantly, which indicated that they attracted an increasing amount of scholars' attention, but the hot value of "health" and "Gis" decreased significantly recently, which suggested that research interest in these two topics is decreasing.																	1864-5909	1864-5917															10.1007/s12065-020-00391-y		APR 2020											
J								Downsizing and enhancing broad learning systems by feature augmentation and residuals boosting	COMPLEX & INTELLIGENT SYSTEMS										Broad learning system (BLS); Stacked structure; Boosting; Functional link neural network (FLNN); Incremental learning	REGRESSION; MODEL	Recently, a broad learning system (BLS) has been theoretically and experimentally confirmed to be an efficient incremental learning system. To get rid of deep architecture, BLS shares the same architecture and learning mechanism of the well-known functional link neural networks (FLNN), but works in broad learning way on both the randomly mapped features of original features of data and their randomly generated enhancement nodes. As such, BLS often requires a huge heap of hidden nodes to achieve the prescribed or satisfactory performance, which may inevitably cause both overwhelming storage requirement and overfitting phenomenon. In this study, a stacked architecture of broad learning systems called D&BLS is proposed to achieve enhanced performance and simultaneously downsize the system architecture. By boosting the residuals between previous and current layers and simultaneously augmenting the original input space with the outputs of the previous layer as the inputs of current layer, D&BLS stacks several lightweight BLS sub-systems to guarantee stronger feature representation capability and better classification/regression performance. Three fast incremental learning algorithms of D&BLS are also developed, without the need for the whole re-training. Experimental results on some popular datasets demonstrate the effectiveness of D&BLS in the sense of both enhanced performance and reduced system architecture.																	2199-4536	2198-6053				JUL	2020	6	2					411	429		10.1007/s40747-020-00139-2		APR 2020											
J								Cosine adapted modified whale optimization algorithm for control of switched reluctance motor	COMPUTATIONAL INTELLIGENCE										cosine adapted modified whale optimization algorithm; proportional integral controller; switch reluctance motor; torque ripple; whale optimization algorithm	TORQUE RIPPLE REDUCTION; SPEED CONTROL; HYBRID; GENERATOR	Whale optimization algorithm (WOA) imitates social conduct of humpback whales which is inspired by bubble net hunting strategy of humpback whales. In the present study, Cosine adapted modified whale optimization algorithm (CamWOA) which is a modified version of WOA, has been proposed where cosine function is incorporated for the selection of control parameter "d" which governs the position of whales during optimization process. Also, correction factors are employed to modify the movement of search agents during the search process. These changes provide a proper balance between exploration and exploitation phases in CamWOA technique. The performance of CamWOA is analyzed by testing on a set of benchmark functions and compared with other state-of-the-art algorithms. It is observed that CamWOA outperforms other state-of-the-art metaheuristic algorithms in majority of benchmark functions. The efficiency of CamWOA is also evaluated by solving a multiobjective engineering problem pertaining to control of switched reluctance motor. The simulation results confirm that CamWOA yields very promising and competitive results compared to that of WOA and other metaheuristic optimization algorithms.																	0824-7935	1467-8640															10.1111/coin.12310		APR 2020											
J								Investigations on adaptive connectivity and shape prior based fuzzy graph-cut colour image segmentation	EXPERT SYSTEMS										dynamic fuzzy graph cut; fuzzy graph cut; image segmentation; maximum-a-posterior; optimized adaptive connectivity and shape prior		Image segmentation is a challenging problem in computer vision with wide application. It is a process which considers the similarity criterion required to separate an image into different homogenous connected regions. First, an Optimized Adaptive Connectivity and Shape Prior in Modified Graph Cut Segmentation method has been applied to handle the structural irregularities in images. Second, an Optimized Adaptive Connectivity and Shape Prior in Modified Fuzzy Graph Cut Segmentation (Opac-MFGseg) is proposed to partition the images based on feature values. In this method, a fuzzy rule based system is used with optimization algorithm to provide the information on how much a specific feature is involved in image boundaries. The graph obtained from this fuzzy approach is further used in adaptive shape prior in modified graph cuts framework. Moreover, this method supports moving images (videos). In such a situation, a fully dynamic method called Optimized Adaptive Connectivity and Shape Prior in Dynamic Fuzzy Graph Cut Segmentation (Opac-DFGseg) method is proposed for the image segmentation. The effectiveness of the Opac-MFGseg and Opac-DFGseg methods is tested in terms of average sensitivity, precision, area overlap measure, relative error, and accuracy and computation time.																	0266-4720	1468-0394				OCT	2020	37	5			SI					10.1111/exsy.12554		APR 2020											
J								Global and local multi-view multi-label learning with incomplete views and labels	NEURAL COMPUTING & APPLICATIONS										Incomplete views and labels; Label-specific features; Label correlation; Partial pairwise constraints	MISSING LABELS; CLASSIFICATION	Multi-view multi-label learning is widely used in multiple fields, and it aims to process data sets represented by multiple forms (views) and labeled by multiple classes. But most real-world data sets maybe loss some labels and views due to lack of manpower and equipment failure and this causes some difficulties in processing data sets. In this paper, we develop a global and local multi-view multi-label learning with incomplete views and labels (GLMVML-IVL) to process this. In GLMVML-IVL, the usage of label-specific features indicates that class label is determined by some specific features rather than all features; global and local label correlations are taken into consideration with clustering technology; the construction of the pseudo-class label matrix offsets the defect of missing (partial) labels; the adoption of low-rank assumption matrix restores incomplete views; a consensus multi-view representation is put to use to encode the complementary information from different views; the regularizer imposed on label matrix reflects the partial pairwise constraints. Different from traditional methods, this is the first attempt to design a multi-view multi-label learning method with incomplete views and labels by the learning of label-specific features, pseudo-class label matrix, low-rank assumption matrix, global and local label correlations, complementary information, and regularizer imposed on label matrix. Experimental results validate that GLMVML-IVL improves the performance of traditional multi-view multi-label learning methods in statistical and achieves a better performance.																	0941-0643	1433-3058				SEP	2020	32	18			SI		15007	15028		10.1007/s00521-020-04854-2		APR 2020											
J								A low-cost multichannel NIRS oximeter for monitoring systemic low-frequency oscillations	NEURAL COMPUTING & APPLICATIONS										fNIRS; Circulation; Systemic low-frequency oscillations; MSP430	NEAR-INFRARED SPECTROSCOPY; CEREBRAL HEMODYNAMICS; OLDER-ADULTS; FLUCTUATIONS; STIMULATION; STATE	Systemic low-frequency oscillations (sLFOs) are non-neuronal oscillations at 0.01-0.15 Hz. These sLFOs travel through the entire body and the brain with symmetrical (across the midline of the body) and highly predictable delays, where they can be observed with functional near-infrared spectroscopy (fNIRS) and blood oxygen level-dependent functional magnetic resonance imaging. Their characteristics may serve as useful biomarkers for detecting and monitoring circulatory dysfunction. Pure sLFOs can be collected in the periphery (e.g., fingers, toes, earlobes). Here we present a 7-channel NIRS oximeter [MNO] for sLFOs detection and analysis in the periphery, which we named concurrent continuous wave fNIRS system (CON-CW fNIRS). Our CON-CW fNIRS is small (10 x 10 x 20 cm(3)), highly portable, has low-power consumption and is highly cost-effective (below $300). We show that our device is highly reliable and can reproduce values acquired with a commercial fNIRS device with direct comparison (r(max) = 0.908 increment [HbO] andr(max) = 0.841 increment [Hb]) and when compared to previously published data.																	0941-0643	1433-3058				OCT	2020	32	19			SI		15629	15641		10.1007/s00521-020-04897-5		APR 2020											
J								Automatic lung cancer detection from CT image using improved deep neural network and ensemble classifier	NEURAL COMPUTING & APPLICATIONS										Computer-aided detection (CAD); Improved deep neural network (IDNN); Hybrid swarm intelligent rough set approach; Ensemble classifier	DNA; SEGMENTATION	The development of the computer-aided detection system placed an important role in the clinical analysis for making the decision about the human disease. Among the various disease examination processes, lung cancer needs more attention because it affects both men and women, which leads to increase the mortality rate. Traditional lung cancer prediction techniques failed to manage the accuracy because of low-quality image that affects the segmentation process. So, in this paper new optimized image processing and machine learning technique is introduced to predict the lung cancer. For recognizing lung cancer, non-small cell lung cancer CT scan dataset images are collected. The gathered images are examined by applying the multilevel brightness-preserving approach which effectively examines each pixel, eliminates the noise and also increase the quality of the lung image. From the noise-removed lung CT image, affected region is segmented by using improved deep neural network that segments region in terms of using layers of network and various features are extracted. Then the effective features are selected with the help of hybrid spiral optimization intelligent-generalized rough set approach, and those features are classified using ensemble classifier. The discussed method increases the lung cancer prediction rate which is examined using MATLAB-based results such as logarithmic loss, mean absolute error, precision, recall andF-score.																	0941-0643	1433-3058															10.1007/s00521-020-04842-6		APR 2020											
J								Load prediction using (DoG-ALMS) for resource allocation based on IFP soft computing approach in cloud computing	SOFT COMPUTING										Difference of Gaussian; Load prediction; Soft computing approach; Improved flower pollination algorithm; Resource allocation; Function points; Workload		In today's world, most of the applications run with the service of cloud computing, which proceeds the process using the internet. In the case of cloud computing, based on customer needs, they may increase or decrease resource utilization. Virtualization is the process of multiplexing the resources from physical machines to virtual machines. However, it is challenging to prevent overloading for each physical machine of an automatic resources management system which affects virtualization to allocate the resources dynamically. To overcome these concerns, a new algorithm is proposed in this work, which can predict the future load precisely in the physical machine and decide which may be overloaded next. Then, the necessary action is taken to prevent overload in the system. In this work, the prediction of loads for allocating future resources is presented, and the dynamic scheduling and resource allocation for the predicted tasks are performed using IFPA. The difference of Gaussian-based adaptive least mean square filter is employed for predicting the loads function points which are used to estimate the complexity and cost rate. Also, a soft computing technique (improved flower pollination algorithm) is employed for the effective resource allocation strategy. The performance of the approach is intended and compared with other conventional works. The results proved that the work has better accuracy in load prediction and provide a way to allocate the resource precisely. At the same time, the traffic at the physical machines is significantly controlled.																	1432-7643	1433-7479				OCT	2020	24	20					15307	15315		10.1007/s00500-020-04864-1		APR 2020											
J								Application and research for electricity price forecasting system based on multi-objective optimization and sub-models selection strategy	SOFT COMPUTING										Electricity prices; Sub-models selection strategy; Combined model; Hybrid forecasting system	MARKET-CLEARING PRICE; NEURAL-NETWORK; WAVELET TRANSFORM; PREDICTION; LOAD; ANFIS	In general, electricity prices reflect the cost to build, finance, maintain, and operate power plants and the electricity grid. Therefore, the cost-optimized scheduling of industrial loads with accurate price forecasts is very important. As such, recent studies have attempted to combine models to forecast electricity prices more accurately. Earlier combined models have tended to ignore the selection of sub-models and data analyses, leading to poor forecasting performance. In order to select the best forecasting models in a combined model, we propose a hybrid electricity price forecasting system that includes a data analysis module, a sub-model selection strategy module, optimized forecasting processing, and a model evaluation module. As such, the hybrid system fully exploits the advantages of a single model, thus improving the forecasting performance of the combined model. The experimental results show that the proposed system selects optimal sub-models effectively and successfully identifies future trend changes in the electricity price. Thus, the system can be an effective tool in the planning and implementation of smart grids.																	1432-7643	1433-7479				OCT	2020	24	20					15611	15637		10.1007/s00500-020-04888-7		APR 2020											
J								Modeling and mechanism analysis of inertia and damping issues for wind turbines PMSG grid-connected system	SOFT COMPUTING										High proportion of renewable energy; Virtual inertia; Direct-drive wind turbines generation system; Inertia and damping characteristics; Power electronics-dominated power systems	ENERGY-STORAGE; POWER; IMPACT; GENERATORS; STABILITY	Aiming at the problem of global climate change and energy crisis, wind power has become the focus of energy sustainable development in all countries. The wind turbines (WTs) power system is connected to the grid via the power electronic converter, causing the system inertia level to drop. In this paper, the direct-drive WT system is considered as the research object, and the whole-system frequency response model is established. The inertia and damping characteristics of the WT converter systems with virtual inertia control are analyzed. With the support of fan rotor kinetic energy and the energy saved in a capacitor, the simple control can also make the system exhibit different degrees of inertia and damping features. The results show that the equivalent inertia and the WT inertia time constant, capacitance parameters and virtual control parametersk(d)are related; the equivalent damping parameter is related to the steady-state operating point parameters and the virtual control parameterk(p); the equivalent synchronization parameter is related to the steady-state operating point parameters and the virtual inertia control parameterk(i). Finally, the correctness of the inertial and damping characteristics of the WT grid-connected system is verified by simulation, which provides a theoretical reference for studying the inertial damping of power electronic dominant systems.																	1432-7643	1433-7479				OCT	2020	24	20					15681	15691		10.1007/s00500-020-04897-6		APR 2020											
J								A strategy of PI plus repetitive control for LCL-type photovoltaic inverters	SOFT COMPUTING										Repetitive controller; PI controller; PV inverter; Weighting coefficient; Harmonics suppression	CONTROL SCHEME; POWER; VOLTAGE	Due to the traditional grid-connected current control method of single Proportional Integral (PI) and Repetitive Control (RC) strategies, the photovoltaic inverter output current will have a distortion problem, which can not only maintain the stability of the whole photovoltaic system, but also the current quality of the photovoltaic inverter grid-connected system is reduced in the case of high-order LCL photovoltaic inverter control system operation. So, a strategy of PI + repetitive control in two-phase stationary frame is proposed. The introduction of the weighting coefficientmof the PI controller branch can enhance the adjustment ability of the PI link and accelerate the responding speed to meet the dynamic characteristics of the entire operating system, while the other weighting coefficientnon the repetitive controller branch can increase the correction capability and eliminate the steady-state error to meet system steady-state requirements. The scheme not only simplifies the coordinate transformation and decoupling calculation process, but also improves the harmonics suppression ability of the photovoltaic inverter and reduces the total harmonic distortion rate of the grid-connected current. At the same time, with the load operation mode change, the system can also retain better stability and keep a fast dynamic response capability, and the grid current can be able to return to a stable state in one cycle. Finally, the effectiveness of theoretical analysis and the strategy is verified by simulation.																	1432-7643	1433-7479				OCT	2020	24	20					15693	15699		10.1007/s00500-020-04898-5		APR 2020											
J								Prediction of geometry deviations in additive manufactured parts: comparison of linear regression with machine learning algorithms	JOURNAL OF INTELLIGENT MANUFACTURING										Additive manufacturing; PA12; Polyamide; Machine learning; Dimensional accuracy; Support vector regression; Decision tree regressor; Multilayer perceptron; Gradient boosting regressor	SHRINKAGE COMPENSATION; PROCESS PARAMETERS; OPTIMIZATION; ORIENTATION; MODEL	Dimensional accuracy in additive manufacturing (AM) is still an issue compared with the tolerances for injection molding. In order to make AM suitable for the medical, aerospace, and automotive industries, geometry variations should be controlled and managed with a tight tolerance range. In the previously published article, the authors used statistical analysis to develop linear models for the prediction of dimensional features of laser-sintered specimens. Two identical builds with the same material, process, and build parameters were produced, resulting in 434 samples for mechanical testing (ISO 527-2 1BA). The developed linear models had low accuracy, and therefore needed an application of more advanced data analysis techniques. In this work, machine learning techniques are applied for the same data, and results are compared with the previously reported linear models. The linear regression model is the best for width. Multilayer perceptron and gradient boost regressor models have outperformed other for thickness and length. The recommendations on how the developed models can be used in the future are proposed.																	0956-5515	1572-8145															10.1007/s10845-020-01567-0		APR 2020											
J								Performance comparison of five metaheuristic nature-inspired algorithms to find near-OGRs for WDM systems	ARTIFICIAL INTELLIGENCE REVIEW										Channel spacing; Conventional computing; Equally and unequally spaced channel allocation; Four-wave mixing; Metaheuristic; Nature-inspired algorithm; Near-optimal Golomb ruler; Optimization	CHANNEL ALLOCATION TECHNIQUE; GOLOMB; FREQUENCY; OPTIMIZATION; WALK	The metaheuristic approaches inspired by the nature are becoming powerful optimizing algorithms for solving NP-complete problems. This paper presents five nature-inspired metaheuristic optimization algorithms to find near-optimal Golomb ruler (OGR) sequences in a reasonable time. In order to improve the search space and further improve the convergence speed and optimization precision of the metaheuristic algorithms, the improved algorithms based on mutation strategy and Levy-flight search distribution are proposed. These two strategies help the metaheuristic algorithms to jump out of the local optimum, improve the global search ability so as to maintain the good population diversity. The OGRs found their potential application in channel-allocation method to suppress the four-wave mixing crosstalk in optical wavelength division multiplexing systems. The results conclude that the proposed algorithms are superior to the existing conventional computing algorithms i.e. extended quadratic congruence and search algorithm and nature-inspired optimization algorithms i.e. genetic algorithms, biogeography based optimization and simple big bang-big crunch to find near-OGRs in terms of ruler length, total optical channel bandwidth and computation time. The idea of computational complexity for the proposed algorithms is represented through the Big O notation. In order to validate the proposed algorithms, the non-parametric statistical Wilcoxon analysis is being considered.																	0269-2821	1573-7462				DEC	2020	53	8					5589	5635		10.1007/s10462-020-09829-2		APR 2020											
J								A direct consistency test and improvement method for the analytic hierarchy process	FUZZY OPTIMIZATION AND DECISION MAKING										Multiple criteria decision making; Analytical hierarchy process; Pairwise comparison matrix; Consistency test; 3 tuples	PAIRWISE COMPARISON METHOD; COMPARISON MATRIX; INCONSISTENT; PRIORITIES; DECISION; WEIGHTS; AHP	The consistency test is a vital component of pairwise comparison matrices if meaningful results are to be guaranteed, and it has been studied extensively since the analytic hierarchy process was developed by Saaty. However, when using the existing methods, it is imperative to carry out matrix operations, which are usually not intuitional. In this paper, a direct method, from the perspective of 3 tuples, is proposed to test and improve the consistency of a pairwise comparison matrix, which is far more intuitional and easier to understand. In the proposed 3 tuples iterative method, only simple mathematical calculations are needed, without the need for matrix operations. Furthermore, the calculation of Saaty's consistency ratio is only conducted to verify the proposed method. Some related theorems and propositions are proved mathematically or verified by random simulations. The proposed method is applied to some published examples to verify its effectiveness and practicability. Finally, we provide an improved iterative method, which is based on the 3 tuples iterative method. The improved iterative method and the 3 tuples iterative method are closely linked and each has its own different perspective. Moreover, they are applied on different occasions and reinforce one another.																	1568-4539	1573-2908				SEP	2020	19	3					359	388		10.1007/s10700-020-09323-y		APR 2020											
J								Hybridizing genetic algorithm and grey wolf optimizer to advance an intelligent and lightweight intrusion detection system for IoT wireless networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Bio-inspired optimization; Genetic algorithm; Grey wolf optimizer; Feature selection; Intrusion detection system; Internet of things; AWID	ARTIFICIAL BEE COLONY; PARTICLE SWARM OPTIMIZATION; FEATURE-SELECTION APPROACH; RANDOM-FOREST; INTERNET; MACHINE; CHALLENGES; DATASET; GA	Open wireless sensor networks (WSNs) in Internet of things (IoT) has led to many zero-day security attacks. Since intrusion detection is a key security solution, this paper presents a lightweight machine learning-based intrusion detection technique with high performance for resource limited IoT wireless networks namely, IoT intrusion detection system (IoTIDS). IoTIDS is based on hybridization of genetic algorithm (GA) and grey wolf optimizer (GWO), termed as GA-GWO. The main aim of the hybrid algorithm for the IoTIDS is to reduce the dimensionality of the huge wireless network traffic through intelligent selecting the most informative traffic features. By hybridizing, we try to eliminate their weaknesses through GA and GWO strengths. The effectiveness of the GA-GWO on IoTIDS is evaluated using AWID (aegean wi-fi intrusion dataset) as a new real-world wireless intrusion dataset, after preprocessing it under different scenarios. The experimental results proved that the proposed GA-GWO individually not only improved the performance of the IoTIDS in terms of computational costs, but it also enabled the IoTIDS to detect ? with high accuracy and low false alarm rate. Furthermore, GA-GWO in comparison to the original GA and GWO and other recent existing methods like FS, weight, and parameter optimization of SVM based on the GA (FWP-SVM-GA) and binary GWO (BGWO) has proven to be more effective.																	1868-5137	1868-5145															10.1007/s12652-020-01919-x		APR 2020											
J								Mitigation of commutation failure method in LCC converter based on HVDC systems by mean of modeling and simulation	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Dynamic state; HVDC system; LCC converter; Commutation failures; Protection function; Fault condition		Commutation failures is the commonest perturbation can appear in the inverter valves during or after the faults. Therefore, understanding the system fault is very important for regulation and protection design. Simulation by means of digital real time simulator can provide quickly the simulation results, and also represents the most effective methods to find the system fault behavior. This paper proposes a simplified method to detect and avoid the commutations failures in an HVDC inverter. Besides, the inverter is connected to the AC system which is considered as a weak AC grid having a low short circuit ratio. The results are provided by means of the digital real time simulator using the RT-lab platform and achieved by the simulation in the loop approach. Although the real time simulation allows for testing the behavior of these kinds of systems under adverse conditions that rarely occur in the field but are important to evaluate. 12 pulses line-commutated converter HVDC system is used in this study (both stations the rectifier and the inverter are based on thyristor valves). The function of the protection function is validated against the real time simulation of the HVDC inverter following different kind of faults, both kind of fault are the single phase to ground AC fault and a short circuit at the inverter valve. The obtained results show clearly the effectiveness of the proposed method to avoid the commutation failure by ensuring and maintaining a safe commutation process on the inverter station in the worst case.																	1868-5137	1868-5145															10.1007/s12652-020-01924-0		APR 2020											
J								Local Turn-Boundedness: A Curvature Control for Continuous Curves with Application to Digitization	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Discrete geometry; Local turn-boundedness; Par-regularity; Well-composedness; 4-Connected; Digitization; Curvature	RECONSTRUCTION; SHAPES	This article focuses on the classical problem of the control of information loss during the digitization step. The properties proposed in the literature rely on smoothness hypotheses that are not satisfied by the curves including angular points. The notion of turn introduced by Milnor in the article On the Total Curvature of Knots generalizes the notion of integral curvature to continuous curves. Thanks to the turn, we are able to define the local turn-boundedness. This promising property of curves does not require smoothness hypotheses and shares several properties with the par(r)-regularity, in particular well-composed digitizations. Besides, the local turn-boundedness enables to constrain spatially the continuous curve as a function of its digitization.																	0924-9907	1573-7683				JUN	2020	62	5			SI		673	692		10.1007/s10851-020-00952-x		APR 2020											
J								Residuated lattice of L-fuzzy ideals of a ring	SOFT COMPUTING										Ring; Ideal; L-fuzzy ideal; Residuated lattice	SUBALGEBRAS; PRIME	In 1988, given a complete Brouwerian lattice L := (L;.,.; 0, 1) and a ring A := ( A; +, center dot; -; 0) with unity 1, Swamy and Swamy (JMath Anal Appl 134:94-103, 1988) built a lattice structure, on the set of L-fuzzy ideals of A, and investigated some of its arithmetic properties. Since the residuation theory is richer than the lattice theory [see, Ciungu (Non-commutative multiple-valued logic algebras, Springer monographs in mathematics, Springer, Berlin, 2014), Galatos et al. (An algebraic glimpse at substructural logics, volume 151 of studies in logic and the foundations of mathematics, Elsevier, Amsterdam, 2007), Jipsen and Tsinakis (in: Martinez (ed) Ordered algebraic structures, Kluwer Academic Publisher, Dordrecht, 2002), Piciu (Algebras of fuzzy logic, Editura Universitaria Craiova, Craiova, 2007)], in this paper, we consider the notion of fuzzy ideals rather under a complete Brouwerian residuated lattice L := ( L;.,., , , ; 0, 1). A residuated lattice Fid(A, L) := Fid(A, L);., +,., ., ;.0, 1 is built on the set Fid(A, L) of L-fuzzy ideals of A and it is shown that the latter is both an extension of L and the residuated lattice Id(A) := Id(A); n, +, ,., ; {0}, A on the set Id(A) of ideals of A.																	1432-7643	1433-7479				JUN	2020	24	12			SI		8717	8724		10.1007/s00500-020-04894-9		APR 2020											
J								An ANN-based harmonic mitigation and power injection technique for solar-fed distributed generation system	SOFT COMPUTING										Artificial neural network; Distributed generation; Grid interconnection; Harmonic compensation; Power quality (PQ); Renewable energy source	SHUNT ACTIVE-FILTER; MULTILEVEL CONVERTER; CONTROLLER; NETWORK	In this article, a fifteen-level cascaded H-bridge multilevel inverter with active power filtering capability is suggested to increase the power quality in a single-phase distributed generation (DG) system. Integrating renewable power sources in the distribution line becomes easier with advancements in power electronic converters. At the same time, increase in usage of nonlinear loads leads to electronic pollutions like harmonics and power factor issues. Since most of the DG systems have interfacing inverters, it can be utilized for power quality improvement. This proposed work presents a control scheme to gain maximum utilization of these grid interfacing inverters with artificial neural network (ANN). An instantaneous p-q theory in a-ss-0 reference frame-based control algorithm is derived to control the inverter in the single-phase distribution system. In the ANN-based approach, the control algorithm is solved and the ANN is trained based on the switching angles obtained. The trained system is integrated with DG system to operate as a multitasking circuit by adding active power filter (APF) operation. It is proposed to utilize the interfacing inverter as: (1) interfacing inverter to add power produced from renewable sources and (2) APF to mitigate the harmonics. Both functions are accomplished simultaneously. The proposed work is verified with extensive MATLAB/Simulink, and the obtained results demonstrate that the proposed approach delivers a notable improvement in power quality concerning reduction in total harmonic distortion and injection of real power generated through RES into the distribution line. A 3-kWp photovoltaic panel with multifunctioning inverter designed utilizing ANN is executed in an experimental setup to prove the efficiency of the proposed approach.																	1432-7643	1433-7479				OCT	2020	24	20					15763	15772		10.1007/s00500-020-04907-7		APR 2020											
J								Echoes of myth and magic in the language of Artificial Intelligence	AI & SOCIETY										Artificial Intelligence; Religion; Science fiction; Existential risk; Philosophy of science; Technological singularity	AI	To a greater extent than in other technical domains, research and progress in Artificial Intelligence has always been entwined with the fictional. Its language echoes strongly with other forms of cultural narratives, such as fairytales, myth and religion. In this essay we present varied examples that illustrate how these analogies have guided not only readings of the AI enterprise by commentators outside the community but also inspired AI researchers themselves. Owing to their influence, we pay particular attention to the similarities between religious language and the way in which the potential advent of greater than human intelligence is presented contemporarily. We then move on to the role that fiction, science fiction most of all, has historically played and is still playing in the discussion of AI by influencing researchers and the public, shifting the weights of different scenarios in our collectively perceived probability space. We sum up by arguing that the lore surrounding AI research, ancient and modern, points to the ancestral and shared human motivations that drive researchers in their pursuit and fascinate humanity at large. These points of narrative entanglement where AI meets the wider culture should serve to amplify the call to engage ourselves with the discussion of the potential destination of this technology.																	0951-5666	1435-5655															10.1007/s00146-020-00966-4		APR 2020											
J								A semantically enriched text mining system for clinical decision support	COMPUTATIONAL INTELLIGENCE										classification; clinical decision support system; medical entities recognition; MetaMap; text mining	METAMAP	Existing systems to support decision-taking process based on textual information of clinical reports are insufficient. Currently, there are few systems that unify different subtasks in a single and user-friendly framework, easing therefore the clinical work by automating complex and arduous tasks such as the detection of clinical alerts as well as clinical information coding. To address this issue, MiNerDoc is proposed as a new text mining (TM) system whose main objective is to support clinical decision-taking processes by analyzing textual clinical reports in a unified framework. MiNerDoc is a really alluring TM system that includes two relevant tasks in the medical field, that is, detection of risk factors according to five medical entities (disease, pharmacologic, region/part body, procedure/test, and finding/sign) and automatic prediction of standardized diagnostic codes (MeSH descriptors associated with diseases). MiNerDoc integrates a combination of techniques from the TM discipline along with the terminological and semantic enrichment provided by the MetaMap tool and UMLS metathesaurus. Some study cases as well as a wide experimental analysis on real clinical reports have been carried out to demonstrate the effectiveness and promising performance of MiNerDoc on two different tasks, that is, medical entities recognition (FMeasure 81.54%) and diagnostic classification (FMeasure(mic) 81.04%).																	0824-7935	1467-8640															10.1111/coin.12322		APR 2020											
J								Relevant and irrelevant predictors in PLS2	JOURNAL OF CHEMOMETRICS										projection to latent structures regression; relevant and irrelevant predictors; variable importance score	VARIABLE SELECTION METHODS; ORTHOGONAL PROJECTIONS; REGRESSION; IDENTIFICATION; MODELS	Partial least square regression (PLS) is largely applied to solve regression problems when correlation and redundancy are present in the data. In spite of many studies about feature selection and variable importance have been published, to select the subset of relevant features useful to explain the behaviour of the system under investigation and the subset of irrelevant predictors that can be ignored is still an open issue. Here, a new strategy to measure variable importance is introduced, and a wrapper method is proposed for selecting relevant and irrelevant predictors. The variable importance measure is developed grouping the predictors in classes of equivalent features by clustering in the latent space and considering the variations of the goodness of the PLS2 model generated perturbing the block of the predictors. The wrapper method implements stability selection using bootstrap and feature selection. The behaviour of the new variable importance score and its use within the wrapper method are discussed investigating two simulated and one real data set.																	0886-9383	1099-128X				AUG	2020	34	8							e3237	10.1002/cem.3237		APR 2020											
J								EAFIM: efficient apriori-based frequent itemset mining algorithm on Spark for big transactional data	KNOWLEDGE AND INFORMATION SYSTEMS										Frequent itemset mining; Apache Spark; Apriori algorithm; Large-scale datasets		Frequent itemset mining is considered a popular tool to discover knowledge from transactional datasets. It also serves as the basis for association rule mining. Several algorithms have been proposed to find frequent patterns in which the apriori algorithm is considered as the earliest proposed. Apriori has two significant bottlenecks associated with it: first, repeated scanning of input dataset and second, the requirement of generation of all the candidate itemsets before counting its support value. These bottlenecks reduce the effectiveness of apriori for large-scale datasets. Reasonable efforts have been made to diminish these bottlenecks so that efficiency can be improved. Especially, when the data size is larger, even distributed and parallel environments like MapReduce does not perform well due to the iterative nature of the algorithm that incurs high disk overhead. Apache Spark, on the other hand, is gaining significant attention in the field of big data processing because of its in-memory processing capabilities. Apart from utilizing the parallel and distributed computing environment of Spark, the proposed scheme named efficient apriori-based frequent itemset mining (EAFIM) presents two novel methods to improve the efficiency further. Unlike apriori, it generates the candidates 'on-the-fly,' i.e., candidate generation, and count of its support values go simultaneously when the input dataset is being scanned. Also, instead of using the original input dataset in each iteration, it calculates the updated input dataset by removing useless items and transactions. Reduction in size of the input dataset for higher iterations enables EAFIM to perform better. Extensive experiments were conducted to analyze the efficiency and scalability of EAFIM, which outperforms other existing methodologies.																	0219-1377	0219-3116				SEP	2020	62	9					3565	3583		10.1007/s10115-020-01464-1		APR 2020											
J								Delaunay triangulation-based spatial colocation pattern mining without distance thresholds	STATISTICAL ANALYSIS AND DATA MINING										spatial colocation pattern; distance threshold; Delaunay triangulation; heterogeneous distribution density; neighboring polygon	CO-LOCATION PATTERNS; ALGORITHM; DISCOVERY	A spatial colocation pattern is a group of spatial features whose instances frequently appear together in close proximity to each other. The proximity of instances is generally measured by the distance between them. If the distance is smaller than a distance threshold that is specified by users, they have a neighbor relationship. However, it is difficult for users to give a suitable distance threshold and mining results also vary widely with different distance thresholds. In addition, using distance thresholds are hard to accurately obtain neighborhoods of instances in heterogeneous distribution density data sets. In this study, we propose a new method for determining the neighbor relationship of instances in space without the distance threshold based on Delaunay triangulation (DT). We design three filtering strategies, such as a feature invalid edge, a global positive edge, and a local positive edge, to constrain the original DT to accurately extract the neighborhoods of instances in space. Then, a miner called DT-based colocation (DTC) pattern mining is developed. Different from the traditional algorithms which adopt the time-consuming generate-test candidate model, DTC directly collects the table instances of colocation patterns from the constrained DT by building neighboring polygons and filters prevalent patterns. We compare the results mined by DTC with by the traditional algorithms at macrolevel and microlevel on both real and synthetic data sets to prove that the DTC algorithm improves the effectiveness and fineness of mining results.																	1932-1864	1932-1872				JUN	2020	13	3					282	304		10.1002/sam.11457		APR 2020											
J								Efficient weighted probabilistic frequent itemset mining in uncertain databases	EXPERT SYSTEMS										probability model; pruning; uncertain database; weighted probabilistic frequent itemset		Uncertain data mining has attracted so much interest in many emerging applications over the past decade. An issue of particular interest is to discover the frequent itemsets in uncertain databases. As an item would not appear in a transaction of such database for certain, several probability models are presented to measure the frequency of an itemset, and the frequent itemset over probabilistic data generally has two different definitions: the expected support-based frequent itemset and probabilistic frequent itemset. Meanwhile, it is noted that the frequency itself cannot identify useful or meaningful patterns in some scenarios. Other measures such as the importance of items should be also taken into account. To this end, some studies recently have been done on weighted (importance) frequent itemset mining in uncertain databases. However, they are only designed for the expected support-based frequent itemset, and suffer from low efficiency due to generating too many frequent itemset candidates. To address this issue, we propose a novel weighted probabilistic frequent itemsets (w-PFIs) algorithm. Moreover, we derive a probability model for the support of a w-PFI candidate in our method and present three pruning techniques to narrow the search space and remove the unpromising candidates immediately. Extensive experiments have been conducted on both real and synthetic datasets, to evaluate the performance of our w-PFI algorithm in terms of runtime, accuracy and scalability. Results show that our algorithm yields the best performance among the existing algorithms.																	0266-4720	1468-0394														e12551	10.1111/exsy.12551		APR 2020											
J								Predictive reliability and validity of hospital cost analysis with dynamic neural network and genetic algorithm	NEURAL COMPUTING & APPLICATIONS										Hospital cost analysis; Medical informatics; Artificial neural networks; Genetic algorithm; Strategic management	SYSTEM; IMPROVE	Hospital cost analysis (HCA) becomes a key topic and forefront of politics, social welfare and medical discourse. HCA includes a wide range of expenses; yet the foremost attention relates to the money expense in which hospital managers would like to draw a figure of incomes in the past and future. Based on the HCA results, they can develop many plans for improving hospital's service quality and investing in potential healthcare services in order to deliver better services with lower costs. Machine learning methods are often opted for prediction in HCA. In this paper, we propose a new method for HCA that uses genetic algorithm (GA) and artificial neural network (ANN). Operators of GA are used to boost up calculation to get optimal weights in the forward propagation of ANN. Experiments on a real database of Hanoi Medical University Hospital (HMUH) including calculus of kidney and ureter inpatients show that the new method achieves better accuracy than the relevant ones including linear regression, K-nearest neighbors (KNN), ANN and deep learning. The mean squared error of the proposed model gets the lowest value (0.00360), compared to those of deep learning, KNN and linear regression which are 0.00901, 0.01205 and 0.01718 respectively.																	0941-0643	1433-3058				SEP	2020	32	18			SI		15237	15248		10.1007/s00521-020-04876-w		APR 2020											
J								Multi-view spectral clustering via sparse graph learning	NEUROCOMPUTING										Clustering; Multi-view; Sparse; Spectral clustering		Although numerous multi-view spectral clustering algorithms have been developed, most of them generally encounter the following two deficiencies. First, high time cost. Second, inferior operability. To this end, in this work we provide a simple yet effective method for multi-view spectral clustering. The main idea is to learn a consistent similarity matrix with sparse structure from multiple views. We show that proposed method is fast, straightforward to implement, and can achieve comparable or better clustering results compared to several state-of-the-art algorithms. Furthermore, the computation complexity of proposed method is approximately equivalent to the single-view spectral clustering. For these advantages, it can be considered as a baseline for multi-view spectral clustering. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						1	10		10.1016/j.neucom.2019.12.004													
J								Adaptive finite-time cluster synchronization of neutral-type coupled neural networks with mixed delays	NEUROCOMPUTING										Neutral-type coupled neural networks (NCNNs); Finite-time cluster synchronization; Adaptive control; Mixed delays	COMPLEX DYNAMICAL NETWORKS; VARYING DELAYS; ANTI-SYNCHRONIZATION; STATE ESTIMATION; CONSENSUS; SYSTEMS; DISCRETE; COMMUNICATION; TRACKING; NODES	This paper investigates the adaptive finite-time cluster synchronization problem of the neutral-type coupled neural networks (NCNNs) with mixed delays. Compared with the traditional neural networks, the model of NCNNs is more general in some sense, due to that it involves state delays, distributed delays and coupling delays. In this paper, a novel, adaptive and closed-loop control control algorithm is proposed to achieve the finite-time cluster synchronization of NCNNs with mixed delays. In addition, the sufficient conditions on the control parameters for stabilizing the closed-loop system are derived by leveraging the Lyapunov stability argument. Finally, the simulation results are carried out to illustrate the validity and feasibility of the proposed algorithm. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						11	20		10.1016/j.neucom.2019.11.046													
J								Lag H-infinity synchronization and lag synchronization for multiple derivative coupled complex networks	NEUROCOMPUTING										Adaptive state feedback controller; Complex networks (CNs); Lag H-infinity synchronization; Lag synchronization; Multiple derivative couplings	DELAYED DYNAMICAL NETWORKS; DIFFUSION NEURAL-NETWORKS; OUTPUT SYNCHRONIZATION; ADAPTIVE-CONTROL; PINNING SYNCHRONIZATION; PASSIVITY	This paper mainly devotes to the study of lag H-infinity synchronization and lag synchronization issues for the multiple derivative coupled complex networks (MDCCNs) with and without external disturbances, which have never been investigated. On one side, with the help of state feedback controller, adaptive state feed- back controller and Lyapunov functionals, two criteria are developed to insure the lag synchronization for the MDCCN with external disturbances. On the other side, we also discuss the lag synchronization in the MDCCN in virtue of choosing appropriate state feedback controller and adaptive state feedback controller. Lastly, two numerical examples are put forward to verify the lag H-infinity synchronization and lag synchronization criteria. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						46	56		10.1016/j.neucom.2019.11.100													
J								Combining contextual neural networks for time series classification	NEUROCOMPUTING										Time series classification; Contextual convolutional neural networks; Contextual long short-term memory; Attention; Multilayer perceptron		Ten years ago, linear models were applied in various domains. Before application of the algorithms, several current studies extracted features presumed to represent parochial markings from the data using engineering techniques. Recently the deep learning domain offered opportunities to directly feed data into the model without any extensive hand-crafted feature engineering techniques. In this paper, the proposed framework does the feature extraction in a non-supervised (i.e. self-supervised) manner using both Contextual Long Short-Term Memory (CLSTM) and Contextual Convolutional Neural Networks (CCNN). We can then concatenate data obtained from the CLSTM and CCNN blocks, feed it into the Attention block, pass it through the Multilayer Perceptron (MLP) block, ultimately passing it through a terminal layer for classification. The task involved here was non-trivial as there is a major challenge in implementing our model to solve the time series classification (TSC) problem: overfitting. We deal with this challenge as follows; firstly, we adjusted the number of neurons in each of the stages. Secondly we introduced dropouts after every layer in each stage of this model. Finally experiments regarding the University of California Riverside (UCR) dataset indicates the model's superiority. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						57	66		10.1016/j.neucom.2019.10.113													
J								Object-adaptive LSTM network for real-time visual tracking with adversarial data augmentation	NEUROCOMPUTING										Visual tracking; LSTM network; Generative adversarial network; Data augmentation		In recent years, deep learning based visual tracking methods have obtained great success owing to the powerful feature representation ability of Convolutional Neural Networks (CNNs). Among these methods, classification-based tracking methods exhibit excellent performance while their speeds are heavily limited by the expensive computation for massive proposal feature extraction. In contrast, matching-based tracking methods (such as Siamese networks) possess remarkable speed superiority. However, the absence of online updating renders these methods unadaptable to significant object appearance variations. In this paper, we propose a novel real-time visual tracking method, which adopts an object-adaptive LSTM network to effectively capture the video sequential dependencies and adaptively learn the object appearance variations. For high computational efficiency, we also present a fast proposal selection strategy, which utilizes the matching-based tracking method to pre-estimate dense proposals and selects high-quality ones to feed to the LSTM network for classification. This strategy efficiently filters out some irrelevant proposals and avoids the redundant computation for feature extraction, which enables our method to operate faster than conventional classification-based tracking methods. In addition, to handle the problems of sample inadequacy and class imbalance during online tracking, we adopt a data augmentation technique based on the Generative Adversarial Network (GAN) to facilitate the training of the LSTM network. Extensive experiments on four visual tracking benchmarks demonstrate the state-of-the-art performance of our method in terms of both tracking accuracy and speed, which exhibits great potentials of recurrent structures for visual tracking. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						67	83		10.1016/j.neucom.2019.12.022													
J								Discrete ZNN models of Adams-Bashforth (AB) type solving various future problems with motion control of mobile manipulator	NEUROCOMPUTING										Zhang neural network (ZNN); Future linear system (FLS); Future division (FD); Future quadratic minimization (FQM); Future equality-constrained quadratic programming (FECQP); Adams-Bashforth (AB) type	RECURRENT NEURAL-NETWORK; FINITE-TIME SOLUTION; REDUNDANT MANIPULATORS; REPETITIVE MOTION; SYSTEMS; MATRIX; OPTIMIZATION; STABILITY; EQUATIONS; INVERSE	Zhang neural network (ZNN), being a special type of recurrent neural network, has shown powerful abilities to solve a great variety of continuous time-varying problems. In order to solve future problems (or termed discrete time-varying problems), discrete ZNN (DZNN) models should be developed. In this paper, four kinds of future problems, i.e., future linear system (FLS), future division (FD), future quadratic minimization (FQM), and future equality-constrained quadratic programming (FECQP), are investigated. The DZNN models of Adams-Bashforth (AB) type are thus proposed for solving four kinds of future problems. Compared with conventional DZNN models of Euler type and Taylor type, the performances of DZNN models of AB type in terms of accuracy are better. Meanwhile, the numerical results substantiate the efficacy and superiority of DZNN models of AB type for solving future problems. Furthermore, the motion control of mobile robot manipulator is conducted to substantiate the efficacy of DZNN models of AB type for solving future problems. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						84	93		10.1016/j.neucom.2019.11.039													
J								AutoGAN-based dimension reduction for privacy preservation	NEUROCOMPUTING										Generative adversarial nets; Auto-encoder; Neural-network; Privacy preservation; Dimension reduction; Access control	COMPRESSIVE PRIVACY	Protecting sensitive information against data exploiting attacks is an emerging research area in data mining. Over the past, several different methods have been introduced to protect individual privacy from such attacks while maximizing data-utility of the application. However, these existing techniques are not sufficient to effectively protect data owner privacy, especially in the scenarios that utilize visualizable data (e.g. images, videos) or the applications that require heavy computations for implementation. To address these problems, we propose a new dimension reduction-based method for privacy preservation. Our method generates dimension-reduced data for performing machine learning tasks and prevents a strong adversary from reconstructing the original data. We first introduce a theoretical approach to evaluate dimension reduction-based privacy preserving mechanisms, then propose a non-linear dimension reduction framework motivated by state-of-the-art neural network structures for privacy preservation. We conducted experiments over three different face image datasets (AT&T, YaleB, and CelebA), and the results show that when the number of dimensions is reduced to seven, we can achieve the accuracies of 79%, 80%, and 73% respectively and the reconstructed images are not recognizable to naked human eyes. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						94	103		10.1016/j.neucom.2019.12.002													
J								Developing parallel ant colonies filtered by deep learned constrains for predicting RNA secondary structure with pseudo-knots	NEUROCOMPUTING										RNA secondary structure; Pseudo-knots; Deep learned constrains; Parallel ant colonies; Recurrent neural network	ALGORITHM; MODEL	RNA plays important roles in cells besides being a simple carrier of genetic information. RNA secondary structure prediction is an efficient way to explore its biochemical function. RNA secondary structure prediction with pseudo-knots is really difficult, which has been proven as an NP-hard problem. Many of the existing predictions seemed to be limited not only by the quality of the search algorithms but also by the quality of the objective functions used. In this paper, a novel prediction model called DpacoRNA is proposed to improve the accuracy of the RNA secondary structure prediction with pseudo-knots, which mainly consists of two features: a parallel search algorithm and learning structural constraints using a deep model. First, based on the base-paired and single-stranded probabilities as multiple objective functions, DpacoRNA applied a parallel ant colony optimization strategy to predict the RNA secondary structure. Second, a bi-directional LSTM recurrent neural network was used to learn the base-pairing constraints. Finally, the constraints learned from the deep model were applied to the output of parallel ant colonies to refine the final secondary structures. To examine the strength and the weakness of the proposed method, multiple RNA types, including RNase P RNA, 5s rRNA, hammerhead ribozyme, transfer RNA, and tmRNA, were used to carefully benchmark DpacoRNA with other state-of-the-art solutions. The final results showed that DpacoRNA was competitive to the other methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						104	114		10.1016/j.neucom.2019.12.041													
J								Finite-time event-triggered non-fragile state estimation for discrete-time delayed neural networks with randomly occurring sensor nonlinearity and energy constraints	NEUROCOMPUTING										Event-triggered scheme; Non-fragile state estimation; Delayed neural networks; Sensor nonlinearity; Energy constraint	VARYING COMPLEX NETWORKS; H-INFINITY CONTROL; SYSTEMS; SYNCHRONIZATION; SUBJECT	In this paper, the problem of event-triggered non-fragile state estimator design for discrete-time delayed neural networks (DNNs) is investigated over finite-time span. In consideration of the changes of environment and high sensitivity, external disturbances and/or parameter uncertainties might be involved in estimator parameters of the concerned DNNs. Therefore, it is one of our main objectives to design a non-fragile state estimator subject to the norm bounded gain variation. The sensor nonlinearity is supposed to occur in a random way. In the meanwhile, the event-triggered scheme and energy constraints are adopted in state estimator design for the purpose of energy and resource saving. By using the Lyapunov stability theory and some analytical techniques, sufficient conditions are established to guarantee that the estimation error system is finite-time bounded and meet a prescribed mixed H-infinity and passivity performance constraint. Furthermore, the estimator gains are obtained via solving a set of linear matrix inequalities (LMIs). Finally, two numerical examples are exploited to demonstrate the effectiveness of the developed technique. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						115	129		10.1016/j.neucom.2019.12.038													
J								MRAC for unknown discrete-time nonlinear systems based on supervised neural dynamic programming	NEUROCOMPUTING										Nonlinear systems; Approximate dynamic programming (ADP); Neural dynamic programming (NDP); Model reference adaptive control (MRAC); Adaptation and robustness	APPROXIMATE OPTIMAL-CONTROL; REFERENCE ADAPTIVE-CONTROL; MULTIAGENT SYSTEMS; DESIGN; IDENTIFICATION; CONTROLLER; NETWORKS; TRACKING	This paper investigates the model reference adaptive control (MRAC) problem for unknown nonlinear discrete-time systems, which is how to guarantee adaptation to the variable reference input and robustness to perturbation. A supervised neural dynamic programming (SNDP) approach is developed to solve this MRAC problem, which includes a learning mode and a control mode. In the learning mode, a data-based adaptive critic learning algorithm is proposed, which guarantees that the controlled objective adaptively tracks the reference model on behavior. Such an algorithm also ensures flexible switching from the learning mode to the control mode under which the robustness of the closed-loop control systems is further improved. By employing a newly defined mode scheduler that regulates the learning mode and the control mode, the adaptation and the robustness of the systems are both achieved by the developed SNDP approach. Its uniformly ultimately bounded property is proved by using Lyapunov method. Simulation results verify that the developed SNDP approach ensures the adaptation to the variable reference input, and has superiority over some traditional MRAC methods in improving the robustness. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						130	141		10.1016/j.neucom.2019.12.023													
J								Bi-Connect Net for salient object detection	NEUROCOMPUTING										Salient object detection; Forward connection; Reverse side connection	VISUAL-ATTENTION; FEATURES; SCENE	As a challenging task for pixel-wise image analysis, salient object detection has made huge progress in recent years. However, there still exists a difficult problem: detection of distinguishing a salient and non-salient object in multiple objects under complex background (e.g. blur, translucent, light reflection, etc.). Our proposed method cast this difficulty as information dissolve problem in deep convolutional network, which is manifested as: first, the model cannot grab whole details of a salient object at training phrase; second, due to the isolation between layers and blocks, the valued information is blocked within a block, which leads to the difficulty in obtaining the position and the edge of salient objects simultaneously; third, the output of the network is a low-resolution saliency map, which cannot accurately express the edge of salient objects. To address information dissolve problems, we construct a Bi-Connect Net (BCN) composed of forward connection subnet and reverse side connection subnet. Besides, the proposed adaptive learning fusion method not only stress all blocks contribution but also combine multiple features with different scale, so that grab more details on the right salient location and precise edges at the same time. Extensive experiments show that our proposed Bi-Connect Net can outperform the state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						142	155		10.1016/j.neucom.2019.12.020													
J								New criteria on event-triggered cluster synchronization of neutral-type neural networks with Levy noise and non-Lipschitz condition	NEUROCOMPUTING										Coupled neutral-type neural network; Cluster synchronization; Event-trigger control; Non-Lipschitz condition	COMPLEX DYNAMICAL NETWORKS; INFINITY STATE ESTIMATION; PINNING CONTROL; EXPONENTIAL SYNCHRONIZATION; STOCHASTIC PERTURBATION; STABILITY; SYSTEMS; STABILIZATION; SUBJECT; DELAYS	In this paper, two kinds of the exponential cluster synchronization of stochastic coupled neutral-type neural networks with Levy noise under non-Lipschitz condition are investigated. The non-Lipschitz condition has much weaker requirement than the usual Lipschitz condition, so the neuron activation functions have a wider range of options. Using the general Ito's formula and the nonnegative semi-martingale convergence theorem, the general sufficient conditions of two kinds of exponential synchronization are derived for the systems with an event-triggered pinning controller. A numerical example is presented to verify the effectiveness of the proposed criteria. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						156	169		10.1016/j.neucom.2019.11.099													
J								Deep hybrid dilated residual networks for hyperspectral image classification	NEUROCOMPUTING										Classification; Hyperspectral image; Deep residual learning; Hybrid dilated convolution; Spectral-spatial feature extracting		This study presents a new architecture for deep convolution networks, end-to-end hybrid dilated residual networks wherein 3D cube images are input for hyperspectral image (HSI) classification, and this is termed as 3D-2D SSHDR. The proposed networks could greatly improve the performance of HSI classification as they select the spectral bands adaptively and avoid the limitations of handcrafted selection. Specifically, 3D spectral residual blocks are initially used to learn discriminant features from rich spectral features. Subsequently, extracted 3D images with spectral features are reshaped into 2D feature maps as the input for spatial feature learning, which does not affect the extraction of spatial-spectral features and could reduce the number of parameters. This is followed by using hybrid dilated convolution (HDC) residual blocks to continue learning discriminative spatial features, expanding the receptive field of the convolution kernel without increasing the computational complexity, and avoiding the gridding effect generated by the dilated convolution. Finally, the proposed networks are trained via supervised learning. Besides, this method could accelerate the convergence speed and alleviate the over-fitting problem due to the added batch normalization layers and the design of a multi-residual structure. Experimental results validate that the performance of the proposed network is superior on several HSI benchmark datasets when compared with that of existing methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						170	181		10.1016/j.neucom.2019.11.092													
J								Dynamic attention network for semantic segmentation	NEUROCOMPUTING										Semantic segmentation; Deformable convolution; Geometric transformation; Attention mechanism		Semantic segmentation networks usually utilize special pyramid structure after encoder or combine low-level and high-level feature maps in decoder to capture multi-scale context information, which we term them feature combination and feature connection, respectively. However, both such frameworks are less valid with the fixed geometric structure or unsuitable interim. In this paper, we advocate Dynamic Attention Network (DAN) to solve these problems. First, we design a Deformable Attention Pyramid (DAP) module to perform a self-adjustable descriptor of high-level output, which utilizes deformable function to model geometric transformation. With DAP, semantic information can be captured effectively. Second, we propose a Fusing Attention Interim (FAI) module to guide the back-propagation of long-short range information in each level of decoder. We evaluate DAN on the challenging PASCAL VOC 2012 and Cityscapes segmentation benchmarks and find that it achieves state-of-the-art results without post-processing. Our observation can be concluded that the flexible structure that possesses dynamic attention mechanism is beneficial to learn multi-scale context information. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						182	191		10.1016/j.neucom.2019.12.042													
J								Point Encoder GAN: A deep learning model for 3D point cloud inpainting	NEUROCOMPUTING										Point cloud; Neural network; Inpainting; Encoder; Generative adversarial nets (GANs)		In this paper, we propose a Point Encoder GAN for 3D point cloud inpainting. Different from other 3D object inpainting networks, our network can process point cloud data directly without any labeling and assumption. We use a max-pooling layer to solve the unordered of point cloud during the learning procedure. We add two T-Nets (from PointNet) to the encoderdecoder pipeline, which can yield better feature representation of the input point cloud and a more suitable rotation angle of the output point cloud. We then propose a hybrid reconstruction loss function to measure the difference between the two sets of unordered data. Using small sample models on ModelNet40 only, the proposed Point Encoder GAN yields end-to-end inpainting results surprisingly. Experiment results have shown a high success rate. Several technical measures are used to identify the good qualities of our generated models. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						192	199		10.1016/j.neucom.2019.12.032													
J								SIST: Online Scale-Adaptive Object tracking with Stepwise Insight	NEUROCOMPUTING										Correlation filter; Boundary effect; Target insight; Scale insight; Collaborative scheme	VISUAL TRACKING; OCCLUSION	Correlation filter (CF) has achieved great success in the tracking community due to its high efficiency and effectiveness. However, in most existing CF trackers: (i) they mainly focus on object localization and cannot handle scale variations well; (ii) the learned CF is unavoidably deteriorated by the boundary effects, especially the background pixels inside the bounding box; (iii) there exists a localization gap between the original image and feature map. To handle these issues, we propose an online scale-adaptive correlation tracker by collaboratively learning a target-insight model (TIM) for object localization and a scale-insight model (SIM) for progressive refinement. In TIM, we introduce a target likelihood map to impose discriminative weights on the image, and then formulate a target-insight correlation filter for encoding the target as well as its surrounding context, which can handle boundary effect and distractors. Furthermore, we developed a SIM based on multiple instance learning and structured-labelled samples, which realizes the scale estimation and position refinement over time. Specifically, a hard negatives mining strategy is introduced to update the SIM rather than random sampling, which helps realize long-term tracking with re-capturing the missing target even tracking failure occurs. Extensive experiments on four benchmarks demonstrate that our method outperforms several state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						200	212		10.1016/j.neucom.2019.11.102													
J								Finite time impulsive synchronization of fractional order memristive BAM neural networks	NEUROCOMPUTING										Memristor; Impulsive control; Finite time synchronization; BAM Neural networks	CHAOTIC SYSTEMS; STABILIZATION	This paper is concerned with the finite time impulsive synchronization problem of fractional order memristive BAM neural networks (MBAMNNs) with switching jumps mismatch. By utilizing the properties of fractional calculus and comparison principle, a novel fractional order result about finite time impulsive stability is obtained. In the light of the double layer structure of fractional order MBAMNNs, two impulsive controllers are designed for the response fractional order MBAMNNs. By combing properties of Gamma functions and some analysis techniques of impulsive interval, several sufficient conditions are given which ensure finite time synchronization of fractional order MBAMNNs, and the upper bound of the setting time for synchronization is estimated, which is related to the fractional order of the system. Finally, numerical simulation is provided to demonstrate the effectiveness of the obtained results. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						213	224		10.1016/j.neucom.2019.12.056													
J								State synchronization of controlled nodes via the dynamics of links for complex dynamical networks	NEUROCOMPUTING										Time-varying complex dynamical network; Nodes subsystem (NS); Links subsystem (LS); State synchronization	DECENTRALIZED CONTROL; STRUCTURAL BALANCE	A continuous time-varying complex dynamical network with the graph model may be regarded to be composed of two interconnected subsystems, one of which is the nodes subsystem (NS) and the other is the links subsystem (LS). The two subsystems can be modeled mathematically by the state differential equations, in which the weighted values of links are regarded as the state variables of LS. This paper mainly focuses on the dynamics of LS, which is modeled as the Riccati matrix differential equation, and investigates the state synchronization of NS associated with the synthesized state feedback controller for NS and the constructed coupling relation in LS. Firstly, this paper proposes the equivalent condition of state synchronization by using the matrix algebra method. Then, the state feedback controller for NS is proposed and the coupling relation in LS is also constructed based on the Lyapunov stability theory, by which the asymptotic state synchronization of NS is sure to be realized. Finally, numerical simulations are given to verify the effectiveness of the theoretical results in this paper. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				APR 7	2020	384						225	230		10.1016/j.neucom.2019.12.055													
J								Deep convolutional neural network for accurate segmentation and quantification of white matter hyperintensities	NEUROCOMPUTING										White matter hyperintensity; Stroke; Convolutional neural network; Dilated block; Multi-scale features	STROKE LESION SEGMENTATION; AUTOMATIC SEGMENTATION; MR-IMAGES; VALIDATION; DISEASE; DROPOUT	White matter hyperintensities (WMHs) appear as regions of abnormally signal intensity on magnetic resonance imaging (MRI) images, that can be identified in MRI images of elderly people and ischemic stroke patients. However, manual segmentation and quantification of images with WMHs is laborious and time-consuming. Moreover, ischemic stroke lesion and WMHs appear as similar signals in MRI images, making it difficult to accurately segment the WMHs. Analysis of WMH-containing images is important for clinical diagnosis, and thus several segmentation methods have been proposed. However, these methods cannot accurately differentiate WMH and ischemic stroke lesions. We propose a deep convolutional neural network, M2DCNN, that can accurately segment WMHs and distinguish them from ischemic stroke lesions. M2DCNN consists of two subnets that rely on a set of novel multi-scale features and a novel architecture (inclusion of dense and dilated blocks). Our model is trained and evaluated on two public segmentation challenges with multi-modality MRI images. Empirical tests demonstrate that M2DCNN outperforms current segmentation methods. We empirically demonstrate that M2DCNN effectively separates WMHs from stroke lesions. Finally, ablation experiments reveal that both multi-scale features and architectural elements in our method contribute to the improved predictive performance. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						231	242		10.1016/j.neucom.2019.12.050													
J								AdaBoost-inspired multi-operator ensemble strategy for multi-objective evolutionary algorithms	NEUROCOMPUTING										Multi-objective optimization; AdaBoost; Multi-operator ensemble; Credit assignment; Subpopulation update	MANY-OBJECTIVE OPTIMIZATION; NONDOMINATED SORTING APPROACH; DIFFERENTIAL EVOLUTION; GENETIC ALGORITHM; SELECTION; DECOMPOSITION; EXPLORATION	Evolutionary algorithms have shown prominent performance in solving various kinds of multi-objective optimization problems (MOPs), but most of them only use single operator that is often sensitive to the characteristics of problems. As different operators have different search patterns, a proper combination of multiple operators can be more efficient and robust than using one single operator in solving complex problems. However, how to ensemble multiple operators based on their performances in optimization process is a challenging task. In the machine learning field, it is well known that AdaBoost can effectively ensemble multiple classifiers by giving the proper weights based on their classification errors. Inspired by this ensemble way, we propose a multi-operator ensemble (MOE) strategy based on multiple subpopulations for evolutionary multi-objective optimization. In the proposed strategy, the survival rate of each subpopulation after environmental selection is used to evaluate the performance of the operator, and then a credit assignment scheme is developed by using the weight update method in AdaBoost. Based on these credits, an emigration-immigration mechanism is designed to update the subpopulation that can adaptively reward or punish the computational resources for operators. Experimental results on three complex test suites demonstrate that the proposed MOE can significantly improve the performance of multi-objective evolutionary algorithms on different types of MOPs. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						243	255		10.1016/j.neucom.2019.12.048													
J								Optimal robust control of vehicle lateral stability using damped least-square backpropagation training of neural networks	NEUROCOMPUTING										Artificial Neural Networks; Damped Least-Square Backpropagation; Vehicle Control; Optimal Control	DIRECT YAW MOMENT; INTEGRATED CONTROL; ADAPTIVE-CONTROL; SYSTEMS; BRAKING	Chassis control systems play a significant role in achieving the desired vehicle performance and stability during various severe maneuvers. A probabilistic estimation approach by hybridization of optimal robust control and a damped least-square backpropagation based neural networks (NN) is proposed to design a control system for dealing with unknown nonlinear dynamics of a passenger car. To this end, a four-wheel active steering (4WAS) model is employed and a multilayer perceptron (ML) feed-forward backpropagation neural network (FFBPNN) model is developed as an approximator. The optimal robust control is employed to regulate the yaw rate and side-slip angle of the vehicle to follow the desired vehicle response. The developed FFBPNN model is trained to distinguish the nonlinear dynamics of the vehicle and the corresponding optimal feedback gain during a wide range of operating conditions via the state variables. The robustness of the controller is evaluated using Lyapunov stability method. The performance of the proposed controller is analyzed considering the open-loop and closed-loop responses of the nonlinear vehicle model and a sliding mode controller to track the desired yaw rate and side-slip angle responses. The results obtained during severe maneuvers suggest that the proposed control method can substantially enhance the handling and stability performances of the vehicle. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						256	267		10.1016/j.neucom.2019.12.045													
J								Robust form-closure grasp planning for 4-pin gripper using learning-based Attractive Region in Environment	NEUROCOMPUTING										Attractive Region in Environment (ARIE); Generalized robotic grasping; Learning-based grasping; 4-pin gripper design	OBJECTS	In terms of the closure theory, for 3D objects, it usually requires at least 7 grasp points to ensure a form closure grasp, which is too strict for real applications. Instead, using a 4-point planar grasp is much more practical. In this paper, a robust form-closure grasping planning algorithm is proposed for a 4-pin gripper to obtain stable grasp points and improve the generalization to grasp objects that have not been seen before. Besides, a lightweight, 3-DoF (Degree of Freedom) 4-pin gripper based on our algorithm is designed for 3D object grasping. The proposed algorithm consists of two parts. First, based on Attractive Region in Environment (ARIE), the stability of the whole grasping process by obtaining form-closure grasp points is ensured. Second, considering the uncertainty of the environment, a learning grasp quality measurement is proposed to make evaluation of robustness for each group of grasp points. Our simulation and physical experiments are performed to test and verify the effectiveness of the gripper and the proposed algorithm. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						268	281		10.1016/j.neucom.2019.12.039													
J								Simultaneous hyperspectral image super-resolution and geometric alignment with a hybrid camera system	NEUROCOMPUTING										Simultaneous HSI super-resolution and alignment; Sparse representation; Alternative optimization algorithm; Hybrid camera system	QUALITY ASSESSMENT; SPECTROMETER; STATISTICS	Existing snapshot hyperspectral cameras usually suffer from limited spatial resolution in order to maintain reasonable temporal and spectral resolution. In contrast, the spatial resolution of commercial RGB cameras is quite high. Therefore, many methods have been developed for hyperspectral image (HSI) super-resolution by fusing a high resolution RGB image and a low resolution HSI. These methods have been extensively evaluated by using simulated image pair with exact geometric alignment. However, the effect of misalignment on these methods, which always arises in hybrid camera system, has rarely been investigated. In this paper, we present an effective approach for simultaneous HSI super-resolution and geometric alignment of the image pair with drastically contrasting spatial resolution. Besides, we also conduct a systematic evaluation of the misalignment effect on five state-of-the-art hybrid HSI super-resolution methods under five different geometric transformations and three benchmark datasets. Experimental results on both synthetic data and real images show that the proposed method outperforms the current state-of-the-art HSI super-resolution methods with a misaligned hybrid camera system in terms of both objective metric and subjective visual quality. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						282	294		10.1016/j.neucom.2019.12.024													
J								Fine-grained visual categorization of butterfly specimens at sub-species level via a convolutional neural network with skip-connections	NEUROCOMPUTING										Convolutional neural network; Skip-connection; Fine-grained visual categorization; Deep learning; Butterfly sub-species	IMAGE; CLASSIFICATION; RECOGNITION; ALGORITHM; MODEL	Fine-grained visual categorization often suffers from the challenges that the subordinate categories within an entry-level category can only be distinguished by subtler discriminations. It demands an effective algorithm to learn a multiple-perspective density distribution for precious categorization. To shape a coarse-to-fine object perception, a hierarchical convolutional neural network (CNN) denoting the skip-connections convolutional neural network (S-CCNN) was proposed, focusing on a butterfly domain at subspecies level due to the fine-grained structure of the category taxonomy. Specifically, based on the serial backbone, three skip-connections with Grating layers are established to link the earlier layers and upper layers of network, and integrated with DropConnect, exponential linear unit (ELU), and local response normalization (LRN) to alleviate over-fitting and vanishing gradient. Benefitting from the long-span skip-connections, coarse-grained context with orientation descriptions and finer-grained context with semantic representations can be both took into consideration, and they are jointly incorporated into framework. S-CCNN can achieve the cross-utilization of object-level and part-level representations, and its rationality were evidenced with both theory and practice. For effectness verification, a total of 24,836 lab-made images of butterfly specimens spanning 56 sub-species are utilized as testing samples, while 173,852 augmented images are employed for model training. S-CCNN delivers a consistent and significant boost in performance, i.e., validation accuracy achieved 94.17% and testing accuracy achieved 93.36%, which outperformed state-of-the-arts. S-CCNN can easily relish accuracy gains from skip-connections in fine-grained visual categorization of butterfly sub-species, without any bells and whistles. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						295	313		10.1016/j.neucom.2019.11.033													
J								Scale-Recursive Network with point supervision for crowd scene analysis	NEUROCOMPUTING										Crowd density; Scale-Recursive; Crowd counting; Weakly supervised learning; Joint training		Crowd scene analysis, and in particular its density estimation, is a challenging task due to the lack of spatial information, scale variation, and the large amount of supervised-learning parameters. In order to address these challenges, we propose a Scale-Recursive encoder-decoder Network with Point Supervision (SRN+PS). On the one hand, an encoder-decoder recurrent structure uses features between adjacent scales to tackle scale variation, and a novel loss function, called the row vector-based counting loss, is proposed to focus on the crowd counting accuracy. On the other hand, we employ an additional point segmentation task in training and combine features learned from the two tasks above. The Euclidean loss, row vector-based counting loss, and two-label focal loss are integrated by a joint training scheme, which improves both the quality of density map estimation and the performance of crowd counting. Finally, we propose a weakly supervised framework based on the SRN structure and the Convolutional Winner-Take-All(CWTA) module. In this framework, most parameters are obtained by unsupervised learning with the exception of a few which are tuned by supervised learning in model training. As a result, our multi-scale structure can obtain salient object sparse spatial features from unsupervised learning. Experiments on the ShanghaiTech, UCF_CC_50 and UCSD datasets demonstrate the effectiveness of our proposed method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						314	324		10.1016/j.neucom.2019.12.070													
J								Finite-time synchronization of switched neural networks with state-dependent switching via intermittent control	NEUROCOMPUTING										Finite-time synchronization; Switched neural networks; Aperiodically intermittent control	EXPONENTIAL SYNCHRONIZATION; VARYING DELAYS; STABILIZATION; STABILITY; SYSTEMS; NOISE	In this paper, the finite-time synchronization problem for switched neural networks with state-dependent switching under intermittent control is discussed. Different from previous literature, this paper uses aperiodically intermittent control to make switched neural networks achieve finite-time synchronization. A new differential inequality is established to get our main results. Meanwhile, based on graph theory and differential inclusions theory, a proper Lyapunov function is proposed to derive a criterion to ensure finite-time synchronization of switched neural networks. And the convergence time, closely related to the topological structure of networks, is obtained and does not rely on control widths or rest widths for aperiodically intermittent control. Finally, a numerical example is presented to illustrate the effectiveness and feasibility of our results. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						325	334		10.1016/j.neucom.2019.12.031													
J								Reduction of JPEG compression artifacts based on DCT coefficients prediction	NEUROCOMPUTING										Compression artifacts; Discrete cosine transform (DCT); Receptive field; Convolutional neural networks (CNNs); JPEG		The image compression-decompression process causes image quality degradations such as blocking and ringing artifacts. A convolutional neural network based on DCT domain is proposed to learn the mapping relationship between JPEG images and original images to reduce compression artifacts in this work. The convolutional neural network can exploit the prior knowledge of JPEG compression in DCT domain. The compression artifacts reduction network which is proposed in this work has three advantages. First, it can expand the receptive field and eliminate the discontinuity between each 8 x 8 block by overlapping extraction of patches. Second, it is based on the essence of distortion and can predict true DCT coefficients more accurately to reduce the loss of JPEG images. Third, it can obviously reduce the compression artifacts in JPEG images. Experiments on compressed images demonstrate that our approach achieves state-of-the-art performance in both the objective parameters and the subjective visual quality. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						335	345		10.1016/j.neucom.2019.12.015													
J								Compressed sensing MR image reconstruction via a deep frequency-division network	NEUROCOMPUTING										Magnetic resonance imaging; Compressed sensing; Iterative reconstruction; Deep learning; Convolutional neural network	SELECTION	Compressed sensing MRI (CS-MRI) is considered as a powerful technique for decreasing the scan time of MRI while ensuring the image quality. However, state of the art reconstruction algorithms are still subjected to two challenges including terrible parameters tuning and image details loss resulted from over-smoothing. In this paper, we propose a deep frequency-division network (DFDN) to face these two image reconstruction issues. The proposed DFDN approach applies a deep iterative reconstruction network (DIRN) to replace the regularization terms and the corresponding parameters by a stacked convolution neural network (CNN). And then multiple DIRN blocks are cascaded continuously as one deeper neural network. Data consistency (DC) layer is incorporated after each DIRN block to correct the k-space data of intermediate results. Image content loss is computed after each DC layer and frequency-division loss is gained by weighting the high frequency loss and low frequency loss after each DIRN block. The combination of image content loss and frequency-division loss is considered as the total loss for constraining the network training procedure. Validations of the proposed method have been performed on two brain datasets. Visual results and quantitative evaluations show that the proposed DFDN algorithm has better performance in sparse MRI reconstruction than other comparative methods. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				APR 7	2020	384						346	355		10.1016/j.neucom.2019.12.011													
J								Two-level structure swarm formation system with self-organized topology network	NEUROCOMPUTING										Self-organized formation network; Relation-invariable persistent formation (RIPF); Two-level control system; Neural-dynamic based model predictive control (MPC)	OPTIMIZATION	In this work, a two-level mobile robot swarm system with self-organized formation network is proposed. Initially, based on the position information of the robots, a relation-invariable persistent formation (RIPF) algorithm can automatically organize the swarm network and construct an optimal persistent formation. At the upper formation planning level, the collision-free reference paths of the swarm can be planned for guiding the robots to reach and maintain a desired distance with their neighbors. Then, at the lower formation tracking control level, a neural-dynamic combined model predictive control (MPC) method is applied to drive the swarm moving on the reference paths. The MPC can reformulate the system into a convex minimization problem, which can further be transformed into a constrained quadratic programming (QP) problem such that an efficient QP solver, called primal-dual neural network (PDNN), is implemented to obtain the optimal control inputs online for the robots. In the end, simulation results show the effectiveness of the proposed formation system. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						356	367		10.1016/j.neucom.2019.11.053													
J								Finite-time synchronization of delayed fractional-order heterogeneous complex networks	NEUROCOMPUTING										Fractional-order systems; Heterogeneous complex networks; Finite-time synchronization; Discontinuous feedback controller	MITTAG-LEFFLER STABILITY; NEURAL-NETWORKS; SYSTEMS; DESIGN	This paper is devoted to exploring the finite-time (FET) synchronization problem of time-varying delay fractional-order (FO) coupled heterogeneous complex networks (TFCHCNs) with external interference via a discontinuous feedback controller. Firstly, we propose a novel Lemma which is useful for discussing the FET stability and synchronization problem of FO systems. Secondly, based on the proposed Lemma, a discontinuous feedback controller is designed to guarantee the FET synchronization of TFCHCNs with external interference. Moreover, the upper bound of settling-time function is obtained. Finally, two simulation examples are provided to verify the practicability of our findings. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 7	2020	384						368	375		10.1016/j.neucom.2019.11.043													
J								Soft plus plus , a multi-parametric non-saturating non-linearity that improves convergence in deep neural architectures	NEUROCOMPUTING										Activation function; Deep learning; Error backpropagation; Multi-layer perceptron; Convolutional neural network	NETWORKS; NETS	A key strategy to enable training of deep neural networks is to use non-saturating activation functions to reduce the vanishing gradient problem. Popular choices that saturate only in the negative domain are the rectified linear unit (ReLU), its smooth, non-linear variant, Softplus, and the exponential linear units (ELU and SELU). Other functions are non-saturating across the entire real domain, like the linear parametric ReLU (PReLU). Here we introduce a nonlinear activation function called Soft++ that extends PReLU and Softplus, parametrizing the slope in the negative domain and the exponent. We test identical network architectures with ReLU, PReLU, Softplus, ELU, SELU, and Soft++ on several machine learning problems and find that: i) convergence of networks with any activation function depends critically on the particular dataset and network architecture, emphasizing the need for parametrization, which allows to adapt the activation function to the particular problem; ii) non-linearity around the origin improves learning and generalization; iii) in many cases, non-saturation across the entire real domain further improves performance. On very difficult learning problems with deep fully-connected and convolutional networks, Soft++ outperforms all other activation functions, accelerating learning and improving generalization. Its main advantage lies in its dual parametrization, offering flexible control of the shape and gradient of the function. (C) 2019 The Author(s). Published by Elsevier B.V.																	0925-2312	1872-8286				APR 7	2020	384						376	388		10.1016/j.neucom.2019.12.014													
J								An inverse model-based multiobjective estimation of distribution algorithm using Random-Forest variable importance methods	COMPUTATIONAL INTELLIGENCE										estimation of distribution algorithm; Gaussian process; inverse modeling; multiobjective optimization; random forest variable importance	EVOLUTIONARY ALGORITHM; OPTIMIZATION	Most existing methods of multiobjective estimation of distributed algorithms apply the estimation of distribution of the Pareto-solution on the decision space during the search and little work has proposed on making a regression-model for representing the final solution set. Some inverse-model-based approaches were reported, such as inversed-model of multiobjective evolutionary algorithm (IM-MOEA), where an inverse functional mapping from Pareto-Front to Pareto-solution is constructed on nondominated solutions based on Gaussian process and random grouping technique. But some of the effective inverse models, during this process, may be removed. This paper proposes an inversed-model based on random forest framework. The main idea is to apply the process of random forest variable importance that determines some of the best assignment of decision variables (x(n)) to objective functions (f(m)) for constructing Gaussian process in inversed-models that map all nondominated solutions from the objective space to the decision space. In this work, three approaches have been used: classical permutation, Naive testing approach, and novel permutation variable importance. The proposed algorithm has been tested on the benchmark test suite for evolutionary algorithms [modified Deb K, Thiele L, Laumanns M, Zitzler E (DTLZ) and Walking Fish Group (WFG)] and indicates that the proposed method is a competitive and promising approach.																	0824-7935	1467-8640															10.1111/coin.12315		APR 2020											
J								Neuromodulated multiobjective evolutionary neurocontrollers without speciation	EVOLUTIONARY INTELLIGENCE										Artificial neural network; Hebbian learning; Multiobjective; NEAT-MODS; Neuromodulation; Speciation	NEUROEVOLUTION	Neuromodulation is a biologically-inspired technique that can adapt the per-connection learning rates of synaptic plasticity. Neuromodulation has been used to facilitate unsupervised learning by adapting neural network weights. Multiobjective evolution of neural network topology and weights has been used to design neurocontrollers for autonomous robots. This paper presents a novel multiobjective evolutionary neurocontroller with unsupervised learning for robot navigation. Multiobjective evolution of network weights and topologies (NEAT-MODS) is augmented with neuromodulated learning. NEAT-MODS is an NSGA-II based multiobjective neurocontroller that uses two conflicting objectives. The first rewards the robot when it moves in a direct manner with minimal turning; the second objective is to reach as many targets as possible. NEAT-MODS uses speciation, a selection process that aims to ensure Pareto-optimal genotypic diversity and elitism. The effectiveness of the design is demonstrated using a series of experiments with a simulated robot traversing a simple maze containing target goals. It is shown that when neuromodulated learning is combined with multiobjective evolution, better-performing neural controllers are synthesized than by evolution alone. Secondly, it is demonstrated that speciation is unnecessary in neuromodulated neuroevolution, as neuromodulation preserves topological innovation. The proposed neuromodulated approach is found to be statistically superior to NEAT-MODS alone when applied to solve a multiobjective navigation problem.																	1864-5909	1864-5917															10.1007/s12065-020-00394-9		APR 2020											
J								A survey on exponential random graph models: an application perspective	PEERJ COMPUTER SCIENCE										Exponential random graph models survey; Exponential random graphs; ERGM; ERGMs' survey; ERGMs' applications	P-ASTERISK MODELS; SOCIAL NETWORKS; LOGIT-MODELS; TOPOLOGICAL PROPERTIES; LOGISTIC REGRESSIONS; BAYESIAN-INFERENCE; COLLABORATION; CONNECTIVITY; GOVERNANCE; MARKETS	The uncertainty underlying real-world phenomena has attracted attention toward statistical analysis approaches. In this regard, many problems can be modeled as networks. Thus, the statistical analysis of networked problems has received special attention from many researchers in recent years. Exponential Random Graph Models, known as ERGMs, are one of the popular statistical methods for analyzing the graphs of networked data. ERGM is a generative statistical network model whose ultimate goal is to present a subset of networks with particular characteristics as a statistical distribution. In the context of ERGMs, these graph's characteristics are called statistics or configurations. Most of the time they are the number of repeated subgraphs across the graphs. Some examples include the number of triangles or the number of cycle of an arbitrary length. Also, any other census of the graph, as with the edge density, can be considered as one of the graph's statistics. In this review paper, after explaining the building blocks and classic methods of ERGMs, we have reviewed their newly presented approaches and research papers. Further, we have conducted a comprehensive study on the applications of ERGMs in many research areas which to the best of our knowledge has not been done before. This review paper can be used as an introduction for scientists from various disciplines whose aim is to use ERGMs in some networked data in their field of expertise.																	2376-5992					APR 6	2020									e269	10.7717/peerj-cs.269													
J								Safety requirements vs. crashing ethically: what matters most for policies on autonomous vehicles	AI & SOCIETY										Autonomous vehicles; Self-driving vehicles; Ethical crashing; Trolley problem; Safety argument; Vision zero	TROLLEY; RESPONSIBILITY	The philosophical-ethical literature and the public debate on autonomous vehicles have been obsessed with ethical issues related to crashing. In this article, these discussions, including more empirical investigations, will be critically assessed. It is argued that a related and more pressing issue is questions concerning safety. For example, what should we require from autonomous vehicles when it comes to safety? What do we mean by 'safety'? How do we measure it? In response to these questions, the article will present a foundation for a continued discussion on these issues and an argument for why discussions about safety should be prioritized over ethical concerns related to crashing.																	0951-5666	1435-5655															10.1007/s00146-020-00964-6		APR 2020											
J								Automatic liver cancer detection in abdominal liver images using soft optimization techniques	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										ICA-GLCM; FFNN; Watershed algorithm; Liver cancer; Fuzzy K-means algorithm		The liver is present underneath the diaphragm and extends from right to left upper part of the belly. The liver is an organ which has many responsibilities for producing different chemicals needed for physical body. The conversion of an image into information is helpful for research people to share. The method of conversion prevents manual error because it depends on technology and algorithm. This paper deals with the detection of liver cancer as implemented using optimization techniques. The paper discusses scanning, filtering, segmentation, feature extraction, and exhibit through artificial neural network. In real time data sets, feed forward neural network is applied for classification and detection of liver cancer. Filtering is mainly used to reduce noise and smoothing edges. Then segmentation is used to extract needed region so that result can be stored in less storage space. Feature extraction is done through gray level and co-occurrence matrix that can yield the result in various parameters. This matrix is helpful to differentiate the tumors namely benign and malignant. Artificial neural network is trained to differentiate those tumors. The result is analyzed by following the parameters like accuracy, area, correlation, entropy, homogeneity, contrast, and similarity index.																	1868-5137	1868-5145															10.1007/s12652-020-01885-4		APR 2020											
J								Integration of RNN with GARCH refined by whale optimization algorithm for yield forecasting: a hybrid machine learning approach	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										BPNN; RNN; WOA; GARCH; Volatility clustering	NEURAL-NETWORK; MODEL; RAINFALL; ARIMA	Forecasting yield is a challenging task in all agricultural crops. So, it is imperative to develop a machine learning hybrid model with available data for yield forecasting. The main objective of this research is to develop a novel hybrid model for forecasting the sugarcane yield on non-linear time series data. Recurrent neural network typically holds a long memory that allows sufficient forecasts with a fewer number of parameters. The weights and thresholds of the recurrent neural network are optimized by whale optimization algorithm to improve the efficiency of the neural network, and thus obtaining accurate results. Consecutively increasing the performance in forecasting volatility of time series is a challenging task. BPNN-GARCH, RNN-GARCH, and novel WOARNN-GARCH referred to as hybrid models; fitted with precipitation as well as sugarcane yield data and a novel method was found more appropriate for forecasting the volatility of rainfall as well as sugarcane yield for medium-term which facilitated in observing the future incidences for the next few years. Combining a statistical model like generalized autoregressive conditional heteroskedasticity with recurrent neural network refined by whale optimization algorithm grants a novelty of predicting the yield with volatility in time series analysis.																	1868-5137	1868-5145															10.1007/s12652-020-01922-2		APR 2020											
J								An automated exploring and learning model for data prediction using balanced CA-SVM	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Supporting vector machine; Learning model; Balanced CA-SVM		The rainfall prediction is important for metrological department as it closely associated with our environment and human life. An accuracy of rainfall prediction has great important for countries like India whose economy is dependent on agriculture. Because of dynamic nature of atmosphere, statistical techniques fail to predict rainfall information. The process of support vector machine (SVM) is to find an optimal boundary also known as hyper plane in which separates the samples (examples in a dataset) of different classes by a maximum margin. The proposed model uses the dynamic integrated model for exploring and learning large amount of data set. Balanced communication-avoiding support vector machine (CA-SVM) prediction model is proposed to achieve better performance and accuracy with limited number of iteration without any error. The rain fall dataset is used for performance evaluation. The proposed model starts with independent sample to the integrated samples without any collision in prediction. The proposed algorithm achieves 89% of accuracy when compared to the existing algorithms. The simulations demonstrate that prediction models indicate that the performance of the proposed algorithm Balanced CA-SVM has much better accuracy than the local learning model based on a set of experimental data if other things are equal. On the other hand, simulation results demonstrate the effectiveness and advantages of the Balanced CA-SVM model used in machine learning and further promises the scope for improvement as more and more relevant attributes can be used in predicting the dependent variables.																	1868-5137	1868-5145															10.1007/s12652-020-01937-9		APR 2020											
J								Nitrogen Deficiency Prediction of Rice Crop Based on Convolutional Neural Network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Nitrogen deficiency prediction; CNN; SVM; Statistical analysis; Wilcoxon signed-rank test; LCC; Rice plant		Nitrogen (N) concentration is a significant parameter to check the status of health in rice crop. Nitrogen (N) plays an essential role in the growth and productivity of rice plant. This paper proposes a convolutional neural network (CNN) based approach for prediction of rice nitrogen deficiency. The pre-trained CNN architecture is modified to improve the classification accuracy with the inclusion of pre-eminent classifier like support vector machine (SVM) by replacing the last output layer of CNN. Here, six leading deep learning architectures such as ResNet-18, ResNet-50, GoogleNet, AlexNet, VGG-16 and VGG-19 with SVM are used for prediction of nitrogen deficiency with 5790 number image samples. The performance of each classifier is measured and compared in terms of accuracy, sensitivity, specificity, false positive rate (FPR) and F1 score. Again, the statistical analysis is performed to choose the better classification model considering the results of 100 independent simulations. The statistical analysis confirmed the superiority of ResNet-50+SVM than the other five CNN-based classification models with an accuracy of 99.84%. Besides, the accuracy score of CNN classification models is compared with other traditional image classification models such as bag-of-feature, colour feature + SVM, local binary patterns (LBP) + SVM, histogram of oriented gradients (HOG)+SVM and Gray Level Co-occurrence Matrix (GLCM)+SVM.																	1868-5137	1868-5145															10.1007/s12652-020-01938-8		APR 2020											
J								Efficient segmentation of the lung carcinoma by adaptive fuzzy-GLCM (AF-GLCM) with deep learning based classification	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Image processing; Image segmentation; Adaptive fuzzy-GLCM; GoogLeNet CNN architecture; Fluorescence bronchoscopy	BRONCHOSCOPY; NODULES	Image processing is an innovative method to convert the real image into a sharp digital image by applying various functions upon it. However, it is a difficult task for physicians in the medical field. The significant difficulty is with the segmentation of images due to the blurred contrast and artifacts at the boundary edges. Hence in this paper, an efficient and adaptive fuzzy-GLCM based segmentation method was proposed. The images derive from the process of bronchoscopy. The ultimate goal of the proposed methodology was the accurate recognition of the lung carcinoma, which undergoes segmentation. The adaptive F-GLCM segmentation method enables the early and easy detection of lung cancer, which helps both the physicians and the patients for proper initial medication. Then the classification was done with the help of the GoogLeNet CNN architecture, which will reveal whether the cancerous growth was in a benign or in a malignant stage. Then the performance analysis of the proposed method was measured by comparing it with the other existing methodology.																	1868-5137	1868-5145															10.1007/s12652-020-01874-7		APR 2020											
J								Effective and efficient multitask learning for brain tumor segmentation	JOURNAL OF REAL-TIME IMAGE PROCESSING										Brain tumor segmentation; Image segmentation; Deep learning; Multitask learning	NEURAL-NETWORKS; STABILITY	Recently, brain tumor segmentation has achieved great success, partially because of deep learning-based relation exploration and multiscale analysis. However, the computational complexity hinders the real-time application. In this paper, we propose a revised multitask learning approach in which a lightweight network with only two scales is adopted to segment different kinds of tumor regions. Moreover, we design a hybrid hard sampling method that considers both sample sparsity and effectiveness. Extensive experiments on the BraTS19 segmentation challenge dataset have shown that our proposed method improves the Dice coefficient by a margin of 0.4-1.0 for different kinds of brain tumor regions and obtains results that are competitive with state-of-the-art brain tumor segmentation approaches.																	1861-8200	1861-8219															10.1007/s11554-020-00961-4		APR 2020											
J								Incrementally updating approximations based on the graded tolerance relation in incomplete information tables	SOFT COMPUTING										Three-way decision; Rough set; Three-way approximations; Graded tolerance relation; Incomplete information; Incremental learning	ROUGH SET APPROACH; 3-WAY DECISION; ATTRIBUTE GENERALIZATION; CLASSIFICATION	The incremental learning methods based on rough set theory are effective in acquiring knowledge in dynamically changing information tables. In this paper, we focus on the effective acquisition of decision rules by incrementally updating approximations when an incomplete information table changes. First of all, we present a four-step model to obtain three-way decision rules in an incomplete information table based on the graded tolerance relation. The first step presents the graded tolerance relation between objects. The second step calculates the degrees of objects belonging to approximations by using fuzzy logic operators. Besides, we propose a relation matrix to calculate the degrees efficiently. The third step gets three-way approximations by applying a pair of thresholds to the degrees. The fourth step obtains three-way decision rules based on the descriptions of objects. According to the four-step model, we find the notion of approximations plays an essential role in rule acquisition. Incrementally updating approximations are an effective method to obtain decision rules when an incomplete information changes. Accordingly, we study the incrementally updating approximations by incrementally updating the relation matrix when changing attributes, objects, and the attribute value of an object. Finally, experimental results illustrate that the incremental methods are more effective than non-incremental methods.																	1432-7643	1433-7479				JUN	2020	24	12			SI		8655	8671		10.1007/s00500-020-04838-3		APR 2020											
J								A latent variable model for two-dimensional canonical correlation analysis and the variational inference	SOFT COMPUTING										Canonical correlation analysis; Probabilistic dimension reduction; Matrix-variate distribution; Variational expectation maximization	PROBABILISTIC PCA; MIXTURES	The probabilistic dimension reduction has been and is a major concern. Probabilistic models provide a better interpretability of the dimension reduction methods and present a framework for their further extensions. In pattern recognition problems, data that have a matrix or tensor structure is initially transformed into a vector format. This eliminates the internal structure of the data. The available perspective is to maintain the internal structure of each data while reducing the dimensionality, which can reduce the small sample size problem. Canonical correlation analysis is one of the most important techniques in dimension reduction in multi-view data. A two-dimensional canonical correlation analysis as an extension of canonical correlation analysis has been proposed to preserve the matrix structure of the data. Here, a new probabilistic framework for two-dimensional canonical correlation analysis is proposed, where the matrix-variate distributions are applied to model the relation between the latent matrix and the two-view matrix-variate observed data. These distributions, specific to the matrix data, can provide better understanding of two-dimensional canonical correlation analysis and pave the way for further extensions. In general, there does not exist any analytical maximum likelihood solution for this model; therefore, here the two approaches, one based on the expectation maximization and other on variational expected maximization, are proposed for learning the model parameters. The synthetic data are applied to evaluate the convergence and quality of the mapping of these algorithms. The functionalities of these methods and their counterparts are compared on the real face datasets.																	1432-7643	1433-7479				JUN	2020	24	12			SI		8737	8749		10.1007/s00500-020-04906-8		APR 2020											
J								Enhancement of network lifetime using fuzzy clustering and multidirectional routing for wireless sensor networks	SOFT COMPUTING										Clustering; Fuzzy logic; Cluster head (CH); Multidirectional routing; LEACH	PROTOCOL; ENERGY; ARCHITECTURE	Wireless sensors are those devices which sense any physical quantity. Group of sensor node working together can be termed as a wireless sensor network (WSN). The lifetime of WSN is critical and is based greatly on energy consumption for data transmission. By using the available energy efficiently, the nodes can operate for a longer time thereby increasing the network lifetime. In this paper, fuzzy-based clustering is used for clustering, which selects an optimal cluster head (CH). Fuzzy clustering is made with the help of residual energy and distance as fuzzy descriptors. However, when the network is heterogeneous, fuzzy-based clustering is found in efficient in many cases. To improve efficiency, a multidirectional routing is proposed along with fuzzy clustering. Using multidirectional routing, possible multiple paths between node and BS will be found out and the path with least hop will be selected as a routing path. The simulation results have been compared with traditional low energy adaptive clustering hierarchy, and a significant improvement in the lifetime of a node has been observed.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11805	11818		10.1007/s00500-020-04900-0		APR 2020											
J								A new hybridization of DBSCAN and fuzzy earthworm optimization algorithm for data cube clustering	SOFT COMPUTING										Data cube; Dimension move; DBSCAN clustering; Fuzzy logic controller; Dynamic tuning parameters; Earthworm optimization algorithm		Data aggregation from different databases into a data warehouse creates multidimensional data such as data cubes. With regard to the 3D structure of data, data cube clustering has significant challenges to perform on data cube. In this paper, new preprocessing techniques and a novel hybridization of DBSCAN and fuzzy earthworm optimization algorithm (EWOA) are proposed to solve the challenges. Proposed preprocessing consists of an assigned address to each cube cell and dimension move to create a related 2D data from the data cube and new similarity metric. The DBSCAN algorithm, as a density-based clustering algorithm, is adopted based on both Euclidean and newly proposed similarity metric, which are called DBSCAN1 and DBSCAN2 for the related 2D data. A new hybridization of the EWOA and DBSCAN is proposed to improve the DBSCAN, and it is called EWOA-DBSCAN. Also, to dynamically tune parameters of EWOA, a fuzzy logic controller is designed with two fuzzy group rules of Mamdani (EWOA-DBSCAN-Mamdani) and Sugeno (EWOA-DBSCAN-Sugeno), separately. These ideas are proposed to present efficient and flexible unsupervised analysis for a data cube by utilizing a meta-heuristic algorithm to optimize DBSCAN's parameters and increasing the efficiency of the idea by applying dynamic tuning parameters of the algorithm. To evaluate the efficiency, the proposed algorithms are compared with DBSCAN1 and GA-DBSCAN1, GA-DBSCAN1-Mamdani and GA-DBSCAN1-Sugeno. The experimental results, consisting of 20 runs, indicate that the proposed ideas achieved their targets.																	1432-7643	1433-7479				OCT	2020	24	20					15529	15549		10.1007/s00500-020-04881-0		APR 2020											
J								Diversified viral marketing: The power of sharing over multiple online social networks	KNOWLEDGE-BASED SYSTEMS										Viral marketing; Diffusion model; Social networks; Advertisement; Influence maximization	INFLUENCE MAXIMIZATION; SENTIMENT ANALYSIS; MODELS; USERS	The popularity of online social networks (OSNs) makes them attractive platforms to advertise products. Previous work on marketing in OSNs utilized older diffusion models that do not capture the interactions of modern OSNs and hence there is a need to develop a model that accounts for the interactions that occur in current OSNs. In this paper, we introduce a new model for information flow in online social networks that captures the sharing behavior exercised by users when they pass information from one online social network to their social circles in another network. We, then, formulate a problem of maximizing the marketing reach where the diversity of users' other social networks is taken as a constraint. We also propose a greedy algorithm to solve the aforementioned optimization problem. Numerical results show that the proposed algorithm achieves better results than algorithms that are based on classical degree centrality metric and with comparable running time. (c) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105430	10.1016/j.knosys.2019.105430													
J								Variable precision rough set based unsupervised band selection technique for hyperspectral image classification	KNOWLEDGE-BASED SYSTEMS										Dimensionality reduction; Feature selection; Hyperspectral image; Rough set; Support vector machines	FEATURE-EXTRACTION; SUBSET; REDUCTION; FRAMEWORK	Unsupervised band selection is still a relevant research topic for mitigating certain challenges of hyperspectral image classification. In this paper, a greedy unsupervised hyperspectral band selection technique is proposed based on variable precision rough set (VPRS). The proposed technique defined a novel dependency measure by exploiting VPRS. Furthermore, the dependency measure is defined in such a way that it became less sensitive to the degree of misclassification parameter beta in VPRS. Our technique first computed the similarity between every pair of bands using the proposed dependency measure and selected a band from the pair that produced maximum similarity value. After that a novel criterion is proposed to select the informative bands one-by-one by adopting first order incremental search. The effectiveness of the proposed band selection technique is assessed by comparing it with five state-of-the-art techniques using three hyperspectral data sets. (c) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105414	10.1016/j.knosys.2019.105414													
J								Cross-modal recipe retrieval via parallel- and cross-attention networks learning	KNOWLEDGE-BASED SYSTEMS										Recipe retrieval; Parallel-attention network; Cross-attention network; Cross-modal retrieval		Cross-modal recipe retrieval refers to the problem of retrieving a food image from a list of image candidates given a textual recipe as the query, or the reverse side. However, existing cross-modal recipe retrieval approaches mostly focus on learning the representations of images and recipes independently and sewing them up by projecting them into a common space. Such methods overlook the interplay between images and recipes, resulting in the suboptimal retrieval performance. Toward this end, we study the problem of cross-modal recipe retrieval from the viewpoint of parallel- and cross-attention networks learning. Specifically, we first exploit a parallel-attention network to independently learn the attention weights of components in images and recipes. Thereafter, a cross-attention network is proposed to explicitly learn the interplay between images and recipes, which simultaneously considers word-guided image attention and image-guided word attention. Lastly, the learnt representations of images and recipes stemming from parallel- and cross-attention networks are elaborately connected and optimized using a pairwise ranking loss. By experimenting on two datasets, we demonstrate the effectiveness and rationality of our proposed solution on the scope of both overall performance comparison and micro-level analyses. (c) 2019 Published by Elsevier B.V.																	0950-7051	1872-7409				APR 6	2020	193								105428	10.1016/j.knosys.2019.105428													
J								Appearance and shape based image synthesis by conditional variational generative adversarial network	KNOWLEDGE-BASED SYSTEMS										Image synthesis; Deep generative models; Variational inference; Generative adversarial network		Person image synthesis based on shape and appearance using deep generative models opens the door in mickle applications, such as person re-identification (ReID) and movie industry. The methods of image synthesis are driven by producing the image of an object directly, which fail to recover spatial deformations when images are generated. In this paper, we present a conditional variational generative adversarial network (CVGAN) to synthesize desired images guided by target shape by modeling the inherent interplay between shape and appearance. Firstly, the shape and appearance of the given images are disentangled by adopting variational inference, which enables us to generate person images with arbitrary shapes. Secondly, to preserve the details and generate photo-realistic images, the Kullback-Leibler (KL) loss is adopted to reduce the gap between the condition image and generated image. Thirdly, to prevent partly gradient vanishing problem for training our framework stably, we propose combined general learning method, where the discriminative network leverages least squares loss. In addition, we experiment on COCO, DeepFashion and Market-1501 datasets, and results demonstrate that VGAN significantly improves the synthesis of images on discriminability, diversity and quality over the existing methods. (c) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105450	10.1016/j.knosys.2019.105450													
J								Dense members of local cores-based density peaks clustering algorithm	KNOWLEDGE-BASED SYSTEMS										Dense members; Local cores; Density peaks; Clustering	FAST SEARCH; FIND	An efficient clustering algorithm by fast search and find of density peaks (DP) was proposed and attracted much attention from researchers. It assumes that cluster centers are surrounded by lower density points and have a larger distance from points with higher densities. According to the characteristic of cluster centers, we can easily obtain centers from decision graph. However, DP algorithm fails to cluster manifold data sets, especially when there are a lot of noises in the manifold data sets. In this paper, we propose a dense members of local core-based density peaks clustering algorithm DLORE-DP. First, we find local cores to represent the data set. After that, only dense members of local cores are taken into consideration when computing the graph distance between local cores, avoiding the interference of noises. Then, natural neighbor-based density and the new defined graph distance are used to construct decision graph on local cores and DP algorithm is employed to cluster local cores. Finally, we assign each remaining point to the cluster its representative belongs to. The new defined graph distance helps our algorithm cluster manifold data sets and the elimination of low density points makes it more robust. Moreover, since we only calculate the graph distance between local cores, instead of all pairs of points, it greatly reduces the running time. The experimental results on synthetic and real data sets show that DLORE-DP is more effective, efficient and robust than other algorithms when clustering manifold data sets with noises. (c) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105454	10.1016/j.knosys.2019.105454													
J								The bare-bones differential evolutionary for stochastic joint replenishment with random number of imperfect items	KNOWLEDGE-BASED SYSTEMS										B2C; Multi-item; Stochastic demand; Imperfect item; Evolutionary algorithm	INVENTORY MODEL; OPTIMIZATION ALGORITHM; SEARCH ALGORITHM; RFID TECHNOLOGY; SUPPLY CHAIN; INVESTMENT; DEMAND; POLICY; IDENTIFICATION; PARAMETERS	In e-business running, not only uncertainties in customers' demands cause great risks from the marketing side, but also imperfect qualities of multi-item induce great losses at the supplying side. Hence, in this paper a joint replenishment problem (IJRP) simultaneously considering the stochastic demands and random number of imperfect items for the first time is proposed and a meta-heuristic, namely, bare-bones differential evolutionary (BBDE) algorithm is redesigned based on the solution structure containing the continuous variable and discrete variable to solve the IJRP problem. Through intensive numerical experiments, it has been testified that BBDE is not only superior to genetic evolution algorithm (GA) and particle swarm optimization (PSO) algorithm at the best-found TC, the lowest mean value, and the smallest standard error for three small tested JRPs (GJRP, SJRP and IJRP), but also a competitive algorithm to differential evolution (DE) and bare bones PSO (BBPSO) for three small scale JRPs (GJRP, SJRP and IJRP) at the aspects of finding the best-found TC, the lowest mean value, the smallest standard error and the least of computation time. BBDE also shows great efficiency in solving some large scale IJRPs comparing to DE and BBPSO, the limitation of BBDE for large scale IJRP is also reported and analyzed. (c) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105416	10.1016/j.knosys.2019.105416													
J								A temporal-window framework for modelling and forecasting time series	KNOWLEDGE-BASED SYSTEMS										Selection; Forecasting; Time series; Pattern recognition; Time-window; Dynamic time warping	SUPPORT VECTOR MACHINES; NEURAL-NETWORK; FEATURE-SELECTION; HYBRID ARIMA; ANN MODEL; SYSTEM; GENERATION	Time series have become a valuable source of study in many areas, mainly because it encapsulates some underlying time-index variables. A significant part of these studies is dedicated to fit a single model to the past data to forecast future values of the series. However, single models may not be able to adequately fit local patterns; that is, particular and eventually recurrent variations dynamically incorporated in the series as time evolves. This temporal-window oriented paradigm has been at the vanguard of time series modelling and forecasting exercises. The present paper proposes a simple local-pattern oriented system to model and forecast time series. Our approach involves three steps: (i) the time series is split into k subsets in such a way that each subset may intercept its neighbours; (ii) each subset is modelled, considering lags according to confidence intervals of the auto-correlation function; and (iii) pattern recognition of the target values of the time series in relation to the modelled subsets, via dynamic time warping. The usefulness of the proposed framework is illustrated by modelling and forecasting real-world time series. Evaluation metrics were adopted to compare the proposed approach with multilayer perceptron neural networks and support vector regression predictors. The results provided by published models are also taken into account and it was found that the proposed system presented better performance than the compared models in the experiments. (c) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105476	10.1016/j.knosys.2020.105476													
J								Deep multi-granularity graph embedding for user identity linkage across social networks	KNOWLEDGE-BASED SYSTEMS										Granular computing; Graph embedding; Social network analysis; User identity linkage	HIERARCHICAL ORGANIZATION; KNOWLEDGE; PREDICTION	There have been increasing interests in user identity linkage (UIL) across social networks since it supports many applications such as cross-net recommendation, link prediction, and network fusion. Existing graph embedding based techniques cannot sufficiently model the higher-order structural properties in UIL. Moreover, the very limited supervisory anchor pairs (SAP), which are crucial for the task of UIL across social networks, are not utilized effectively. In this paper, a novel framework named multi-granularity graph embedding (MGGE) is proposed. And as an extension, a deep multi-granularity graph embedding model (DeepMGGE) is further developed. DeepMGGE uses the random walk (RW) to capture the higher-order structural proximities which is ignored by IONE Liu et al. (2016). Besides, DeepMGGE employs a heuristic edge-weighting mechanism given by deep learning to better capture the non-linear SAP-oriented structural properties. Experiments on real social networks demonstrate that the DeepMGGE outperforms state-of-the-art methods. (c) 2019 Published by Elsevier B.V.																	0950-7051	1872-7409				APR 6	2020	193								105301	10.1016/j.knosys.2019.105301													
J								A deep-learning approach to mining conditions	KNOWLEDGE-BASED SYSTEMS										Natural language processing; Text mining; Condition mining; Neural networks	GRADIENT	A condition is a constraint that determines when a consequent holds. Mining them in text is paramount to understand many sentences properly. In the literature, there are a few pattern-based proposals that fall short regarding recall because it is not easy to characterise unusual ways to express conditions with hand-crafted patterns; there is one machine-learning proposal that is bound to the Japanese language, requires specific-purpose dictionaries, taxonomies, and heuristics, works on opinion sentences only, and was evaluated very shallowly. In this article, we present a deep-learning proposal to mine conditions that does not have any of the previous drawbacks; furthermore, we have performed a comprehensive experimental study on a large multi-lingual dataset on many common topics; our conclusion is that our proposals are similar to the state of the art in terms of precision, but improve recall enough to beat them in terms of F-1 score. (c) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105422	10.1016/j.knosys.2019.105422													
J								Siamese attentional keypoint network for high performance visual tracking	KNOWLEDGE-BASED SYSTEMS										Visual tracking; Siamese hourglass networks; Cross-attentional module; Keypoint detection	OBJECT TRACKING	Visual tracking is one of the most fundamental topics in computer vision. Numerous tracking approaches based on discriminative correlation filters or Siamese convolutional networks have attained remarkable performance over the past decade. However, it is still commonly recognized as an open research problem to develop robust and effective trackers which can achieve satisfying performance with high computational and memory storage efficiency in real-world scenarios. In this paper, we investigate the impacts of three main aspects of visual tracking, i.e., the backbone network, the attentional mechanism, and the detection component, and propose a Siamese Attentional Keypoint Network, dubbed SATIN, for efficient tracking and accurate localization. Firstly, a new Siamese lightweight hourglass network is specially designed for visual tracking. It takes advantage of the benefits of the repeated bottom-up and top-down inference to capture more global and local contextual information at multiple scales. Secondly, a novel cross-attentional module is utilized to leverage both channel-wise and spatial intermediate attentional information, which can enhance both discriminative and localization capabilities of feature maps. Thirdly, a keypoints detection approach is invented to trace any target object by detecting the top-left corner point, the centroid point, and the bottom-right corner point of its bounding box. Therefore, our SATIN tracker not only has a strong capability to learn more effective object representations, but also is computational and memory storage efficiency, either during the training or testing stages. To the best of our knowledge, we are the first to propose this approach. Without bells and whistles, experimental results demonstrate that our approach achieves state-of-the-art performance on several recent benchmark datasets, at a speed far exceeding 27 frames per second. (c) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105448	10.1016/j.knosys.2019.105448													
J								Community-oriented attributed network embedding	KNOWLEDGE-BASED SYSTEMS										Representation learning; Attributed network embedding (ANE); Community detection; Topic model	REPRESENTATION; CLASSIFICATION	Network embedding aims to map vertices in a complex network into a continuous low-dimensional vector space. Meanwhile, the original network structure and inherent properties must be preserved. Most of the existing methods merely focus on preserving local structural features of vertices, whereas they largely ignore the community patterns and rich attribute information. For example, the title of papers in an academic citation network could imply their research directions, which are potentially valuable in seeking more meaningful representations of these papers. In this paper, we propose a Community-oriented Attributed Network Embedding (COANE) framework, which can smoothly incorporate the community information and text contents of vertices into network embedding. We design a margin-based random walk procedure on the network coupled with flexible margins among communities, which limit the scope of random walks. Inspired by the analogy between vertex sequences and documents, the statistical topic model is adopted to extract community features in the network. Furthermore, COANE integrates textual semantics into representations through the topic model while preserving their structural correlations. Experiments on real-world networks indicate that our proposed method outperforms six state-of-the-art network embedding approaches on network visualization, vertex classification and link prediction. (c) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105418	10.1016/j.knosys.2019.105418													
J								Disagreement based semi-supervised learning approaches with belief functions	KNOWLEDGE-BASED SYSTEMS										Machine learning; Semi-supervised learning; Belief functions; Uncertainty		In many machine learning tasks, it is usually difficult to obtain enough labeled samples. Semi-supervised learning that exploits unlabeled samples in addition to labeled ones has attracted a lot of research attentions. Traditional semi-supervised methods may encounter uncertainty problems and information loss when dealing with those samples having ambiguous class belongingness. In this paper, the uncertainties encountered in semi-supervised learning are addressed using the theory of belief functions, and semi-supervised learning methods based on belief functions are proposed. The proposed methods label the unlabeled data with belief modeling. They can effectively use limited supervised information to facilitate the classification process. Experimental results based on benchmark data sets show that the proposed approaches can effectively exploit unlabeled data and perform better compared with prevailing approaches. (c) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105426	10.1016/j.knosys.2019.105426													
J								Minimizing the influence of rumors during breaking news events in online social networks	KNOWLEDGE-BASED SYSTEMS										Breaking news; Influence minimization; Online social networks; Rumor propagation model; Markov chain; Survival theory	PROPAGATION; SPREAD; MODEL	The malicious rumors have tremendously attracted a more substantial number of researchers to join the fight against the propagation of these types of information in online social networks (OSNs). The spread of rumors has a severe impact on society, which can creates political conflicts, shape public opinion and weakens their trust in governments; therefore, it must be stopped as soon as it is detected. This paper investigates the problem of minimizing the influence of malicious rumors that emerge during breaking news, which are characterized by the dissemination of a large number of malicious information over a short period. Therefore, we introduce the problem of multi-rumors influence minimization (MRIM) in OSNs and propose a solution to it. To this end, we design a multi-rumor propagation model named the HISBMmodel that captures the propagation process of multi-rumors in OSNs. Moreover, we present a new formulation of an individual's opinion toward a rumor based on a Markov chain representation, which adds a layer of realism to the proposed model. Subsequently, we propose a dynamic blocking period (DBP) approach as a solution for the MRIM problem. The main objective is to minimize both the spread and the influence of these rumors in OSNs. The proposed method selects and blocks nodes that most likely to spread a large number of rumors and support them. Different from existing methods, the proposed solution does not block nodes for an unlimited period, but this period is estimated according to the high activity of a node in an OSN. The survival theory has been exploited in this work to provide a solution formulated from the perspective of probabilistic inference of networks. Consequently, an algorithm has been proposed based on a likelihood principle to select the target nodes, which guarantees a (1 - 1/e)-approximation of the optimal solution. The experimental results show that the HISBMmodel could capture the propagation of multi-rumor propagation more accurately than classical models and provides metrics to assess the impact of rumors efficiently. Moreover, the results show the outstanding performance of the proposed approach compared to the other solution in the literature. The experimental results show that in the worst-case, the DBP achieves on an average 37.66% reduction on the impact of rumors, compared to 18.46% obtained by the second-best method. However, in the best-case the performance of the proposed method reached 93.38% where second-best method achieved only 57.65% on an average. Besides, even though when the number of rumors is high, the DBP could achieve on an average 68.01% reduction on the impact of rumors. (c) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105452	10.1016/j.knosys.2019.105452													
J								Multi-domain modeling of atrial fibrillation detection with twin attentional convolutional long short-term memory neural networks	KNOWLEDGE-BASED SYSTEMS										Atrial fibrillation detection; Convolutional long short-term memory neural networks; ECG; Interpretable attention mechanism; Wavelet transform	COMPUTER-AIDED DIAGNOSIS; AUTOMATED DETECTION; ARRHYTHMIAS; CLASSIFICATION; TRANSFORM; INTERVALS; FEATURES	Atrial fibrillation (AF) is a common arrhythmia, and its incidence increases with age. Many methods have been developed to identify AF, including both the hand-picked features by experts and the recent emerging artificial intelligent (AI) methods. As the traditional hand-picked features have almost reached the boundary of their capability, the AI methods have shown their great potentials to achieve high accuracy for the AF identification. However, some common AI methods, especially deep learning methods, do not provide good properties of interpretability, making it difficult to explore the internal relationship between input and prediction results. In addition, most of the reported methods are only for the intra-patient test of AF and Non-AF. In this study, we try to develop an AF detector based on a twin-attentional convolutional long short-term memory neural network (TAC-LSTM), which can not only generate results with high accuracy but also enable a human-friendly function to provide the possible explanations of the automated extracted features by AI. TAC-LSTM was applied to extract multi-domain features of ECG signals for AF detection and to mine the influence of different input segments on the final prediction results. Finally, the proposed method is validated on the MIT-BIH Atrial Fibrillation Database (AFDB) with intra-patient test and inter-patient test and the results also have shown that multi-domain features extracted by TAC-LSTM can provide more useful information. Collectively, TAC-LSTM can be used for clinicians as an auxiliary diagnostic tool. (c) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105460	10.1016/j.knosys.2019.105460													
J								An improved CNN framework for detecting and tracking human body in unconstraint environment	KNOWLEDGE-BASED SYSTEMS										Human tracking; Re-localization; Cascading of networks; Convolutional neural network; Semi-synthesized dataset	VISUAL TRACKING; OBJECT TRACKING	Human tracking and localization play a crucial role in many applications like accident avoidance, action recognition, safety and security, surveillance and crowd analysis. Inspired by its use and scope, we introduced a novel method for human tracking (one or many) and re-localization in a complex environment with large displacement. The model can handle complex background, variations in illumination, changes in target pose, the presence of similar target and appearance (pose and clothes), the motion of target and camera, occlusion of the target, background variation, and massive displacement of the target. Our model uses three convolutional neural network based deep architecture and cascades their learning such that it improves the overall efficiency of the model. The first network learns the pixel level representation of small regions. The second architecture uses these features and learns the displacement of a region with its category between moved, not-moved, and occluded classes. Whereas, the third network improves the displacement result of the second network by utilizing the previous two learning. We also create a semi-synthetic dataset for training purpose. The model is trained on this dataset first and tested on a subset of CamNeT, VOT2015, LITIV-tracking and Visual Tracker Benchmark database without training with real data. The proposed model yield comparative results with respect to current state-of-the-art methods based on evaluation criteria described in Object Tracking Benchmark, TPAMI 2015, CVPR 2013 and ICCV 2017. (c) 2019 Published by Elsevier B.V.																	0950-7051	1872-7409				APR 6	2020	193								105198	10.1016/j.knosys.2019.105198													
J								Bag-of-Concepts representation for document classification based on automatic knowledge acquisition from probabilistic knowledge base	KNOWLEDGE-BASED SYSTEMS										Natural language processing; Text representation; Document classification; Knowledge base; Interpretability	NEURAL-NETWORK; TEXT	Text representation, a crucial step for text mining and natural language processing, concerns about transforming unstructured textual data into structured numerical vectors to support various machine learning and data mining algorithms. For document classification, one classical and commonly adopted text representation method is Bag-of-Words (BoW) model. BoW represents document as a fixed-length vector of terms, where each term dimension is a numerical value such as term frequency or tf-idf weight. However, BoW simply looks at surface form of words. It ignores the semantic, conceptual and contextual information of texts, and also suffers from high dimensionality and sparsity issues. To address the aforementioned issues, we propose a novel document representation scheme called Bag-of-Concepts (BoC), which automatically acquires useful conceptual knowledge from external knowledge base, then conceptualizes words and phrases in the document into higher level semantics (i.e. concepts) in a probabilistic manner, and eventually represents a document as a distributed vector in the learned concept space. By utilizing background knowledge from knowledge base, BoC representation is able to provide more semantic and conceptual information of texts, as well as better interpretability for human understanding. We also propose Bag-of-Concept-Clusters (BoCCl) model which clusters semantically similar concepts together and performs entity sense disambiguation to further improve BoC representation. In addition, we combine BoCCl and BoW representations using an attention mechanism to effectively utilize both concept-level and word-level information and achieve optimal performance for document classification. (c) 2019 Published by Elsevier B.V.																	0950-7051	1872-7409				APR 6	2020	193								105436	10.1016/j.knosys.2019.105436													
J								Particle filtering methods for stochastic optimization with application to large-scale empirical risk minimization	KNOWLEDGE-BASED SYSTEMS										Stochastic optimization; Stochastic gradient descent; Particle filtering; Kalman filtering; Logistic regression; Classification; Incremental proximal method; Static parameter estimation; Empirical risk minimization; Supervised learning	SEQUENTIAL MONTE-CARLO; CONVERGENCE RESULT; QUALITY; TARGET; TERM	This paper is concerned with sequential filtering based stochastic optimization (FSO) approaches that leverage a probabilistic perspective to implement the incremental proximity method (IPM). The present FSO methods are derived based on the Kalman filter (KF) and the extended KF (EKF). In contrast with typical methods such as stochastic gradient descent (SGD) and IPMs, they do not need to pre-schedule the learning rate for convergence. Nevertheless, they have limitations that inherit from the KF mechanism. As the particle filtering (PF) method outperforms KF and its variants remarkably for nonlinear non-Gaussian sequential filtering problems, it is natural to ask if FSO methods can benefit from PF to get around of their limitations. We provide an affirmative answer to this question by developing two PF based stochastic optimizers (PFSOs). For performance evaluation, we apply them to address nonlinear least-square fitting with simulated data, and empirical risk minimization for binary classification of real datasets. Experimental results demonstrate that PFSOs outperform remarkably a benchmark SGD algorithm, the vanilla IPM, and KF-type FSO methods in terms of numerical stability, convergence speed, and flexibility in handling diverse types of loss functions. (c) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105486	10.1016/j.knosys.2020.105486													
J								GMM: A generalized mechanics model for identifying the importance of nodes in complex networks	KNOWLEDGE-BASED SYSTEMS										Influential nodes; Complex networks; Gravity model	INFLUENTIAL NODES; COMMUNITY STRUCTURE; DYNAMICS; IDENTIFICATION; COOPERATION; CENTRALITY; BEHAVIOR; GROWTH; GRAPH	How to assess the importance of nodes in the network is an open question. There are many ways to identify the importance of nodes in complex networks. However, these methods have their own shortcomings and advantages. In particular, some methods based on the importance of nodes between interactions between nodes have been proposed. These methods utilize local information or path information. How to combine local and global information is still a problem. In this paper, a generalized mechanical model is proposed that uses global information and local information. To verify the effectiveness of the method, some experiments were performed on a total of ten real networks. In particular, an innovative experimental network-based quality assessment was proposed to validate the method of identifying the importance of nodes. (c) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105464	10.1016/j.knosys.2019.105464													
J								A common attribute reduction form for information systems	KNOWLEDGE-BASED SYSTEMS										Exact reduction; Invariant matrix; Equivalence relation; Information system; Rough set; Discernibility matrix	DECISION SYSTEMS; ROUGH; APPROXIMATION; GRANULARITY	An information system is an important form of knowledge representation, and attribute reduction plays an important role in machine learning, data mining, and intelligent systems. Several techniques are available to solve problems of attribute reduction but a common characterization for them is needed. This paper proposes the concepts of exact reductions and their reduction-invariant matrices. We obtain a unified mathematical model of attribute reduction by exactness for information systems, and show that frequently used methods of attribute reduction for information systems are exact. Specifically, we show that the positive region reduction for a decision table is exact. Our model theoretically unifies frequently used approaches to reduction. We also used a case study using the UCI dataset to verify the effectiveness of our proposed model. (c) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105466	10.1016/j.knosys.2019.105466													
J								Robust boosting via self-sampling	KNOWLEDGE-BASED SYSTEMS										Boosting; Loss function; Robustness; Self-sampling	ADABOOST	Boosting is a widely used ensemble meta-algorithm due to its excellent performance in combining weak learners into a strong learner. However, vanilla boosting methods are proved to be sensitive to noise due to the lack of restriction to the always misclassified samples during iterations. In this work, we present a new aspect to overcome the sensitiveness problem by combining the self-sampling learning framework, which can pursue reliable samples and smooth the training process based on the sample reliability measurement designed for the boosting procedure. Experimental results on the synthetic data and several real-world datasets show that the self-sampling regime can automatically optimize an appropriate training subset in different noise environments, and robust boosting algorithms we proposed outperform the state-of-the-art methods. (c) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105424	10.1016/j.knosys.2019.105424													
J								Robust neighborhood embedding for unsupervised feature selection	KNOWLEDGE-BASED SYSTEMS										Machine learning; Unsupervised learning; Feature selection; Neighborhood embedding; Manifold structure	CLASSIFICATION	Unsupervised feature selection is an efficient approach of dimensionality reduction for alleviating the curse of dimensionality in the countless unlabeled high-dimensional data. In view of the sparseness of the high-dimensional data, we propose a robust neighborhood embedding (RNE) method for unsupervised feature selection. First, with the fact that each data point and its neighbors are close to a locally linear patch of some underlying manifold, we obtain the feature weight matrix through the locally linear embedding (LLE) algorithm. Second, we use l1-norm to describe reconstruction error minimization, i.e., loss function to suppress the impact of outlier and noises in the dataset. As the RNE model is convex but non-smooth, we exploit alternation direction method of multipliers (ADMM) to solve it. Finally, extensive experimental results on benchmark datasets validate that the RNE method is effective and superior to the state-of-the-art unsupervised feature selection algorithms in terms of clustering performance. (c) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105462	10.1016/j.knosys.2019.105462													
J								A cross-region transfer learning method for classification of community service cases with small datasets	KNOWLEDGE-BASED SYSTEMS										Community service; Case classification; Small datasets; Transfer learning; Domain adaptation	DOMAIN ADAPTATION; REGULARIZATION; MODEL	The precise classification of community service cases is the most fundamental aspect of intelligent community service systems. However, data imbalance makes it challenging to achieve the desired level of precise classification. Existing transfer learning methods use open Internet knowledge for identifying case features and mining potential feature relations. However, community service cases have the characteristics such as short text length and non-public content, which restrict the transfer learning modes. In this paper, a cross-region transfer learning method is proposed to solve the classification problem of cases with small datasets in data imbalance situation while considering the perspective of regional cooperation. First, an ontology modeling method is applied to standardize the case features, reducing the effect of semantic ambiguity on transfer results. Secondly, to improve the effectiveness of source domain classification, this paper utilizes an extended marginal fisher analysis where the distance is measured by the inner product between data. Next, the mapping matrix from target domain to source domain is learned through domain adaptation. Finally, the method is verified based on the empirical data from Lanzhou and Beidaihe in China. Experimental results on classifiers show the proposed method helps regions to improve case classification rates significantly through knowledge complementation. The proposed approach can be followed to build case-based community service systems of reasonable accuracy and limited sample sizes. (c) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105390	10.1016/j.knosys.2019.105390													
J								On rule acquisition methods for data classification in heterogeneous incomplete decision systems	KNOWLEDGE-BASED SYSTEMS										Rough set; Heterogeneous incomplete decision systems; Rule acquisition; Data classification; Reduction	ROUGH SET APPROACH; ATTRIBUTE REDUCTION; APPROXIMATIONS	In the age of big data, lots of data obtained is low-quality data characterized by heterogeneousness and incompleteness, referred to as heterogeneous incomplete decision systems (HIDSs) in this paper. Data classification is an important task in machine learning, with the ability to discover valuable knowledge hidden in HIDSs. However, systematic studies on data classification in HIDSs are rarely reported. Especially, there is a lack of adaptive classification methods for HIDSs, which can deal directly with heterogeneous incomplete data and do not require prior discretization of numerical attributes or filling in missing values. In this paper, a unified representation model, called parameterized tolerance granulation model (PTGM), is proposed to deal with heterogeneous incomplete data. And the principle of an adaptive granulation method of constructing appropriate PTGMs is also described using difference-based collaborative optimization. Based on PTGMs, decision logic language is used to describe classifiers consisting of decision rules satisfying given conditions. Then, a discernibility function-based and a heuristic function-based classification methods are proposed to obtain all optimized rule sets (classifiers) and to generate a particular optimized rule set, respectively. The heuristic function-based method is actually an adaptive classification method, which can deal directly with heterogeneous incomplete data. Furthermore, detailed theoretical analyses are given to illustrate the correctness and effectiveness of the proposed methods. The experimental results show that the proposed methods are effective and have obvious advantages in directly handling heterogeneous incomplete data. (c) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105472	10.1016/j.knosys.2020.105472													
J								Reputation-Based Maintenance in Case-Based Reasoning	KNOWLEDGE-BASED SYSTEMS										Case-Based Reasoning; Case Base Maintenance; Case property sets; Case reputation	NOISE-REDUCTION; NEIGHBOR	Case Base Maintenance algorithms update the contents of a case base in order to improve case-based reasoner performance. In this paper, we introduce a new case base maintenance method called Reputation-Based Maintenance (RBM) with the aim of increasing the classification accuracy of a Case-Based Reasoning system while reducing the size of its case base. The proposed RBM algorithm calculates a case property called Reputation for each member of the case base, the value of which reflects the competence of the related case. Based on this case property, several removal policies and maintenance methods have been designed, each focusing on different aspects of the case base maintenance. The performance of the RBM method was compared with well-known state-of-the-art algorithms. The tests were performed on 30 datasets selected from the UCI repository. The results show that the RBM method in all its variations achieves greater accuracy than a baseline CBR, while some variations significantly outperform the state-of-the-art methods. We particularly highlight the RBM_ACBR algorithm, which achieves the highest accuracy among the methods in the comparison to a statistically significant degree, and the RBMcr algorithm, which increases the baseline accuracy while removing, on average, over half of the case base. (c) 2019 Published by Elsevier B.V.																	0950-7051	1872-7409				APR 6	2020	193								105283	10.1016/j.knosys.2019.105283													
J								LAC: Library for associative classification	KNOWLEDGE-BASED SYSTEMS										Associative classification; Association rule mining; Java class library; Classification software		The goal of this paper is to introduce LAC, a new Java Library for Associative Classification. LAC is the first tool that covers the full taxonomy of this classification paradigm through 10 well-known proposals in the field. Furthermore, it includes several measures to quantify the quality of the solutions as well as different input/output data formats. Last but not least, the library also provides a framework to automate experimental studies, supporting both sequential and parallel executions. Thanks to the GPLv3 license, LAC is totally free, open-source and publicly available. (c) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105432	10.1016/j.knosys.2019.105432													
J								Additional planning with multiple objectives for reinforcement learning	KNOWLEDGE-BASED SYSTEMS										Reinforcement learning; Multi-objective; Robotic control	ALGORITHM	Most control tasks have multiple objectives that need to be achieved simultaneously, while the reward definition is the weighted combination of all objects to determine one optimal policy. This configuration has a limitation in exploration flexibility and presents difficulty in reaching a satisfied terminate condition. Although some multi-objective reinforcement learning (MORL) methods have been presented recently, they concentrate on obtaining a set of compromising options rather than one best-performed strategy. On the other hand, the existing policy-improve methods have rarely emphasized on solving multiple objectives circumstances. Inspired by the enhanced policy search methods, an additional planning technique with multiple objectives for reinforcement learning is proposed in this paper, which is denoted as RLAP-MOP. This method provides opportunities to evaluate parallel requirements at the same time and suggests several optimal feasible actions to improve long-term performance further. Meanwhile, the short-term planning adopted in this paper has advantages in maintaining safe trajectories and building more accurate approximate models, which contributes to accelerating the training program. For comparison, an RLAP with single-objective optimization is also introduced in theoretical and experimental studies. The proposed techniques are investigated on a multi-objective cartpole environment and a soft robotic palpation task. The superiorities in the improved return values and learning stability prove that the multiple objectives based additional planning is a promising assistant to improve reinforcement learning. (c) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105392	10.1016/j.knosys.2019.105392													
J								Improving exploration and exploitation via a Hyperbolic Gravitational Search Algorithm	KNOWLEDGE-BASED SYSTEMS										Evolutionary algorithms; Gravitational search algorithm; Optimization algorithms; Exploration and exploitation; Hyperbolic functions	OPTIMIZATION ALGORITHM; SYSTEM; IMAGE; PERFORMANCE; EVOLUTION; DESIGN; GSA	Finding the best compromise between exploration and exploitation phases of a search algorithm is a hard task. The Gravitational Search Algorithm (GSA) is an evolutionary algorithm based on the Newton's universal law of gravitation. Many studies show that GSA suffers from slow exploitation which generates premature convergence. This paper proposes a Hyperbolic Gravitational Search Algorithm (HGSA) able to find an optimal balance between exploration and exploitation. The main contributions of this work are: the definition of suitable hyperbolic acceleration coefficients, the dynamic regulation of the gravitational constant coefficient through hyperbolic function and the definition of a decreasing hyperbolic function for the best agents which attract the other agents. The proposed algorithm is compared with well-known search algorithms on classical benchmark functions, CEC-06 2019 benchmark functions and three-bar truss design problem. The results show that HGSA is better than other algorithms in terms of optimization and convergence performances. (c) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105404	10.1016/j.knosys.2019.105404													
J								Fast discrete factorization machine for personalized item recommendation	KNOWLEDGE-BASED SYSTEMS										Discretization; Factorization machine; Item recommendation		Personalized item recommendation has become an essential target of Web applications, but it suffers from the efficiency problem due to a large volume of data. In particular, feature-based factorization machine models are generally limited by the vast number of feature dimensions, leading to catastrophic computation time. In this paper, we propose a Fast Discrete Factorization Machine (FDFM) method to resolve these issues by applying the hash coding technologies to factorization machine models. Specifically, it discretizes the real-valued feature vectors in the parameter model during the process of learning personalized item rankings, whereby the overall computational time can be greatly reduced. Besides, we propose convergence update rules to optimize the quantization loss of the binarization problem, which can be used in personalized ranking scenarios efficiently. Based on the evaluation in two real-world datasets, our proposed approach consistently shows better performance than other baselines, especially when using shorter binary codes. (c) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105470	10.1016/j.knosys.2019.105470													
J								Random Balance ensembles for multiclass imbalance learning	KNOWLEDGE-BASED SYSTEMS										Classifier ensembles; Imbalanced data; Multiclass classification	DATA-SETS; NEURAL-NETWORKS; STATISTICAL COMPARISONS; BINARIZATION TECHNIQUES; SAMPLING APPROACH; MULTIPLE CLASSES; CLASSIFICATION; SMOTE; CLASSIFIERS; PREDICTION	Random Balance strategy (RandBal) has been recently proposed for constructing classifier ensembles for imbalanced, two-class data sets. In RandBal, each base classifier is trained with a sample of the data with a random class prevalence, independent of the a priori distribution. Hence, for each sample, one of the classes will be undersampled while the other will be oversampled. RandBal can be applied on its own or can be combined with any other ensemble method. One particularly successful variant is RandBalBoost which integrates Random Balance and boosting. Encouraged by the success of RandBal, this work proposes two approaches which extend RandBal to multiclass imbalance problems. Multiclass imbalance implies that at least two classes have substantially different proportion of instances. In the first approach proposed here, termed Multiple Random Balance (MultiRandBal), we deal with all classes simultaneously. The training data for each base classifier are sampled with random class proportions. The second approach we propose decomposes the multiclass problem into two-class problems using one-vs-one or one-vs-all, and builds an ensemble of RandBal ensembles. We call the two versions of the second approach OVO-RandBal and OVA-RandBal, respectively. These two approaches were chosen because they are the most straightforward extensions of RandBal for multiple classes. Our main objective is to evaluate both approaches for multiclass imbalanced problems. To this end, an experiment was carried out with 52 multiclass data sets. The results suggest that both MultiRandBal, and OVO/OVA-RandBal are viable extensions of the original two-class RandBal. Collectively, they consistently outperform acclaimed state-of-the art methods for multiclass imbalanced problems. (c) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105434	10.1016/j.knosys.2019.105434													
J								Data maximum dispersion classifier in projection space for high-dimension low-sample-size problems	KNOWLEDGE-BASED SYSTEMS										High-dimension low-sample-size; Binary linear classifier; Data piling; Hyperplane; Large margin classification	DISCRIMINATION METHODS; VARIABLE SELECTION; FACE RECOGNITION; ENSEMBLE; PREDICTION; MODELS; CANCER	Various applications in different fields, such as gene expression analysis or computer vision, suffer from data sets with high-dimensional low-sample-size (HDLSS), which has posed significant challenges for standard statistical and modern machine learning methods. In this paper, we propose a novel linear binary classifier, denoted by Data Maximum Dispersion classifier (DMDC), which is applicable to general data, especially HDLSS such as genetic microarrays, image and so on. DMDC maximize data dispersion in projection space to find the hyperplane for classification. Our proposed model has several advantages compared to those widely used approaches. First, it works well on HDLSS. Second, it is not sensitive to the intercept term b. Third, the implement of DMDC is easy and holds low computational complexity owing to solve the similar Convex Quadratic Programming formulation as in SVM, rather than the SOCP in the DWD related methods. Fourth, it is robust to the model specification for various real applications. We conduct the evaluations on one simulated and seven real-world benchmark data sets, including face classification, radiomics and mRNA classification. The experiment results manifested the superiority of DMDC compared to the state-of-art algorithms in most cases, or at least obtains comparable results. (c) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105420	10.1016/j.knosys.2019.105420													
J								Community search for multiple nodes on attribute graphs	KNOWLEDGE-BASED SYSTEMS										Community search; Attributed graph; k-edge connected component; Steiner maximum connected component		Community search is individualized and targeted, which identifies local communities containing query nodes and meeting specific conditions only according to users. Community search is only interested in communities that satisfy the designated criteria, and has a wide range of application scenarios in the real world. Traditional community search only uses the topology information, without considering the impact of node attributes information. And many existing algorithms can only deal with a single query node. Therefore, this paper defines the attribute community search problem of multiple query nodes on attribute graphs, which utilizes the edge connectivity as the structural tightness constraint and designs an attribute function as the attribute tightness constraint. To solve this problem, we design a framework that firstly finds the maximum k edge connected-component containing query as a candidate subgraph, then iteratively deletes the node with the least contribution to the attribute function from the candidate subgraph and adjusts the subgraph according to the structural constraints. We also design the attribute Steiner distance that synthesizes the structural information and attribute information, and propose an efficient heuristic expansion algorithm based on distance. The experimental results on several datasets show our proposed methods can find the community containing all query nodes with high tightness of structure and homogeneity of attributes. (c) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105393	10.1016/j.knosys.2019.105393													
J								Interval type-2 fuzzy multi-attribute decision-making approaches for evaluating the service quality of Chinese commercial banks	KNOWLEDGE-BASED SYSTEMS										Multiple attributes analysis; Service quality evaluation; Choquet integral; Interval type-2 fuzzy numbers; Archimedean T-norms	AGGREGATION OPERATORS; MEAN OPERATORS; SETS; INFORMATION; ATTRIBUTES; VARIABLES; TOPSIS	In today's world, with increased competition, the service quality of Chinese commercial banks is recognized as a major factor that is responsible for enhancing competitiveness. Therefore, it is necessary to evaluate and analyse the service quality of Chinese commercial banks to realize their stable development. The service quality evaluation could be recognized as a multi-attribute decision-making (MADM) problem with multiple assessment attributes, both being of a qualitative and quantitative nature. Owing to the growing complexity and high uncertainty of the financial environment, the assessments of attributes cannot always possibly express using a real and/or type1 fuzzy number. Additionally, a heterogeneous relationship often exists among the attributes under many real decision cases. In this study, we create two MADM approaches to handle decision-making problems with interval type-2 fuzzy numbers (IT2FNs) and offer their application to service quality evaluations of commercial banks problems. Specifically, we first define some operations on IT2FNs based on Archimedean T-norms (ATs) and develop a bi-directional projection measure of IT2FNs. Next, by combining the generalized Banzhaf index, the Choquet integral and IT2FNs, we propose the interval type-2 fuzzy Archimedean Choquet (IT2FAC) operator, the Banzhaf IT2FAC (BIT2FAC) operator and the 2-additive BIT2FAC (2ABIT2FAC) operator. Then, we establish two optimal models for deriving the weights of attributes based on a bi-directional projection measure of IT2FNs and Banzhaf function. Finally, we create two novel MADM methods under interval type-2 fuzzy contexts, where an illustrative case concerning the service quality evaluation of Chinese commercial banks is used to explain the created MADM approaches. (c) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105438	10.1016/j.knosys.2019.105438													
J								Automated detection of atrial fibrillation and atrial flutter in ECG signals based on convolutional and improved Elman neural network	KNOWLEDGE-BASED SYSTEMS										Atrial fibrillation; Atrial flutter; Convolutional neural network; Improved Elman neural network	DEEP LEARNING APPROACH; CARDIAC-ARRHYTHMIA; DIAGNOSIS; MODEL; CLASSIFICATION; IDENTIFICATION	Atrial fibrillation (AF) and atrial flutter (AFL) are two common life-threatening arrhythmias. Both are not only easily transformed into each other, but also often cause misdiagnosis due to the similar clinical symptoms. The early efficient and accurate detection of AF and AFL is helpful to reduce the pain and injury of patients suffering from these diseases, and the traditional detection is often inefficient and laborious. Therefore, we propose a new model mechanism using an 11-layers network architecture to automatically classify AF and AFL signals. It is mainly constructed with the convolutional neural network (CNN) and the improved Elman neural network (IENN). Besides, we specifically design two relative models as control subjects to validate the classification performance of the proposed model. 10-fold cross-validation is also implemented on the MIT-BIH AF database (AFDB) and the MIT-BIH arrhythmia database (MITDB), respectively. The obtained results show that the model achieved the accuracy, specificity, and sensitivity of 98.8%, 98.6%, and 98.9% on the AFDB database and 99.4%, 99.1%, and 99.6% on the MITDB database, respectively. The model mechanism has been demonstrated to have more superior performance than two relative models and some advanced algorithms, which can be considered as a reliable and efficient identification system to aid physicians and save lives. (c) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105446	10.1016/j.knosys.2019.105446													
J								SRQA: Synthetic Reader for Factoid Question Answering	KNOWLEDGE-BASED SYSTEMS										Question answering; Multilayer attention; Cross evidence; Adversarial training		The question answering system can answer questions from various fields and forms with deep neural networks, but it still lacks effective ways when facing multiple evidences. We introduce a new model called SRQA, which means Synthetic Reader for Factoid Question Answering. This model enhances the question answering system in the multi-document scenario from three aspects: model structure, optimization goal, and training method, corresponding to Multilayer Attention (MA), Cross Evidence (CE), and Adversarial Training (AT) respectively. First, we propose a multilayer attention network to obtain a better representation of the evidences. The multilayer attention mechanism conducts interaction between the question and the passage within each layer, making the token representation of evidences in each layer takes the requirement of the question into account. Second, we design a cross evidence strategy to choose the answer span within more evidences. We improve the optimization goal, considering all the answers' locations in multiple evidences as training targets, which leads the model to reason among multiple evidences. Third, adversarial training is employed to high-level variables besides the word embedding in our model. A new normalization method is also proposed for adversarial perturbations so that we can jointly add perturbations to several target variables. As an effective regularization method, adversarial training enhances the model's ability to process noisy data. Combining these three strategies, we enhance the contextual representation and locating ability of our model, which could synthetically extract the answer span from several evidences. We perform SRQA on the WebQA dataset, and experiments show that our model outperforms the state-of-the-art models (the best fuzzy score of our model is up to 78.56%, with an improvement of about 2%). (c) 2019 Published by Elsevier B.V.																	0950-7051	1872-7409				APR 6	2020	193								105415	10.1016/j.knosys.2019.105415													
J								An image NSCT-HMT model based on copula entropy multivariate Gaussian scale mixtures	KNOWLEDGE-BASED SYSTEMS										NSCT; Gaussian copula; Copula entropy multivariate Gaussian scale mixtures; HMT; Image denoising	SPARSE; PROBABILITY; TRANSFORM; DOMAIN; SIGNAL	As an important multiscale geometric analysis tool, the nonsubsampled contourlet transform (NSCT) has a strong ability in capturing anisotropy and directional features of images. This paper proposes a copula entropy multivariate Gaussian scale mixtures based nonsubsampled contourlet transform hidden Markov tree model (CE-NSCT-HMT). First, we study the probability density distribution of the NSCT coefficients and observe that they exhibit a sharp peak at the zero amplitude and two heavy tails on both sides of the peak. Second, we redefine the generalized neighborhood relationship of the NSCT coefficients and then analyze the joint statistical property among them. We get that the NSCT coefficients have the strongest correlation with their newly defined 'four neighborhood' coefficients. Third, we use the Gaussian copula function to model the NSCT coefficients with their `four neighborhood' coefficients. Based on the copula entropy value, the copula entropy multivariate Gaussian scale mixtures distribution is proposed, the application of copula entropy helps the contourlet a lot in localizing textures. In addition, we combine these studies with the hidden Markov tree model and propose the CE-NSCT-HMT model. Finally, the proposed model is applied to image denoising and achieve a good performance. The copula entropy is first applied to the correlation measurement of multiscale decomposition coefficients in this paper, which provides a way to further extend the application of copula entropy to other image processing area. (c) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105387	10.1016/j.knosys.2019.105387													
J								Entropy and gravitation based dynamic radius nearest neighbor classification for imbalanced problem	KNOWLEDGE-BASED SYSTEMS										Information entropy; Gravitational force; Nearest neighbor rules; Imbalanced problem; L-p-norm	NAIVE BAYES; ALGORITHMS; SMOTE; PREDICTION	In imbalanced problems, the asymmetric number of samples in different classes brings great challenges to traditional classifiers, especially to the Nearest Neighbors (NN) classifiers. When NN-based classifier deals with imbalanced problems, the criterion of itself makes the classification result data-dependent, thus biasing towards the majority class. To overcome the drawback in NN-based classifiers, a meta heuristic NN-based algorithm named Gravitational Fixed Radius Nearest Neighbor classifier (GFRNN) is proposed to solve imbalanced problems by drawing on Newton's law of universal gravitation. However, GFRNN still has three major problems including negligence of the distribution of samples, unreasonable calculation of data mass and improper distance metric. To this end, this paper proposes an Entropy and Gravitation based Dynamic Radius Nearest Neighbor algorithm (EGDRNN). Different from GFRNN, EGDRNN determines the radius in a dynamic and rapid way. EGDRNN uses entropy information to make samples at different locations have different importance. Finally, by utilizing a general L-p-norm to calculate the distance between two samples, the classification performance is greatly improved. The experimental result validates that the proposed EGDRNN not only achieves the highest classification accuracy but also takes the lowest time consuming among all comparison algorithms. (c) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105474	10.1016/j.knosys.2020.105474													
J								Analyses and applications of optimization methods for complex network reconstruction	KNOWLEDGE-BASED SYSTEMS										Complex network; Discrete-time dynamics; Regression; Network reconstruction	DYNAMICAL NETWORKS; TIME-SERIES; TOPOLOGY; MODELS; REGRESSION; COMMUNITY; SELECTION	Inferring the topology of a network from observable dynamics is a key topic in the research of complex network. With the observation error considered, the topology inferring is formulated as a connectivity reconstruction problem that can be solved through optimization estimation. It is found that the different optimization methods should be selected to deal with the different degrees of noise, different scales of observable time series and such other situations when it comes to the problem of connectivity reconstruction, which has not been analyzed and discussed before yet. In this paper, four regression methods, namely least squares, ridge, lasso and elastic net, are used to solve the problem of network reconstruction in different situations. In particular, a further analysis is made of the effects of each regression method on the network reconstruction problem in detail. Through simulation of a variety of artificial and real networks, as it has turned out, the four regression methods are effective in respect to network reconstruction when certain conditions are respectively satisfied. Based on the experimental results, it is possible to reach some interesting conclusions that can guide our readers to know the internal mechanisms for network reconstruction and choose the appropriate regression method in accordance with the actual situation and existing knowledge. (c) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105406	10.1016/j.knosys.2019.105406													
J								Multiscale cascading deep belief network for fault identification of rotating machinery under various working conditions	KNOWLEDGE-BASED SYSTEMS										Deep belief network; Multiscale feature learning; Rotating machinery; Fault identification	NEURAL-NETWORK; INTELLIGENT DIAGNOSIS; PERMUTATION ENTROPY; SPARSE AUTOENCODER; DISPERSION ENTROPY; ALGORITHM; MODEL	Deep learning is characterized by strong self-learning and fault classification ability without manually feature extraction stage of traditional algorithms. Deep belief network (DBN) is one of the most classic models of deep learning. However, traditional DBN is mainly restricted to learn automatically single scale features from raw vibration signal while identify the fault type, which implies some important information inherent in other scales of vibration data are neglected, thus causing easily unsatisfactory diagnosis result. To alleviate the problem, this paper presents a novel architecture named multiscale cascading deep belief network (MCDBN) for automatic fault identification of rotating machinery, which is aimed at learning the broader feature representation and improving the recognition precision. Firstly, a sliding window with data overlap is adopted to split the collected raw vibration signal to a group of equal-sized sub-signal, and then the improved multiscale coarse-grained procedure of each sub-signal is conducted to obtain the coarse-grained time series at different scales. Meanwhile, Fourier spectrum at different scales is calculated to capture multiscale characteristics. Finally, multiple DBN architecture with three hidden layers are designed to learn high-level feature representation directly from multiscale characteristics in a parallel manner and accomplish fault identification automatically through cascading way and softmax classifier without artificial expertise. Results of two experimental cases with respect to mechanical fault identification under different working conditions have well indicated that the proposed method is provided with preferable diagnostic performance compared with standard DBN and traditional multiscale feature extractors. (c) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105484	10.1016/j.knosys.2020.105484													
J								Proximity-aware heterogeneous information network embedding	KNOWLEDGE-BASED SYSTEMS										Network embedding; Heterogeneous information network; Random walk	DIMENSIONALITY REDUCTION	Network embedding, which aims to learn a high-quality low-dimensional representation for each node in a network, has attracted increasing attention recently. Heterogeneous information networks, with distinguishing types of nodes and relations, are one of the most significant networks. In the past years, heterogeneous information network embedding has been intensively studied. Most popular methods generate a set of node sequences, and feed them into an unsupervised feature learning model to obtain a low-dimensional vector for each node. However, the limitations of these approaches are that their generative node sequences neglect the different importances of diverse relations and they ignore the great value of proximity information which reveals whether two nodes are close or not in the network. To tackle these limitations, this paper presents a novel framework named Proximity-Aware Heterogeneous Information Network Embedding (PAHINE). The native information of a network is extracted from node sequences, which are generated by walking on a probability-sensitive metagraph. Afterwards, the extracted information is fed into deep neural networks to derive the desired embedding vectors. The experimental results on four different heterogeneous networks indicate that the proposed method is efficient and it outperforms the state-of-the-art heterogeneous networks embedding algorithms. (c) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105468	10.1016/j.knosys.2019.105468													
J								Label propagation-based approach for detecting review spammer groups on e-commerce websites	KNOWLEDGE-BASED SYSTEMS										Fake reviews; Review spammer groups; Spammer group detection; Reviewer relationship graph; Label propagation		Online product reviews are very important information resources on e-commerce websites and significantly influence consumers' purchase decisions. Driven by interests, however, some merchants might hire a group of reviewers working together to promote or demote a set of target products by writing fake reviews. Such a collusive fraudulent reviewer group is generally termed a review spammer group and is more harmful to e-commerce websites than individual review spammers. To address this issue, in this paper we propose a label propagation-based approach to detect review spammer groups on e-commerce websites. First, based on the evaluation data of reviewers, we extract the associations between reviewers with respect to review time and product ratings to construct a relationship graph of reviewers. Second, we propose an improved label propagation algorithm with a propagation intensity and an automatic filtering mechanism to find candidate spammer groups based on the constructed reviewer relationship graph. Finally, we propose a ranking algorithm that combines the entropy method and the analytic hierarchy process to rank the candidate spammer groups and thus identify the top-k review spammer groups. The experimental results of the real-world Amazon and Yelp datasets show that the proposed approach performs better than the baseline methods. (c) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105520	10.1016/j.knosys.2020.105520													
J								Clustering by transmission learning from data density to label manifold with statistical diffusion	KNOWLEDGE-BASED SYSTEMS										ayesian transmission system; Transmission learning; Label manifold-based transmission learning machine; Fick's diffusion law	FUZZY C-MEANS	Owing to the tremendous diversity and complexity of data in today's world, some new insights for clustering on data are often desired by developing an alternative to the existing clustering approaches. In this paper, based on the new concepts of the Bayesian transmission system and its transmission learning, a label manifold-based transmission learning machine for clustering (LMTLMC) is accordingly developed. As the first attempt to explain the clustering behavior in a lifelike way, LMTLMC is well justified by revealing the natural parallel between its gradient-based optimization process and the statistical diffusion in statistical physics through the modified Fick's diffusion law for clustering. Practically, LMTLMC is distinctive in its easy implementation in terms of its global analytical solution, its easy parameter settings and its stable and efficient clustering results. Extensive experiments on synthetic datasets and real datasets demonstrate the promising performance and superiority of LMTLMC for clustering tasks, in contrast to the existing clustering algorithms. (c) 2019 Published by Elsevier B.V.																	0950-7051	1872-7409				APR 6	2020	193								105330	10.1016/j.knosys.2019.105330													
J								Knowledge based domain adaptation for semantic segmentation	KNOWLEDGE-BASED SYSTEMS										Domain adaptation; Knowledge; Semantic segmentation		Domain adaptation for semantic segmentation is a challenging problem for two reasons. One reason is that annotating labels is an extremely high cost work. Another reason is that the domain gap between the source and target domains limits the performance of semantic segmentation. In this paper, we propose an unsupervised knowledge based domain adaptation method for semantic segmentation. The proposed method consists of three steps. First, the common knowledge is loaded from the source and target domains. Then, the loaded knowledge is filtered according to the specific input image. In the end, the filtered knowledge is fused with the high-level features to guide domain adaptation. Our main contributions are: (1) a first novel knowledge based domain adaptation approach for semantic segmentation and (2) a triangular constraint for knowledge loading, in which the semantic vectors are smoothly imported. Experimental results on three datasets indicate that our method achieves competitive results in some scenarios compared with the state-of-the-art approaches. (c) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105444	10.1016/j.knosys.2019.105444													
J								Modeling sentiment dependencies with graph convolutional networks for aspect-level sentiment classification	KNOWLEDGE-BASED SYSTEMS										Sentiment classification; Aspect-level; Sentiment dependencies; Graph convolutional networks		Aspect-level sentiment classification aims to distinguish the sentiment polarities over one or more aspect terms in a sentence. Existing approaches mostly model different aspects in one sentence independently, which ignore the sentiment dependencies between different aspects. However, such dependency information between different aspects can bring additional valuable information for aspect-level sentiment classification. In this paper, we propose a novel aspect-level sentiment classification model based on graph convolutional networks (GCN) which can effectively capture the sentiment dependencies between multi-aspects in one sentence. Our model firstly introduces bidirectional attention mechanism with position encoding to model aspect-specific representations between each aspect and its context words, then employs GCN over the attention mechanism to capture the sentiment dependencies between different aspects in one sentence. The proposed approach is evaluated on the SemEval 2014 datasets. Experiments show that our model outperforms the state-of-the-art methods. We also conduct experiments to evaluate the effectiveness of GCN module, which indicates that the dependencies between different aspects are highly helpful in aspect-level sentiment classification(1). (c) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105443	10.1016/j.knosys.2019.105443													
J								Multi-view clustering via clusterwise weights learning	KNOWLEDGE-BASED SYSTEMS										Multi-view; Non-negative matrix factorization; Weight		It is known that the performance of multi-view clustering could be improved by assigning weights to the views, since different views play different roles in the final clustering results. Nevertheless, we observe that weights could be further refined, since in reality different clusters also have different impacts on finding the correct results. We propose a multi-view clustering algorithm with clusterwise weights (MCW), which assigns a weight on each cluster within each view. The objective function of MCW consists of three parts: (1) intra-view clustering: clustering each view by using non-negative matrix factorization; (2) inter-view relationship learning: learning the consensus clustering results by a weighted combination of each view; (3) clusterwise weight learning: learning the weight of a cluster by making the weight be proportional to the average distance between the cluster and other clusters. We present an effective alternating algorithm to solve the non-convex optimization problem. Experimental results on several benchmark datasets demonstrate the superiority of the proposed algorithm over existing multi-view clustering methods. (c) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105459	10.1016/j.knosys.2019.105459													
J								Deep Collaborative Embedding for information cascade prediction	KNOWLEDGE-BASED SYSTEMS										Information cascade prediction; Deep Collaborative Embedding; Network embedding	SOCIAL NETWORKS; DIFFUSION PROBABILITIES; MODELS	Recently, information cascade prediction has attracted increasing interest from researchers, but it is far from being well solved partly due to the three defects of the existing works. First, the existing works often assume an underlying information diffusion model, which is impractical in real world due to the complexity of information diffusion. Second, the existing works often ignore the prediction of the infection order, which also plays an important role in social network analysis. At last, the existing works often depend on the requirement of underlying diffusion networks which are likely unobservable in practice. In this paper, we aim at the prediction of both node infection and infection order without requirement of the knowledge about the underlying diffusion mechanism and the diffusion network, where the challenges are two-fold. The first is what cascading characteristics of nodes should be captured and how to capture them, and the second is that how to model the nonlinear features of nodes in information cascades. To address these challenges, we propose a novel model called Deep Collaborative Embedding (DCE) for information cascade prediction, which can capture not only the node structural property but also two kinds of node cascading characteristics. We propose an auto-encoder based collaborative embedding framework to learn the node embeddings with cascade collaboration and node collaboration, in which way the non-linearity of information cascades can be effectively captured. The results of extensive experiments conducted on real-world datasets verify the effectiveness of our approach. (c) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105502	10.1016/j.knosys.2020.105502													
J								Subspace clustering by simultaneously feature selection and similarity learning	KNOWLEDGE-BASED SYSTEMS										Subspace clustering; Feature selection; Graph learning; Similarity learning; Affinity matrix	LOW-RANK REPRESENTATION; MATRIX FACTORIZATION; FACE RECOGNITION; SPARSE; ALGORITHM	Learning a reliable affinity matrix is the key to achieving good performance for graph-based clustering methods. However, most of the current work usually directly constructs the affinity matrix from the raw data. It may seriously affect the clustering performance since the original data usually contain noises, even redundant features. On the other hand, although integrating manifold regularization into the framework of clustering algorithms can improve clustering results, some entries of the pre-computed affinity matrix on the original data may not reflect the true similarities between data points. To address the above issues, we propose a novel subspace clustering method to simultaneously learn the similarities between data points and conduct feature selection in a unified optimization framework. Specifically, we learn a high-quality graph under the guidance of a low-dimensional space of the original data such that the obtained affinity matrix can reflect the true similarities between data points as much as possible. A new algorithm based on augmented Lagrangian multiplier is designed to find the optimal solution to the problem effectively. Extensive experiments are conducted on benchmark datasets to demonstrate that our proposed method performs better against the state-of-the-art clustering methods. (c) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105512	10.1016/j.knosys.2020.105512													
J								Unsupervised feature selection for balanced clustering	KNOWLEDGE-BASED SYSTEMS										Feature selection; Balanced clustering; ADMM	KRILL HERD ALGORITHM; REGRESSION	In many real-world applications of data mining, such as energy load balance of wireless sensor networks, given data points with balanced distribution, i.e., each class contains approximately the same number of instances, we often need to obtain a clustering result to reflect such balance. In many data, especially the high-dimensional data, such balanced structure is not obvious in the original feature space, due to the noisy and redundant features. Therefore we need to apply feature selection methods to pick several informative features to reveal such balanced structure of data. Feature selection is a fundamental problem in machine learning tasks and has attracted considerable attentions in recent years. However, conventional feature selection methods often focus on how to select the most discriminative features, whereas ignoring the balance property of the data. To tackle this problem, we propose a novel unsupervised feature selection method for balanced clustering which can reveal the intrinsic balanced structure of data. In our method, a balanced regularization term is introduced to select the features which can help to produce balanced clusters. Then, we provide an Alternating Direction Method of Multipliers (ADMM) to optimize the introduced objective function. At last, the experiments are conducted on six benchmark data sets, including Yale and 20NG data sets and so on, by comparing with other state-of-the-art unsupervised feature selection methods published in the literature. The experimental results show that our method not only has better clustering performance but also leads to more balanced clustering structure. (c) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 6	2020	193								105417	10.1016/j.knosys.2019.105417													
J								Md-Net: Multi-scale Dilated Convolution Network for CT Images Segmentation	NEURAL PROCESSING LETTERS										CT image; Segmentation; Network; Accurate segmentation	LUNG	Accurate CT image segmentation is of great importance to the clinical diagnosis. Due to the high similarity of gray values in CT image, the segmented areas are easily affected by their surroundings, which leads to the loss of semantic information. In this paper, we propose a multi-scale dilated convolution network (Md-Net) for CT image segmentation with superior segmentation performance compared with state-of-the-art methods. Specifically, our Md-Net utilizes the dilated convolutions with different sizes to form feature pyramids for extracting the semantic information. Moreover, we use a weighted Diceloss to accelerate the convergence in training process. Meanwhile, the bilinear interpolation and multiple convolutions are taken to reduce the computational cost. Experiment results show that our proposed Md-Net outperforms the representative medical image segmentation methods, including Unet, Unet++, MaskRcnn and CE-Net, in terms of sensitivity, accuracy and area under curve both on lung dataset and Bladder dataset.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2915	2927		10.1007/s11063-020-10230-x		APR 2020											
J								The Propagation Background in Social Networks: Simulating and Modeling	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Social network; information overload; propagation background; simulating; modeling		Recent years have witnessed the booming of online social network and social media platforms, which leads to a state of information explosion. Though extensive efforts have been made by publishers to struggle for the limited attention of audiences, still, only a few of information items will be received and digested. Therefore, for simulating the information propagation process, competition among propagating items should be considered, which has been largely ignored by prior works on propagation modeling. One possible reason may be that, it is almost impossible to identify the influence of propagation background from real diffusion data. To that end, in this paper, we design a comprehensive framework to simulate the propagation process with the characteristics of user behaviors and network topology. Specifically, we propose a propagation background simulating (PBS) algorithm to simulate the propagation background by using users ' behavior dynamics and out-degree. Along this line, an ICPB (independent cascade with propagation background) model is adapted to relieve the impact of propagation background by using users ' in-degree. Extensive experiments on kinds of synthetic and real networks have demonstrated the effectiveness of our methods.																	1476-8186	1751-8520				JUN	2020	17	3					353	363		10.1007/s11633-020-1227-2		APR 2020											
J								Performance evaluation of fuzzy clustered case-based reasoning	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Case-based reasoning; fuzzy clustering; performance evaluation; case-base maintenance	MAINTENANCE; QUALITY; SYSTEM; RETRIEVAL; SELECTION; FAULTS	Case-based reasoning (CBR) is a nature-inspired machine learning technique. It solves a new problem using the existing similar problems with their solutions stored in central repository known as case-base. It results in continuous growth of the case-base enhancing the problem solving capability of the system but at the same time compromising the performance. First performance challenge is continuous growth of the case-base. Second performance challenge is to handle the performance bottleneck without compromising the relevance of cases with their neighbourhood to solve new problems. Different approaches have been introduced in literature to address this performance challenge. In this work, a knowledge-base maintenance approach using fuzzy clustering has been presented which takes care of the performance bottleneck and does not compromise the problem solving capability of CBR. Performance of the proposed approach has been evaluated on different case-bases and results have been compared with the conventional CBR approach.																	0952-813X	1362-3079															10.1080/0952813X.2020.1744194		APR 2020											
J								Fuzzy Control for Chaotic Confliction Model	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Chaotic control; Chaotic confliction model; Coordination; Fuzzy numbers; Time series; Phase portrait; Lyapunov exponent; Bifurcation diagram	DYNAMICAL MODELS; NONLINEAR BEHAVIOR; LOVE MODEL; FUTURE	Nowadays, the research for chaotic dynamics in the social science has been interested by many researchers. This paper proposes a fuzzy controlling method for dynamic confliction model between certain group A and group B based upon love affairs model. In order to control the confliction between group A and group B, we apply control input with the symmetry triangular fuzzy numbers, which are special fuzzy numbers into each group A and group B. We show the result of fuzzy control for chaotic confliction model by using time series and phase portrait while we apply control input with specific time domain.																	1562-2479	2199-3211				SEP	2020	22	6			SI		1961	1971		10.1007/s40815-020-00839-4		APR 2020											
J								A Minimum Trust Discount Coefficient Model for Incomplete Information in Group Decision Making with Intuitionistic Fuzzy Soft Set	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Group decision making; Intuitionistic fuzzy soft set; Incomplete information; Evidence theory	AGGREGATION; OPERATORS; ENTROPY; ATTRIBUTES; WEIGHT	This article proposes a framework to deal with incomplete information in multiple criteria group decision making with intuitionistic fuzzy soft set. To do that, the weighted sum method for choice values and simple mathematical expectation method are extended to the case of obtained fuzzy soft set, and they are proved to have the same result in estimating the incomplete information. In order to reduce the system error in the estimating process cause by multiple decision information, a minimum trust discount coefficient model is established according to the relevant methods of evidence theory. Then, a new definition of entropy for intuitionistic fuzzy sets is introduced to determine the weights of group experts. Therefore, individual decision-making matrices are integrated into a comprehensive decision-making matrix by the integration operation formula of intuitionistic fuzzy soft matrix. The decision making is realized according to the difference of the score values of objects. Finally, the steps of this method are concluded, and one example is given to explain the application of this method.																	1562-2479	2199-3211				SEP	2020	22	6			SI		2025	2040		10.1007/s40815-020-00811-2		APR 2020											
J								Improved face super-resolution generative adversarial networks	MACHINE VISION AND APPLICATIONS										Face super-resolution; GAN; Spectral normalization; Dense blocks		The face super-resolution method is used for generating high-resolution images from low-resolution ones for better visualization. The super-resolution generative adversarial network (SRGAN) can generate a single super-resolution image with realistic textures, which is a groundbreaking work. Based on SRGAN, we proposed improved face super-resolution generative adversarial networks. The super-resolution image details generated by SRGAN usually have undesirable artifacts. To further improve visual quality, we delve into the key components of the SRGAN network architecture and improve each part to achieve a more powerful SRGAN. First, the SRGAN employs residual blocks as the core of the very deep generator network G. In this paper, we decide to employ dense convolutional network blocks (dense blocks), which connect each layer to every other layer in a feed-forward fashion as our very deep generator networks. Moreover, in the past few years, generative adversarial networks (GANs) have been applied to solve various problems. Despite its superior performance, it is difficult to train. A simple and effective regularization method called spectral normalization GAN is used to solve this problem. We have experimentally confirmed that our proposed method is superior to the other existing method in training stability and visual improvements.																	0932-8092	1432-1769				APR 5	2020	31	4							22	10.1007/s00138-020-01073-6													
J								Keyword retrieving in continuous speech using connectionist temporal classification	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Connectionist temporal classification; Deep convolutional neural network; Speech recognition; Speech keyword retrieval	DEEP NEURAL-NETWORKS; RECOGNITION	The issue of Speech Keyword Retrieval (SKR) has received considerable critical attention. SKR aims to retrieve data from a speech repository given by a spoken query. The accuracy of retrieval often depends on the performance of acoustic model. In this paper, we proposed a new speech keyword retrieval framework called DCNN-CTC using Deep Convolutional Neural Network (DCNN) based on Connectionist Temporal Classification (CTC). The proposed method provides new insights into multimedia information retrieval. The pre-trained models are fine-tuned with a CTC loss to predict target keywords, and the features are extracted by DCNN, which is a complete end-to-end acoustic model training. It does not need to align and label the data one by one in advance, and CTC directly outputs the probability of sequence prediction, which greatly improves the processing performance of the speech retrieval system. Our experimental results on benchmark datasets show that our approach leads to stable and robust retrieval performance, and the precision rate and recall rate of DCNN-CTC are much higher than the baseline system.																	1868-5137	1868-5145															10.1007/s12652-020-01933-z		APR 2020											
J								The experimental study of the effectiveness of Kohonen maps and autoassociative neural networks in the qualitative analysis of multidimensional data by the example of real data describing coal susceptibility to fluidal gasification	NEURAL COMPUTING & APPLICATIONS										Multidimensional visualization; Multidimensional data analysis; Data mining; Self-organized neural network; Autoassociative neural network; Kohonen maps	MULTIPARAMETER DATA VISUALIZATION; EVALUATE CLASSIFICATION POSSIBILITIES; SELF-ORGANIZING MAPS; ALGORITHM; PCA; SYSTEMS; FIT	The qualitative analysis of multidimensional data using their visualization allows to observe some characteristics of data in a way which is the most natural for a human, through the sense of sight. Thanks to such an approach, some characteristics of the analyzed data are simply visible. This allows to avoid using often complex algorithms allowing to examine specific data properties. Visualization of multidimensional data consists in using the representation transforming a multidimensional space into a two-dimensional space representing a computer screen. The important information which can be obtained in this way is the possibility to separate points belonging to different classes in the multidimensional space. Such information can be directly obtained if images of points belonging to different classes occupy other areas of the picture presenting these data. The paper presents the effectiveness of the qualitative analysis of multidimensional data conducted in this way through their visualization with the application of Kohonen maps and autoassociative neural networks. The obtained results were compared with results obtained using the perspective-based observational tunnels method, PCA, multidimensional scaling and relevance maps. Effectiveness tests of the above methods were performed using real seven-dimensional data describing coal samples in terms of their susceptibility to fluidal gasification. The methods' effectiveness was compared using the criterion for the readability of the multidimensional visualization results, introduced in earlier papers.																	0941-0643	1433-3058				SEP	2020	32	18			SI		15221	15235		10.1007/s00521-020-04875-x		APR 2020											
J								A novel centralized cloud information accountability integrity with ensemble neural network based attack detection approach for cloud data	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cloud computing (CC); Accountability; Data sharing; Trapdoor generator; Centralized cloud information accountability (CCIA); Imperialist competitive key generation algorithm (ICKGA); Attack detection; Dynamically weighted ensemble neural networks (DWENN); Extensible markup language (XML); java archive (JAR)	IMPERIALIST COMPETITIVE ALGORITHM; SECURITY	Highly scalable services are enabled by cloud computing and easily consumed over the Internet. User data is normally stored remotely in cloud services. Users do not own or operate the machines. This is a key feature of cloud services. Adoption of cloud services by the users are affected due to the fact that the users' concerns about having to lose control on their own information and some attackers will hack them. Therefore to overcome the existing issues, this work proposed a new centralized cloud information accountability integrity with imperialist competitive key generation algorithm (CCIAI-ICKGA) is to resolve the above problems. It also provides the attack detection to monitor the practical utilization of the users' information in the cloud environment. Second, cipher text-policy attribute-based encryption (CP-ABE) with key generation employing ICKGA and trapdoor generator is used to generate the public and private keys for every user. Third, the trapdoor generator ensures data integrity at the user level and also the cloud server level. At last, a dynamically weighted ensemble neural networks (DWENN) classifier is used for attack detection in the network. Additional guarantees of integrity and authenticity are provided by updating the structure of the log records. It is extended the framework in order to provide the security analysis which detects more possible attacks. Finally, a simulation result is carried out and renders a detailed performance analysis of the system. A result from rigorous experimental evaluation demonstrates the efficacy and resourcefulness of the novel CCIAI-ICKGA framework.																	1868-5137	1868-5145															10.1007/s12652-020-01916-0		APR 2020											
J								A new ensemble learning method based on learning automata	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Ensemble learning; Learning automata; Reinforcement learning; Data mining	SENTIMENT ANALYSIS; CLASSIFICATION; PREDICTION; KNOWLEDGE; RANKING; TWITTER	Improving the performance of machine learning algorithms has been always the topic of interest in data mining. The ensemble learning is one of the machine learning methods that, according to the subject literature, it yields better performance than the single base learner in the accuracy parameter. In the ensemble learning, all base learners are considered at the same level in terms of power and separation capabilities. However, whether the ensemble is made of homogeneous based learners or it is made of heterogeneous base learners, in either case, weaknesses and strengths of the base learners are ignored. To overcome this challenge, the stronger coefficient of influence should be assigned to stronger base learners and the lower coefficient of influence should be assigned to weaker base learners. However, given that the data is associated with uncertainty in the real-world issues, it is impossible to determine which base learner performs better than the others under these circumstances. Learning Automata is one of the desirable options of reinforcement learning subject literature to dealing with dynamic environments. The learning automata works by receiving feedback from the environment. In this paper, a method named LAbEL has been proposed which allows the assignment of the coefficient of influence to each base learner in the ensemble dynamically. Due to the use of learning automata, the proposed method works adjusted to the problem space conditions. The LAbEL is based on learning automata and according to its ability to dealing with dynamic environments, it is possible to apply it to issues where data has nonlinear and unpredictable behavior.																	1868-5137	1868-5145															10.1007/s12652-020-01882-7		APR 2020											
J								Real-time accurate eye center localization for low-resolution grayscale images	JOURNAL OF REAL-TIME IMAGE PROCESSING										Center of eye detection; Real-time image processing; Image gradient; Specular highlight removal; Human-computer interaction	GAZE ESTIMATION; PUPIL CENTER; TRACKING; LOCATION	Eye center localization is considered a crucial step for many human-computer interaction (HCI) real-time applications. Detecting the center of eye (COE), accurately and in real time, is very challenging due to the wide variation of poses, eye appearance and specular reflection, especially in low-resolution images. In this paper, an accurate real-time detection algorithm of the COE is proposed. The proposed approach depends on the image gradient to detect the COE. The computational complexity is minimized and the accuracy is improved by down sampling the face resolution and applying a rough-to-fine algorithms, to reduce the search area, in accordance with the Eye Region Of Interest (EROI) and the number of COE candidates, tested by the proposed algorithm. Also, the detection algorithm is applied on a limited number of pixels that represent the iris boundary of the COE candidates. The Look Up Tables (LUTs) are implemented to, initially, store the invariant elements of the proposed image gradient-based algorithm, to reduce the detection time. Before applying the proposed COE detection approach, a modified specular reflection method is used to improve the detection accuracy. The performance of the proposed algorithm has been evaluated by applying it to three benchmark databases: the BIOID, GI4E and Talking Face video datasets, at different face resolutions. Experimental results revealed that the accuracy of the proposed algorithm is up to 91.68% and 96.7% for BIOID and GI4E datasets, respectively, while the minimum achieved average detection time is 2.7 ms. The promising results highlight the potential of the proposed algorithm to be used in some eye gaze-based real-time applications. Comparing the proposed method with the most state-of-the-art approaches showed that the system outperforms most of them and has a comparable performance with the others, in terms of the COE localization accuracy and detection speed.																	1861-8200	1861-8219															10.1007/s11554-020-00955-2		APR 2020											
J								Improving neural machine translation through phrase-based soft forced decoding	MACHINE TRANSLATION										phrase-based; Soft forced decoding; NMT; Reranking; Hybrid MT		Compared to traditional statistical machine translation (SMT), such as phrase-based machine translation (PBMT), neural machine translation (NMT) often sacrifices adequacy for the sake of fluency. We propose a method to combine the advantages of traditional SMT and NMT by exploiting an existing phrase-based SMT model to compute the phrase-based decoding cost for an NMT output and then using this cost to rerank the n-best NMT outputs. The main challenge in implementing this approach is that NMT outputs may not be in the search space of the standard phrase-based decoding algorithm, because the search space of PBMT is limited by the phrase-based translation rule table. We propose a phrase-based soft forced decoding algorithm, which can always successfully find a decoding path for any NMT output. We show that using the phrase-based decoding cost to rerank the NMT outputs can successfully improve translation quality on four different language pairs.																	0922-6567	1573-0573				APR	2020	34	1					21	39		10.1007/s10590-020-09244-y		APR 2020											
J								Suicidal tendencies prediction in Greek poetry	EVOLVING SYSTEMS										Natural language processing (NLP); Suicide; Poetry; Deep learning; Big five; Machine learning	EMOTION DETECTION	Natural language processing (NLP) has been successfully used to predict a writer's tendency of committing suicide, using various text types: suicide notes, micro-blog posts, lyrics and poems. This paper is an extended version of earlier work. We extend our previous work on text mining in Greek Poetry by employing more sophisticated approaches. More specifically we have applied (i) Deep Neural Networks (DNN), (ii) additional morphosyntactic and semantic features based on writers' emotions and Big Five personality traits and (iii) feature selection, for suicide prediction in Greek poetry. We extend previous research to Greek, i.e. a language that has not been tackled before in this setting, using both language-dependent (but easily portable across languages) and language-independent linguistic features in order to represent the poems of 13 Greek poets of the twentieth century. Our results differ significantly from previous literature. In general, our proposed DNN model offers promising results for suicide prediction, despite the fact that this task poses multiple difficulties, especially for a language with limited related research support.																	1868-6478	1868-6486															10.1007/s12530-020-09340-7		APR 2020											
J								Handling of advanced persistent threats and complex incidents in healthcare, transportation and energy ICT infrastructures	EVOLVING SYSTEMS										Incident handling; Web mining; Data fusion; Risk assessment		In recent years, the use of information technologies in Critical Infrastructures is gradually increasing. Although this brings benefits, it also increases the possibility of security attacks. Despite the availability of various advanced incident handling techniques and tools, there is still no easy, structured, standardized and trusted way to manage and forecast interrelated cybersecurity incidents. This paper introduces CyberSANE, a novel dynamic and collaborative, warning and response system, which supports security officers and operators to recognize, identify, dynamically analyse, forecast, treat and respond to security threats and risks and and it guides them to handle effectively cyber incidents. The components of CyberSANE are described along with a description of the CyberSANE data flow. The main novelty of the CyberSANE system is the fact that it enables the combination of active incident handling approaches with reactive approaches to support incidents of compound, highly dependent Critical Information Infrastructures. The benefits and added value of using CyberSANE is described with the aid of a set of cyber-attack scenarios.																	1868-6478	1868-6486															10.1007/s12530-020-09335-4		APR 2020											
J								Free-Chattering Fuzzy Sliding Mode Control of Robot Manipulators with Joints Flexibility in Presence of Matched and Mismatched Uncertainties in Model Dynamic and Actuators	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Robot manipulator; Joint flexibility; Sliding mode control; Voltage based; Chattering; Global asymptotic stability	CONTROL DESIGN; NONLINEAR-SYSTEMS; FEEDBACK-CONTROL; FLEXIBLE LINKS; TRACKING; OBSERVER	In this paper, a simple but effective voltage-based fuzzy sliding mode control (SMC) is proposed to control the position of a class of flexible-joints robot manipulators with n degrees of freedom in presence of matched and mismatched uncertainties related to the electrical and mechanical equations. In order to solve problems in comparison with conventional SMC, a sliding surface with the time-varying parameter is proposed which not only eliminates chattering and increases the stability of the closed loop system, but also prevents the increase of the control input voltage. Furthermore, an approach is proposed to calculate the sliding surface vector coefficients which, in presence of the existing uncertainties, guarantee the global asymptotically stability of the closed loop system. Finally, simulation and practical implementation results are presented to exhibit the helpfulness of the proposed control technique compared to the previous methods.																	0921-0296	1573-0409				OCT	2020	100	1					47	69		10.1007/s10846-020-01178-0		APR 2020											
J								Machine intelligent diagnostic system (MIDs): an instance of medical diagnosis of tuberculosis	NEURAL COMPUTING & APPLICATIONS										Machine intelligent diagnosis system (MIDs); Knowledge-base (KB); Fuzzy logic (FL); Artificial intelligence (AI); Membership function (MF)		The article aims to review, analyze, design and implement a philosophy of medical diagnosis by artificial intelligence (AI) and soft computing techniques. The theme of the paper is that there are abundant corruptions in therapeutic judgment; in its place of appropriate diagnosis, majority practitioners go behind the narrow path by trapping the people at a serious phase. The ordinary community suffers deficient in diagnosis for higher investigative costs as well as the lack of certified practitioners. This article proposes some AI techniques to eradicate this lacuna by designing a prototype. The proposed prototype here termed as machine intelligent diagnostic system (MIDs), which has the capability of learning, thinking, reasoning and managing uncertainty as a real-world doctor. The model structured according to AI techniques considers the different cases of the disease to implement it. This article analyzes the shortcoming of MIDs, which can perform as a doctor to serve society as regular fashion as well as at the time of crisis as a crisis-manager. The degrees of the acuteness of patients' symptoms are perceived by a membership function, which is used to tackle the emotion of the patients, and a fuzzy logic membership function is being used to calculate probabilities of diseases. Finally, this work finds smart results of MIDs, which can serve as doctors to some extent to compensate for the crisis of doctor in the universe.																	0941-0643	1433-3058				OCT	2020	32	19			SI		15585	15595		10.1007/s00521-020-04894-8		APR 2020											
J								A deep neural network-based model for named entity recognition for Hindi language	NEURAL COMPUTING & APPLICATIONS										Neural networks; Machine learning; Sequence labeling; Deep learning; Convolutional neural network; Bi-LSTM		The aim of this work is to develop efficient named entity recognition from the given text that in turn improves the performance of the systems that use natural language processing (NLP). The performance of IoT-based devices such as Alexa and Cortana significantly depends upon an efficient NLP model. To increase the capability of the smart IoT devices in comprehending the natural language, named entity recognition (NER) tools play an important role in these devices. In general, the NER is a two-step process that initially the proper nouns are identified from text and then classify them into predefined categories of entities such as person, location, measure, organization and time. NER is often performed as a subtask while processing natural languages which increases the accuracy level of a NLP task. In this paper, we propose deep neural network architecture for named entity recognition for the resource-scarce language Hindi, based on convolutional neural network (CNN), bidirectional long short-term memory (Bi-LSTM) neural network and conditional random field (CRF). In the proposed approach, initially, we use skip-gram word2vec model and GloVe model to represent words in semantic vectors which are further used in different deep neural network-based architectures. In the proposed approach, we use character- and word-level embedding to represent the text that includes information at fine-grained level. Due to the use of character-level embeddings, the proposed model is robust for the out-of-vocabulary words. Experimental results show that the combination of Bi-LSTM, CNN and CRF algorithms performs better as compared to the other baseline methods such as recurrent neural network, long short-term memory and Bi-LSTM individually.																	0941-0643	1433-3058				OCT	2020	32	20			SI		16191	16203		10.1007/s00521-020-04881-z		APR 2020											
J								A deep reinforcement learning process based on robotic training to assist mental health patients	NEURAL COMPUTING & APPLICATIONS										Robotic; Patient mental health; Deep reinforcement learning; Patient positive attitude	PEOPLE; SCHIZOPHRENIA; DISABILITY; DISORDERS	Nowadays, robots are playing a vital role in healthcare applications to provide patients support and assistance in critical situations. The robots are trained by artificial intelligence systems which help to learn the robot according to their patient needs. However, the robots require the medical staff while diagnosing diseases with maximum accuracy, remote treatment, paralyzed patient treatment and so on. For these precise and accurate issues, an intelligent learning process is applied to train the robot to support the patient's mental health and related task in this work. Initially, the patient health details are collected along with their simple daily routine, medical checkup information and other healthcare details. The gathered details are processed with the help of a deep reinforcement learning process used to get important information. The learning approach uses the state and action process to determine every patient's needs and the respective assistance. Based on the information, robots are trained continuously to keep patient positive attitudes in their mental health problems. The excellence of the system is evaluated using experimental analysis in which the deep reinforcement system ensures a 0.083 error rate and 98.42% accuracy.																	0941-0643	1433-3058															10.1007/s00521-020-04855-1		APR 2020											
J								Synergetic fusion of energy optimization and waste heat reutilization using nature-inspired algorithms: a case study of Kraft recovery process	NEURAL COMPUTING & APPLICATIONS										Multiple-stage evaporator; Energy efficiency; Energy reduction schemes; Flash tanks; Nature-inspired algorithms	MULTIPLE-EFFECT EVAPORATORS; BLACK LIQUOR GASIFICATION; FEED FLOW SEQUENCE; GENETIC ALGORITHMS; SOLVING SYSTEMS; SIMULATION; PAPER; PULP; INTEGRATION; SELECTION	This article presents a novel energy management strategy of multiple-stage evaporator (MSE). The maximum efficiency of MSE is achieved by optimum selection of unknown steady-state process parameters such as vapor temperatures and liquor flow rates. Various energy reduction schemes (ERSs) have been integrated to achieve a substantial enhancement in energy efficiency. For energy optimization, a set of nonlinear mathematical models for various ERSs are formulated and transformed to optimization problems. Three nature-inspired algorithms, namely GA, DE and PSO, are employed to compute these optimal process parameters and hence evaluate the energy efficiency. The simulated results accentuate that these algorithms efficiently converge approximately at the same values. The results reveal that the hybrid model with maximum efficiency of 8.24 is characterized as the most energy-efficient operating strategy. The amalgamation of flash tanks with the intention of reutilizing the waste steam further enhances the energy efficiency by 4.97%, thereby proving to be the most prominent operating strategy with the highest efficiency of 8.65.																	0941-0643	1433-3058															10.1007/s00521-020-04828-4		APR 2020											
J								VNE strategy based on chaos hybrid flower pollination algorithm considering multi-criteria decision making	NEURAL COMPUTING & APPLICATIONS										Virtual network embedding; Genetic algorithm; Flower pollination algorithm; BP neural network; Chaos optimization strategy	INTELLIGENT TRANSPORTATION SYSTEM; GENETIC ALGORITHM; OPTIMIZATION; LOCATION	With the development of science and technology and the need for multi-criteria decision making (MCDM), the optimization problem to be solved becomes extremely complex. The theoretically accurate and optimal solutions are often difficult to obtain. Therefore, meta-heuristic algorithms based on multi-point search have received extensive attention. The flower pollination algorithm (FPA) is a new swarm intelligence meta-heuristic algorithm, which can effectively control the balance between global search and local search through a handover probability, and gradually attracts the attention of researchers. However, the algorithm still has problems that are common to optimization algorithms. For example, the global search operation guided by the optimal solution is easy to lead the algorithm into local optimum, and the vector-guided search process is not suitable for solving some problems in discrete space. Moreover, the algorithm does not consider dynamic multi-criteria decision problems well. Aiming at these problems, the design strategy of hybrid flower pollination algorithm for virtual network embedding problem is discussed. Combining the advantages of the genetic algorithm and FPA, the algorithm is optimized for the characteristics of discrete optimization problems. The cross-operation is used to replace the cross-pollination operation to complete the global search and replace the mutation operation with self-pollination operation to enhance the ability of local search. Moreover, a life cycle mechanism is introduced as a complement to the traditional fitness-based selection strategy to avoid premature convergence. A chaos optimization strategy is introduced to replace the random sequence-guided crossover process to strengthen the global search capability and reduce the probability of producing invalid individuals. In addition, a two-layer BP neural network is introduced to replace the traditional objective function to strengthen the dynamic MCDM ability. Simulation results show that the proposed method has good performance in link load balancing, revenue-cost ratio, VN requests acceptance ratio, mapping average quotation, average time delay, average packet loss rate, and the average running time of the algorithm.																	0941-0643	1433-3058															10.1007/s00521-020-04827-5		APR 2020											
J								Development of an algorithm using the AHP method for selecting software according to its functionality	SOFT COMPUTING										Functionality; Analytic hierarchy process; Correctness; Compatibility; Accuracy		The article develops an algorithm by using the analytic hierarchy process (AHP) method to evaluate the functionality of software. Software functionality is the capacity to perform a number of software product functions. In other words, the software functionality means the ability of software product to perform a number of functions. AHP is a structured technique of unified decision making. It does not answer the question of what is right or wrong, but enables a decision maker to evaluate which of the options he/she chooses complies with his/her needs and to comprehend the problem. Two options are considered here. The first option fulfills the pairwise comparison using three functionality criteria and three functionality alternatives to determine which criterion is superior. Assume that there are three criteria: correctness, compatibility, and accuracy. It is necessary to determine which criterion is most important through pairwise comparison. The second option uses five software, five functionality criteria, and five functionality alternatives. Here, superior criterion and best software are identified. This work is one of the first articles implementing the pairwise comparison and evaluation of software functionality for five criteria and five alternatives through AHP. It will enable the program to work correctly and accurately. This is proven by the experience and results obtained. In all other studies, a maximum of three software functionality criteria have been used so far.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8495	8502		10.1007/s00500-020-04902-y		APR 2020											
J								Performance improvement of cloud security with parallel anarchies society optimization algorithm for virtual machine selection in cloud computing	SOFT COMPUTING										Cloud computing; Convolutional neural network; Advanced encryption standard; Parallel anarchies society optimization		Cloud computing (CC) is a future promising computing technology and has proven tremendous exceptional attainable in managing the hardware and software program sources placed at third-party provider vendors. However, security and virtual machines (VMs) selection to a process a request characterize a large venture. The major problem in the usage of the technology is the security concerns which increase the demand for a robust security mechanism to protect the data on the cloud. Hence in order to resolve this issue, an approach was designed for the purpose of enhancing the cloud security as well as the time of execution known as weighted mean-based convolutional neural network with advanced encryption standard (WMCNN-AES). By employing the method of pseudorandom number generators (PRNGs), random keys are generated and the finest keys are produced by using the WMCNN method. In the WMCNN approach, randomly generated keys are taken as input and the approximation concerning the secret key of the input is produced by the hidden layers. For the purpose of encrypting the data, the finest secret keys are generated by the output layer. By employing the advanced encryption standard (AES) algorithm, encryption is executed. Storage of the files concerning the encrypted data is done in the cloud storage system. Optimal selection of VMs performs a significant enhancement of the performance through reducing the execution time of requests (tasks) coming from stakeholders and maximizing utilization of cloud resources. For the purpose of optimizing the virtual machines' or VMs' selection, parallel anarchies society optimization (PASO) is employed in the cloud environment to increase the performance and resource utilization. When comparing the experimental results of the proposed system with the existing system, it is observed that the proposed system accomplished an enhanced performance with respect to resource utilization, cost, throughput, service latency as well as execution time.																	1432-7643	1433-7479				OCT	2020	24	19					15081	15092		10.1007/s00500-020-04892-x		APR 2020											
J								Improving self-training with density peaks of data and cut edge weight statistic	SOFT COMPUTING										Semi-supervised classification; Self-training; Density peaks; Cut edge weight; Hypothesis testing	CLASSIFICATION; REGRESSION; ENSEMBLE; MACHINE; NOISE	Semi-supervised classification has become an active topic recently, and a number of algorithms, such as self-training, have been proposed to improve the performance of supervised classification using unlabeled data. Considering the influence of spatial distribution of data set and mislabeled samples on the classification performance of self-training method, an improved self-training algorithm based on density peaks and cut edge weight statistic is proposed in this paper. Firstly, the representative unlabeled samples are selected for labels prediction by space structure, which is discovered by clustering method based on density peaks. Secondly, cut edge weight is used as statistics to make hypothesis testing for identifying whether samples are labeled correctly. Thirdly, the labeled data set is gradually enlarged with correctly labeled samples. The above steps are iterated until all unlabeled samples are labeled. The framework of improved self-training method not only makes full use of space structure information, but also solves the problem that some samples may be classified incorrectly. Thus, the classification accuracy of algorithm is improved in a great measure. Extensive experiments on benchmark data sets clearly illustrate the effectiveness of proposed algorithm.																	1432-7643	1433-7479				OCT	2020	24	20					15595	15610		10.1007/s00500-020-04887-8		APR 2020											
J								JMD method for transforming an unbalanced fully intuitionistic fuzzy transportation problem into a balanced fully intuitionistic fuzzy transportation problem	SOFT COMPUTING										Transportation problem; TIFN; Intuitionistic fuzzy transportation problem; Optimal solution		Mahmoodirad et al. (Soft Comput, 2018. 10.1007/s00500-018-3115-z) proposed an approach for solving fully intuitionistic fuzzy transportation problems (FIFTPs) (transportation problems in which each parameter is represented as a triangular intuitionistic fuzzy number). In this approach, firstly, an unbalanced fully intuitionistic fuzzy transportation problem (FIFTP) is transformed into a balanced FIFTP, and then the intuitionistic fuzzy (IF) optimal solution of the transformed balanced FIFTP is obtained. In this paper, it is shown that Mahmoodirad et al.'s approach fails to transform an unbalanced FIFTP and hence, Mahmoodirad et al.'s approach fails to find the IF optimal solution of unbalanced FIFTPs. It is obvious that to overcome this limitation of Mahmoodirad et al.'s approach, there is need to propose a method to transform an unbalanced FIFTP into a balanced FIFTP. Therefore, in this paper, a new method (named as JMD method) is proposed to transform an unbalanced FIFTP into a balanced FIFTP.																	1432-7643	1433-7479				OCT	2020	24	20					15639	15654		10.1007/s00500-020-04889-6		APR 2020											
J								Pragmatic approach for prioritization of flood and sedimentation hazard potential of watersheds	SOFT COMPUTING										Flooding; Sedimentation; Gorganrud basin; TOPSIS; SAW; ELECTRE; VIKOR methods	DECISION-MAKING; MORPHOMETRIC-ANALYSIS; MCDM METHODS; LAND-USE; GIS; SOIL; SUBWATERSHEDS; PREDICTION; SYSTEM; MODEL	Flood is one of the natural disasters that generates a lot of damages every year in different points of the world. Also, soil erosion is among the processes that threats the soil and water resources of the country. The performance of a small watershed is not same as the hydrologic response of a large watershed. Determining the amount of participation and prioritizing the sub-basins in terms of flood generation in the outlet of the basin can be a large help in correct locating the flood control and soil and water conservation projects and leads to decreasing the negative impacts of the flood control operations at unnecessary regions or at the regions with lower priority and also it prevents personal tastes. Therefore, determining the flood generator regions and prioritizing the sub-basins in terms of flooding and sedimentation potential is essential for better management of the watersheds. For this purpose, the aim of this research is to determine the amount of participation of the eastern sub-basins of the Gorganrud River Basin of Golestan province, Iran in flooding and sedimentation and their prioritization in terms of flooding and sedimentation potential using the multi-criteria decision-making methods. In this present study, we used area estimation indices, gravel coefficient, drainage density, basin average slope, basin average height, curve number, cover percentage, sediment yield, sediment delivery ratio, runoff height and concentration time. Indicators are considered to be important indicators affecting water permeability, runoff production and, consequently, the potential for flooding and sedimentation. Following the formation of decision matrix with 13 options (sub-basins) and 11 criteria (evaluation index), Technique For order Preference by Similarity to ideal Solution (TOPSIS), Simple Additive Weighting (SAW), Elimination Et Choice Translation Reality (ELECTRE) and Vise Kriterijumska Optimizacija Kompromisno Resenje (VIKOR) techniques were used to prioritize sub-basins. Borda and Copland methods were used to combine the rank of proposed techniques. Also, in order to validate the models, we estimated the percentage change and the intensity of the changes. The results showed that the highest runoff height index (0.179) and the average height index of the basin had the lowest weight (0.031), according to experts. Considering the results of the combined ranking of the proposed techniques, sub-basins 12, 1 and 2 are in first to third priority, respectively, and have a more critical situation than the rest of the sub-basins. Field studies clearly show the results of the research, because sub-basins 12 and 1 exhibit the highest erosion, poor soil and gradient. Also, zones with flood and sedimentation potential in the area showed that 49/31% of the area in high and very high risk. This study proves that multi-criteria decision-making methods and RS and GIS techniques are very suitable, precise, economically and temporally advantageous, and helpful tools to evaluate and prioritize the sub-basins in the soil erosion and soil and water conservation topics. Therefore, considering the multiple objective functions and the costly watershed management operations, it can be said that multi-criteria decision-making methods can be used for better management of watersheds in terms of biological and structural flood control operations. So, prioritization can be done based on a mathematical logic. The proposed method in this research is very suitable for watersheds without sufficient data. Hence, such researches which are low cost as well as quick can be used and watersheds can be prioritized for management and conservative acts.																	1432-7643	1433-7479				OCT	2020	24	20					15701	15714		10.1007/s00500-020-04899-4		APR 2020											
J								Event- and time-triggered dynamic task assignments for multiple vehicles	AUTONOMOUS ROBOTS										Dynamic task assignment; Multiple vehicles; Event-triggered algorithms; Time-triggered algorithms	TARGET ASSIGNMENT; ALLOCATION; ALGORITHM; TAXONOMY	We study the dynamic task assignment problem in which multiple dispersed vehicles are employed to visit a set of targets. Some targets' locations are initially known and the others are dynamically randomly generated during a finite time horizon. The objective is to visit all the target locations while trying to minimize the vehicles' total travel time. Based on existing algorithms used to deal with static multi-vehicle task assignment, two types of dynamic task assignments, namely event-triggered and time-triggered, are studied to investigate what the appropriate time instants should be to change in real time the assignment of the target locations in response to the newly generated target locations. Furthermore, for both the event- and time-triggered assignments, we propose several algorithms to investigate how to distribute the newly generated target locations to the vehicles. Extensive numerical simulations are carried out which show better performance of the event-triggered task assignment algorithms over the time-triggered algorithms under different arrival rates of the newly generated target locations.																	0929-5593	1573-7527				MAY	2020	44	5					877	888		10.1007/s10514-020-09912-1		APR 2020											
J								A study on compact structural soft sets and an application method	COMPLEX & INTELLIGENT SYSTEMS										Topology; Compactness; Soft set; Compact structural soft set; Decision-making		Since the problems of daily life contain a lot of data and obscurity, it has become a necessity to construct new mathematical methods to solve these problems. In this paper, we have established compact-structural soft sets and studied its basic structural properties. Then, we have proposed an application method for decision-making problems using compact-structural soft sets.																	2199-4536	2198-6053				JUL	2020	6	2					401	409		10.1007/s40747-020-00135-6		APR 2020											
J								A novel real-time object tracking based on kernelized correlation filter with self-adaptive scale computation in combination with color attribution	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Correlation filter; Object tracking; Scale computation; Kernel transformation; Weighted coefficient; Color attribution	VISUAL TRACKING	Recently, object tracking model based on correlation filter has achieved great success. However, most of these methods relying on correlation filter train many classifiers individually, and fail to cope with significant appearance and scale change of object and complex challenging environment. How to accurately and robustly compute object scale relative to template in visual object tracking application is a challenge topic at home and abroad. Many existing tracking algorithms are not very good at big scale variation in complex video. In order to solve the problem, a novel and effective scale computation algorithm based on tracking in detection is proposed in paper. The tracking object is divided into four parts, and compute their scale factors, then get the maximum response location by adopting kernelized correlation filter based on color attribution. Specially, aimed at the difficulty of object location, weighted coefficients are used to remove the abnormal matching points and the desired training output from common classifier is transformed into other forms. Simulation experiment result shows our proposed algorithm outperform others algorithms in color video with big scale variation.																	1868-5137	1868-5145															10.1007/s12652-020-01872-9		APR 2020											
J								A novel possibilistic artificial immune-based classifier for course learning outcome enhancement	KNOWLEDGE AND INFORMATION SYSTEMS										Classification; Artificial immune recognition system; Possibility theory; Uncertain reasoning; Educational data mining	RECOGNITION SYSTEM; PREDICTION; DROPOUT	In this paper, we propose PAIRS3: a possibilistic classification approach based on artificial immune recognition system (AIRS) and the possibility theory. PAIRS3 is applied to address shortcomings in student attainment rates of course learning outcomes by predicting effective remedial actions through learning from assessment rubrics instances. For most of assessment rubric instances, it is difficult to determine the unique most effective remedial action to take. Consequently, each rubric instance will be labeled with uncertain remedial actions which are elicited from quality assurance experts. Elements from possibility theory are used to (1) model the uncertainty about the most effective remedial action labeling each rubric instance and (2) adapt several parts of the standard AIRS algorithm in order to address the uncertainty in class labels. The performance of the proposed method is evaluated against an academic, university level, assessment dataset that has been built progressively over multiple academic semesters. Despite the uncertainty related to the class labels in the dataset, PAIRS3 showed a good performance in terms of accuracy level (close to 75%). Also, when compared to existing state-of-the-art possibilistic classifiers such as PAIRS2, non-specificity possibilistic decision trees (NSPDT), and cluster-based possibilistic decision trees (Clust-PDT), PAIRS3 achieved better accuracy improvement ranging from 10% (in case of Clust-PDT) to 17% (in case of PAIRS2).																	0219-1377	0219-3116				SEP	2020	62	9					3535	3563		10.1007/s10115-020-01465-0		APR 2020											
J								A machine learning method for selection of genetic variants to increase prediction accuracy of type 2 diabetes mellitus using sequencing data	STATISTICAL ANALYSIS AND DATA MINING										Cornish-Fisher expansion; feature selection; nearest shrunken centroid; sparse PCA	GENOME-WIDE ASSOCIATION; NONCONCAVE PENALIZED LIKELIHOOD; LOGISTIC-REGRESSION; POPULATION	Type 2 diabetes mellitus (T2DM) affects millions of people through its life-altering complications. Worldwide, 3.4 million people die of diabetes annually. Studying the effect of genetic polymorphism on T2DM has been plagued by the available sample size. A 2016 Nature Reviews article summarized that the accuracy of predicting future type 2 diabetes from genetic polymorphism is very low at the population level. Innumerable associations between genes, environmental factors, and type 2 diabetes remain to be discovered. This research presents a method to identify subtle effects of genetic variants using whole genome sequencing data and improve prediction accuracy of T2DM at the population level. To achieve this, a new feature selection procedure and a classifier are proposed. The method involves (a) first applying sparse principal component analysis to genotype data to obtain orthogonal features; (b) building a new classifier using single nucleotide polymorphism (SNP)-specific regularization parameters to reduce the false positive rate of feature selection; (c) verifying feature relevance through penalized logistic regression. After application to a dataset containing 625 597 SNPs and 23 environmental variables from each of 3326 humans, the method identified 271 genetic variants with subtle effects on T2DM prediction. These variants led to greatly improved prediction accuracy for new patients at the population level. The proposed method also has the advantage of computational efficiency, over 15 times faster than random forest and extreme gradient boosting (XGBoost) classifiers, and thus provides a promising tool for large-scale genome-wide association studies.																	1932-1864	1932-1872				JUN	2020	13	3					261	281		10.1002/sam.11456		APR 2020											
J								Gesture and Speech Recognizing Helper Bot	APPLIED ARTIFICIAL INTELLIGENCE											AAC; COMMUNICATION; RECOGNITION; SYSTEM; PEOPLE	In industries, difficult work is being decreased everywhere scale to expand effectiveness and exactness, and gain benefit by introducing robots that can do repetitive works at lesser expense of preparing. A onetime establishment of such a gadget may cost an enormous sum at first, yet in the more drawn out run, will end up being more beneficial than difficult work. Out of the part, a basic robotic arm is a standout amongst the most generally introduced machines. Robotic arm is one of the significant undertakings in the present computerization industry. Automated arm is a piece of the mechatronic industry which is a quickly versatile and developing industry today. Distinctive changes and extra features are being associated with the first kind of straightforward robotic arm to upgrade its ease of use under various conditions. In this paper, we are building up a robotic arm which will have a free rotation around multiple axes and we are including the technology of Image Processing with it to make it a visual signal based working robotic arm. The model is a pick and place robotic arm you can take the desired article starting with one place and carry it to another place with its gripper claw. The operations will be constrained by a visual processing framework that reads the gestures and will give directions to the Arm for performing various types of movements and tasks. We are making use of low torque servos to lift light weight.																	0883-9514	1087-6545				JUN 6	2020	34	7					585	595		10.1080/08839514.2020.1740473		APR 2020											
J								Software reliability prediction model with realistic assumption using time series (S)ARIMA model	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Software reliability; Time series; Seasonal ARIMA model; Failure prediction	ARIMA	Software reliability is the important attribute for complex computing systems to provide reliability could cause series issues such as extra cost, development delay and image of the software solution providers. Hence, ensuring the reliability of software before deliver to the customer is essential part for the company. Finding the error in right time with reasonable degree of accuracy helps to prevent the consequences. Several software reliability growth models developed and used to measure the trustworthiness based on development and testing phases with unrealistic assumption over the environment and applied Block box methodologies while constructing model. This paper presents well established statistical time series (S)ARIMA approach for developing a forecasting model that able to provide significantly improved reliability prediction. Using real time publicly available software failure sets, the prediction of proposed model is developed and compared with previously available reliability models.																	1868-5137	1868-5145															10.1007/s12652-020-01912-4		APR 2020											
J								Applying improved K-means algorithm into official service vehicle networking environment and research	SOFT COMPUTING										Vehicle network environment for official use; Virtual official vehicles; Multi-commodity flow; K-means algorithm; Traffic control	LOCAL SEARCH YIELDS	In order to improve the traffic efficiency of official vehicles in the traffic road network, a backpressure routing control strategy for multi-commodity flow (official traffic flow) using official vehicle network environmental data information is proposed. Firstly, the road network composed of official service vehicle-mounted wireless network nodes is used to collect information on road conditions and official service vehicles. In order to improve the real-time and forward-looking route control, an official service vehicle flow forecasting method is introduced to construct a virtual official service vehicle queue. A multi-commodity flow (official service vehicle flow) backpressure route method is proposed, and an official service vehicle control strategy is designed to improve the self-adaptive route of K-means algorithm. In addition, the weight of backpressure strategy is improved according to traffic pressure conditions, and the adaptability of backpressure route algorithm is improved by using optimized parameters. Finally, the simulation results show that the proposed method can effectively control traffic vehicles and improve traffic smoothness.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8355	8363		10.1007/s00500-020-04893-w		APR 2020											
J								A hybrid MCDM-based FMEA model for identification of critical failure modes in manufacturing	SOFT COMPUTING										Multiple criteria decision making (MCDM); Failure mode and effects analysis (FMEA); Risk assessment; Manufacturing	SUSTAINABLE-SUPPLIER SELECTION; EXTENDED VIKOR METHOD; DECISION-MAKING; PARTNER SELECTION; FUZZY DEMATEL; RISK; FRAMEWORK; COPRAS; TOPSIS; TOOL	The effective identification of critical failure modes of individual equipment components or processes and the development of plans for improvement are crucial for the manufacturing industry. Recently, the failure modes and effects analysis (FMEA) approach based on multiple criteria decision making (MCDM) has been utilized effectively for the assessment of primary failure modes and risks. However, the ranking results of failure modes produced by different MCDM methods might be different. This study proposes an integrated risk assessment model where several techniques are combined to produce an FMEA model for the generation of comprehensive failure mode ranking. First, the anticipated costs and environmental protection indicators are included in the FMEA model to enhance the comprehensiveness of assessment. Then, an influential network relationship map of risk factors is obtained by using the decision-making trial and evaluation laboratory (DEMATEL) technique to assist in identifying the critical factors. Finally, the ranking of the failure modes is identified using the four integrated MCDM methods, based on the technique for order preference by similarity to ideal solution (TOPSIS) concept. In addition, data from a machine tool manufacturing company survey are applied to demonstrate the effectiveness and robustness of the proposed model.																	1432-7643	1433-7479				OCT	2020	24	20					15733	15745		10.1007/s00500-020-04903-x		APR 2020											
J								A similarity model based on reinforcement local maximum connected same destination structure oriented to disordered fusion of knowledge graphs	APPLIED INTELLIGENCE										Maximum common subgraph; Same destination paths; Similarity model; Fusion modeling; Fragmentation knowledge graph	ALGORITHM; CONSTRUCTION; HEURISTICS	The alignment and fusion of knowledge graphs have been at entity alignment and fusion which match and align knowledge graphs (KGs) by measuring similarity of the entities in KGs. Nevertheless, false fusion of completely different KGs can be easily caused if only considering entity similarity but ignoring entity relationship similarity. This paper focuses on entity relationship similarity model and KGs fusion method to achieve automatic construction of KGs. First, Same Destination Paths is developed based on Maximum Common Subgraph, which is used to build the Local Maximum Connected Same Destination Structure (LMCSDS) model to measure the entity relationship similarity of KGs. Then, a fusion method for similar fragmentation KGs (FKGs) is developed by analyzing the types of FKGs. Third, a Reinforcement Local Maximum Connected Same Destination Structure (RLMCSDS) similarity model is developed to ensure that the similarity between FKGs can still be measured correctly after fusion of FKGs. Meanwhile, the fusion results obtained by the developed RLMCSDS model and fusion method are theoretically proved to be independent with the fusion order of FKGs. Finally, experimental results on several datasets demonstrate the outstanding performance of the RLMCSDS model in comparison with some existing methods. Moreover, a complete KG can be built by the RLMCSDS model and the fusion method.																	0924-669X	1573-7497				SEP	2020	50	9					2867	2886		10.1007/s10489-020-01673-9		APR 2020											
J								Shaping Emotions in Negotiation: a Nash Bargaining Solution	COGNITIVE COMPUTATION										Negotiation; Emotional model; Feelings; Emotional distance; Bargaining; Game theory	ANGER; DISAPPOINTMENT; CONFLICT; OUTCOMES; PEOPLE; GAME	Modeling emotions in negotiations is an open challenge that attracted an increasing amount of attention from researchers. Bargainers look for achieving an agreement with the opposing parties and, at the same time, try to reach their own goals. This process consists of both bargaining and (game theory) problem solving. Game theory models seek to enlighten the rational negotiations between players, but these models lack the evidence of how emotional motives may influence individuals' behavior. This paper suggests a model for shaping emotions in negotiation using Nash's bargaining approach. We focus on the case where negotiation between players has motives of cooperating, considering eight emotions: anger, fear, joy, sadness, surprise, disgust, guilt, and disappointment. For representing the solution of the problem, we employ a homogeneous Markov game. The simplicity of the model relies on the fact that the emotions are represented by the states of the Markov chain. The relationship between the emotions is represented by a transition matrix that determines the probability of changing between the emotions (states) at any time. Because any emotion can be reached at any time with certain probability, the bargaining Markov game is ergodic. We represent naturally the emotional process of bargaining using a proximal method, which involves the bargaining Nash product for computing the equilibrium of the game. We show the convergence of the method to the emotional equilibrium point. The solution of the Nash bargaining game consists of cooperative emotional strategies, which are transformed in emotional probability distributions. Such emotional probability distributions are measured using an asymmetric distance function that determines the "emotional distance" between players in negotiations. Emotions are measured using an asymmetric distance function because they are different between players. We present a new approach for shaping emotions in negotiations employing Nash's bargaining model. An application example shows the influence of expressing emotions in the relationship process, and those emotions are strategically selected to gain a benefit in negotiations. We show that the magnitude and direction of emotional distance matter and that feelings have an asymmetric effect on the negotiation process.																	1866-9956	1866-9964				JUL	2020	12	4					720	735		10.1007/s12559-020-09713-9		APR 2020											
J								A new L-1-LRC based model for oranges origin identification with near infrared spectra data	EVOLUTIONARY INTELLIGENCE										L-1-LRC; Near infrared spectroscopy; Origin identification	NIR SPECTROSCOPY; PREDICTION; ALGORITHM; NITROGEN	In order to establish an accurate and efficient model for geographical origin identification of oranges, a new model based on L-1-norm linear regression classification (L-1-LRC) is proposed. The proposed L-1-LRC for orange origin identification is based on minimum reconstruction error using the L-1-norm regularization learning method, which can combine the feature selection and classifier learning, and can reveal the structure characteristics of spectral information effectively. The experimental results show that the proposed L-1-LRC model can achieve higher accuracy rate of 92.35% and perform much better than existing models when using only a few training samples. Thus, this work would lead to a new method for fast and efficient identification of geographical origins with near infrared (NIR) spectroscopy.																	1864-5909	1864-5917															10.1007/s12065-020-00399-4		APR 2020											
J								Application research of intelligent speech technology in maritime command and control system	EVOLUTIONARY INTELLIGENCE										Command and control system; Intelligent speech technology; Command terms		According to the requirements of maritime command and control system, a method using intelligent speech technology is put forward to improve the ability of command and control. The architecture of the system is designed, and the functions of speech recognition training, analyzing, processing, implementing and responding are designed and verified. The results of simulation show that the ability of command and control can be improved effectively. This method can be taken as a reference for improving the intelligent ability of the marine command and control system.																	1864-5909	1864-5917															10.1007/s12065-020-00398-5		APR 2020											
J								Ensemble feature selection in medical datasets: Combining filter, wrapper, and embedded feature selection results	EXPERT SYSTEMS										ensemble; feature selection; data mining; dimensionality reduction; feature selection; medial datasets	PRINCIPAL COMPONENT ANALYSIS; CLASSIFICATION; OPTIMIZATION	Feature selection is a process aimed at filtering out unrepresentative features from a given dataset, usually allowing the later data mining and analysis steps to produce better results. However, different feature selection algorithms use different criteria to select representative features, making it difficult to find the best algorithm for different domain datasets. The limitations of single feature selection methods can be overcome by the application of ensemble methods, combining multiple feature selection results. In the literature, feature selection algorithms are classified as filter, wrapper, or embedded techniques. However, to the best of our knowledge, there has been no study focusing on combining these three types of techniques to produce ensemble feature selection. Therefore, the aim here is to answer the question as to which combination of different types of feature selection algorithms offers the best performance for different types of medical data including categorical, numerical, and mixed data types. The experimental results show that a combination of filter (i.e., principal component analysis) and wrapper (i.e., genetic algorithms) techniques by the union method is a better choice, providing relatively high classification accuracy and a reasonably good feature reduction rate.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12553	10.1111/exsy.12553		APR 2020											
J								Robust parallel-batching scheduling with fuzzy deteriorating processing time and variable delivery time in smart manufacturing	FUZZY OPTIMIZATION AND DECISION MAKING										Variable neighborhood search; Multi-Verse Optimizer; Deteriorating effect; Fuzzy processing time; Parallel-batching machines; Delivery		Smart manufacturing is an effective way to improve the efficiency of resource utilization and reduce the response time of making joint decisions for the enterprises. Though, with the globalization of manufacturing enterprises, manufacturing optimization problems often occur in complex manufacturing systems under the deteriorating and fuzzy environment, which brings many challenges to smart manufacturing, such as the lack of coordinating scheduling strategies to guarantee the low latency requirement. This paper investigates a robust parallel-batching scheduling problem with fuzzy processing time and past-sequence-dependent delivery time. Some structural properties are first identified, and an optimal algorithm is further developed for the single-machine scheduling problem. Then, the problem is proved to be NP-hard. We thus design a hybrid Multi-Verse Optimizer-Variable Neighborhood Search algorithm to solve the investigated problem in a reasonable time. Abundant experiments of different scales are conducted to verify the performance of the proposed hybrid method with a comparison of the state-of-the-art methods. The proposed hybrid meta-heuristic shows excellent results, robustness, and computational time performance under various experiments.																	1568-4539	1573-2908				SEP	2020	19	3					333	357		10.1007/s10700-020-09324-x		APR 2020											
J								Exploring of alternative representations of facial images for face recognition	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Image representation; Alternative representation; High resolution; Face recognition	SPARSE REPRESENTATION; SAMPLES	Description and classification of face images is a significant task of computer vision, machine learning and pattern recognition communities. In the past, researchers have made tremendous efforts in this task. Previous researchers always seek high-resolution face images for better image classification. However, with this paper, we present and demonstrate a new opinion that in some cases the use of alternative representations of facial images are very useful for face recognition and properly reducing the image resolution might be beneficial to better classification of face images. This may be attributed to the deformable property of faces and the fact that the proposed alternative representations can in some extent reduce the within-class difference of facial images. Also, the presented idea appear to be useful for helping people to improve face recognition techniques in real worlds.																	1868-8071	1868-808X				OCT	2020	11	10					2289	2295		10.1007/s13042-020-01116-4		APR 2020											
J								epsilon Constrained differential evolution using halfspace partition for optimization problems	JOURNAL OF INTELLIGENT MANUFACTURING										Constrained optimization problem; epsilon Constrained method; Differential evolution; Halfspace partition	ALGORITHM; FORMULATION; DESIGN	There are many efficient and effective constraint-handling mechanisms for constrained optimization problems. However, most of them evaluate all the individuals, including the worse individuals, which waste a lot of fitness evaluations. In this paper, halfspace partition mechanism based on constraint violation values is proposed. Since constraint violation information of individuals in current generation are already known, the positive side of tangent line of one point as positive halfspace is defined. A point is treated as potential point if it locates in the intersect region of two positive halfspaces. Hence, the region includes all these points has greater possibility to obtain smaller constraint violation. Only when the offspring locates in this area, the actual objective function value and constraint violation will be calculated. The estimated worse individuals will be omitted without calculating actual constraint violation and fitness function value. Four engineering optimization and a case study with the grinding optimization process are studied. The experimental results verify the effectiveness of the proposed mechanism.																	0956-5515	1572-8145															10.1007/s10845-020-01565-2		APR 2020											
J								Handwritten word recognition using lottery ticket hypothesis based pruned CNN model: a new benchmark on CMATERdb2.1.2	NEURAL COMPUTING & APPLICATIONS										Handwritten word recognition; CNN model; Lottery ticket hypothesis; Bangla script; CMATERdb2; 1; 2	HOLISTIC APPROACH	Handwritten word recognition, a classical pattern recognition problem, converts a word image into its machine editable form. Mainly two basic approaches are followed to solve this problem, one is segmentation-based and the other is holistic. A number of research attempts have shown that the holistic approach performs better than its counterpart when the lexicon is predefined, fixed and small in size. Relying on this, initial benchmark recognition accuracy on CMATERdb2.1.2, a publicly available database consists of handwritten city names in Bangla, was reported following a holistic word recognition protocol. In the present work, we have followed the same trend to recognize the word samples of the said database and set a new benchmark recognition accuracy. A sparse convolutional neural network (CNN)-based model which is a low-cost trainable model has been developed for this. We have relied on a recently proposed hypothesis, known as lottery ticket hypothesis for pruning the layers of CNN model methodically, and derived a low-resource model having much less number of training parameters. This model competently surpasses the previously reported recognition accuracy on the said database by a significant margin with an axed training cost.																	0941-0643	1433-3058				SEP	2020	32	18			SI		15209	15220		10.1007/s00521-020-04872-0		APR 2020											
J								Optimizing parameters in surface reconstruction of transtibial prosthetic socket using central composite design coupled with fuzzy logic-based model	NEURAL COMPUTING & APPLICATIONS										Surface reconstruction; Contactless laser scanning; Fuzzy logic; Prosthetic socket	LASER-SCANNING SYSTEM; OPTIMIZATION; CONSUMPTION	In this paper, a fuzzy logic-based artificial intelligence technique is delineated to optimize surface reconstruction parameters, initiating from raw point cloud data using contactless scanning of transtibial prosthetic socket. The prosthetic socket having a free-form surface is considered for reconstruction of an accurate and complete digital model, which is further crucial for subsequent design, analysis and production. The input factors considered for the socket surface reconstruction includes number of points, filtering, triangle counts and smoothing level. The experimental trials are performed using face-centered central composite design, and further experimental data are used to establish Mamdani fuzzy logic model to predict the surface standard deviation and required space in computer memory as the response parameters. The confirmation experiment results reveal that the fuzzy model shows good agreement of 96.53% with the experimental measured value. Based on the results, it is proved that established fuzzy model can be used for predicting the surface reconstruction parameters to improve the final accuracy of the transtibial prosthetic socket model. The results concluded that the fuzzy logic provides commendable results; in addition, it successfully predicts the effect of different parameters considered for surface reconstruction on surface accuracy and computer file size.																	0941-0643	1433-3058				OCT	2020	32	19			SI		15597	15613		10.1007/s00521-020-04895-7		APR 2020											
J								Improving collective interpretation by extended potentiality assimilation for multi-layered neural networks	CONNECTION SCIENCE										Neural networks; generalization; autoencoder; potentiality; collective interpretation	MUTUAL INFORMATION; RULES; EXTRACTION	The present paper aims to extend the potential learning method to overcome the problem of collective interpretation, which aims to interpret multi-layered neural networks by compressing them into the simplest ones. In the process of compression, positive, negative, and complicated weights have had unfavourable effects for interpretation. To deal with the problems of collective interpretation, the potential learning is extended only to use positive weights. In addition, to obtain more appropriate weights for interpretation, the number of candidate weights for higher potentialities is first increased as much as possible. Then, from among many candidates, more appropriate weights are selected as more important ones. This extended potentiality learning is expected to produce more stable and more simple representations for easy interpretation. The extended method was applied to three datasets, namely, an artificial dataset, a real eye-tracking dataset, and a student evaluation dataset. In all cases, it was observed that the selectivity of connection weights could be increased. Correspondingly, the majority of connection weights became positive, and the collective weights were quite similar to the regression coefficients of the logistic regression analysis. Finally, for the third dataset (student evaluations), the extended method could extract more explicit input-output relations, compared with the logistic regression analysis, while improving generalisation performance.																	0954-0091	1360-0494				APR 2	2020	32	2					174	203		10.1080/09540091.2019.1674245													
J								Design and Analysis of a Novel Hybrid Processing Robot Mechanism	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Hybrid mechanism; kinematic analysis; large workspace; dexterity; kinematic simulation	PARALLEL KINEMATIC MACHINE; INVERSE KINEMATICS; STIFFNESS ANALYSIS; MANIPULATORS; CALIBRATION	In order to satisfy the requirements of large workspace and high dexterity for processing equipment of oversized cylindrical boxes' spherical crown surfaces in the aerospace industry, a novel serial-parallel hybrid processing robot mechanism is proposed. The degrees of freedom of the 5PUS-(2UR)PU parallel mechanism are obtained by using the screw theory. The inverse kinematics of the hybrid mechanism are analyzed and the velocity Jacobian matrix is established. Then, the constraints of the main factors influencing workspace of the mechanism are given, and the position and posture workspace are obtained. Next, the dexterity and stiffness performance of the mechanism is analyzed based on the Jacobian matrix. The virtual prototype is established, and the theoretical calculation and simulation analysis of the hybrid mechanism with arc curve as the processing trajectory are carried out by using Matlab and Adams software. The research results show that the mechanism can satisfy the requirements of large workspace and high dexterity of oversized cylindrical boxes' spherical crown surface processing, and has feasibility and practical application value.																	1476-8186	1751-8520				JUN	2020	17	3					403	416		10.1007/s11633-020-1228-1		APR 2020											
J								Towards an approach using grammars for automatic classification of masses in mammograms	COMPUTATIONAL INTELLIGENCE										breast cancer diagnosis; grammars; machine learning; mammography; mass classification; syntactic analysis	BREAST MASSES; FEATURES	Approximately 15% of all cancer deaths among women worldwide is due to breast cancer. Mammography is one of the most useful methods for the early detection of this disease. Over the last decade, several papers were published reporting the usage of different computer-aided diagnosis systems using pattern recognition techniques as a second opinion to obtain a more accurate diagnosis. However, the theory of formal languages has not been explored in this field. In this context, the main contribution of this study is to present the usage of a new syntactic approach that is able to classify breast masses found in mammograms as benign or malignant. The experimental tests were performed using a dataset that contains 111 images from different sources. The grammar-based classifiers achieved accuracy values ranging from 89% to 100% depending on the features and the model employed. Furthermore, to achieve a feature dimension reduction, a feature selection technique based on the Gini importance of each feature was employed. Additionally, we compared the obtained results with the grammar-based classifiers to the more traditional classifiers used in this research area, such as artificial neural networks, support vector machines, k-nearest neighbors, and random forest. The best result achieved by the grammar-based classifiers was approximately 10% higher, in terms of accuracy, than the best results produced by the traditional classifiers, showing the strength of this grammatical approach.																	0824-7935	1467-8640															10.1111/coin.12320		APR 2020											
J								ChicWhale optimization algorithm for the VM migration in cloud computing platform	EVOLUTIONARY INTELLIGENCE										Virtual machine migration; Chicken swarm optimization; Cloud computing; Whale optimization algorithm; Resource availability	VIRTUAL MACHINE MIGRATION; DATA CENTERS; PLACEMENT; ENVIRONMENTS; TAXONOMY	Nowadays, Virtual Machine (VM) migration becomes very popular in the cloud computing platform. Various VM migration based mechanisms are designed for optimal VM placement but remain a challenge due to improper energy consumption in the cloud model. This paper proposes an approach for VM migration in the cloud using an optimization algorithm, Chicken-Whale optimization algorithm (ChicWhale), which is developed by integrating the Whale optimization algorithm in Chicken swarm optimization. In the developed approach, a local migration agent is utilized for monitoring the memory and resources utilization in the cloud continuously, and the VM is migrated using the service provider based on the requirement of the VMs to complete a task assigned. At first, the cloud system is designed, and then the proposed ChicWhale is employed by moving the VMs optimally, and the fitness function for best VM migration is carried out by considering several parameters, like load, migration cost, resource availability, and energy. The performance of the VM migration strategy based on ChicWhale is evaluated in terms of energy consumption, resource availability, migration cost, and load. The proposed ChicWhale method achieves the maximal resource availability of 0.989, minimal migration cost of 0.0564, the minimal energy consumption of 0.481, and the minimal load of 0.0001.																	1864-5909	1864-5917				DEC	2020	13	4					725	739		10.1007/s12065-020-00386-9		APR 2020											
J								Distributed deduplication with fingerprint index management model for big data storage in the cloud	EVOLUTIONARY INTELLIGENCE										Cloud storage; Data center; Deduplication; Fingerprint management; Relative deduplication ratio		As data progressively grows within data centers, the cloud storage models face several issues while storing data and offers abilities needed to shift data in an adequate time frame. This study aims to develop a distributed deduplication model to achieve scalable throughput and capacity utilizing many data servers for duplicating data in parallel with minimal loss. This paper proposes a new cloud storage model based on a distributed deduplication with the fingerprint index management (DDFI) model. The DDFI model operates on three main stages. At the initial stage, the DDFI model makes use of an effective routing technique depending upon the similarity level of the data, which leads to low network overhead by rapid identification of storage locations. In the second stage, the duplicate data identification procedure is carried out by the use of the MD5 algorithm. At the final stage, a fingerprint index management process is executed where a fingerprint index comprises fingerprints and its corresponding position details of every written chunk. For optimizing the results of the deduplication performance, the DDFI model manages the fingerprint index in storage space and only sometimes writes to disk at the same time as the cloud database scheme is idle. The simulation outcome exhibited that the presented DDFI model offered maximum results with a higher deduplication ratio (DR) with a minimum overhead of network bandwidth. From the detailed comparative analysis, it is inferred that the presented DFFI model offered maximum relative DR, maximum duplication performance, minimum read bandwidth, and write bandwidth.																	1864-5909	1864-5917															10.1007/s12065-020-00395-8		APR 2020											
J								Neutrosophic structured element	EXPERT SYSTEMS										aggregation operator; multi-attribute decision making; neutrosophic set; operational laws; score function measure; single valued neutrosophic sets	GROUP DECISION-MAKING; SIMILARITY MEASURES; CORRELATION-COEFFICIENT; AGGREGATION OPERATORS; IMAGE SEGMENTATION; TOPSIS METHOD; SETS; MODEL; PREDICTION; ENTROPY	This paper presents a new concept in neutrosophic sets (NS) called neutrosophic structured element (NSE). Based on this concept, we define the operational laws, score function, and some aggregation operators of NS. Finally, as an application of this concept, we propose a decision-making method for a multi-attribute decision making (MADM) problem under NSE information. The results indicate that this concept is a useful tool for dealing with neutrosophic decision problems.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12542	10.1111/exsy.12542		APR 2020											
J								Non functional requirement based software architecture scheme with security requirement using hybrid group search optimization and genetic algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Non functional requirements; Dependability; Reliability; Maintainability; Fuzzy k means clustering; Group search algorithm and genetic algorithm		Managing non functional requirements has been a challenge onto software engineers for a long time. Throughout the years, numerous strategies and methods have been proposed to enhance their elicitation, documentation and approval. Similarly, security requirements emphatically impact the building configuration of complex IT frameworks comparatively as non-functional requirements. Both security building and software engineering furnish strategies to manage such necessities. Nonetheless, there is still a basic crevice concerning the joining of techniques for these two different fields. In this paper we close this crevice as for security prerequisites by proposing a technique that joins software engineering technologies with the best in class security engineering standards. An efficient quality requirement based software model with security requirements is developed by fusing Group Search Optimization and genetic algorithm in our proposed methodology.																	1868-5137	1868-5145															10.1007/s12652-020-01904-4		APR 2020											
J								An effective disease prediction system using incremental feature selection and temporal convolutional neural network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Convolutional neural network; Conditional random field; Correlation coefficient; Linear correlation coefficient; Feature selection; Incremental feature selection; Classification and disease prediction system		Rapid growth of communication technologies and expert systems produces enormous volume of medical data. Deep learning technique is an advancement of machine learning technique for analysing the huge amount of various diseases related medical dataset. Even though, no healthcare systems are achieved better prediction accuracy with the various medical datasets in the past decades. For improving the accuracy level of disease prediction, we develop a disease prediction system to predict the serious death diseases including heart disease, diabetic disease and cancer diseases effectively in this paper. This disease prediction system consists of feature selection method that works as incremental in nature named as Incremental Feature Selection Algorithm (IFSA) which combines the concepts of Intelligent Conditional Random Field (CRF) on feature selection process and the Linear Correlation Coefficient based Feature Selection (ICRF-LCFS) method algorithm and an existing Convolutional Neural Network (CNN) with temporal features (T-CNN). The proposed disease prediction system is evaluated and achieved better prediction accuracy in less time with low false alarm rate.																	1868-5137	1868-5145															10.1007/s12652-020-01910-6		APR 2020											
J								Image demosaicking based on moving least square	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Moving least square; Demosaicking; Color channel; Real time process	FILTER; INTERPOLATION	We present a demosaicking method based on moving least square to estimate to-be-interpolated color pixels. Demosaicking is a severely ill-posed problem and we need to reconstruct two color channels for each pixel. To predict the missing color pixels using the given color pixels, we rely on a generic local approximation function using moving least square. We assume that image details have similar geometric structure in a local region, and we utilize moving least square to represent this geometric feature. Experimental results indicate that the proposed method outperforms existing approaches in both objective and subjective performance.																	1868-5137	1868-5145															10.1007/s12652-020-01843-0		APR 2020											
J								Learning from positive and unlabeled data: a survey	MACHINE LEARNING										Classification; Weakly supervised learning; PU learning	CLASS CLASSIFICATION; TEXT CATEGORIZATION	Learning from positive and unlabeled data or PU learning is the setting where a learner only has access to positive examples and unlabeled data. The assumption is that the unlabeled data can contain both positive and negative examples. This setting has attracted increasing interest within the machine learning literature as this type of data naturally arises in applications such as medical diagnosis and knowledge base completion. This article provides a survey of the current state of the art in PU learning. It proposes seven key research questions that commonly arise in this field and provides a broad overview of how the field has tried to address them.																	0885-6125	1573-0565				APR	2020	109	4					719	760		10.1007/s10994-020-05877-5		APR 2020											
J								Dynamic variational inequality in fuzzy environments	FUZZY OPTIMIZATION AND DECISION MAKING										Fuzzy variational inequality; Differential variational inequality; Fuzzy set; Dynamic system; Variational inequality	EXISTENCE	This paper introduces a class of differential fuzzy variational inequalities, the model provides an efficient approach for solving many dynamic multi-objective optimization problems in fuzzy environments. An existence theorem of the Caratheodory weak solution for the model is established under some suitable assumptions. An algorithm, supported by the convergence analysis, is developed to find the solution. In addition, some examples are given to illustrate the application and the algorithm.																	1568-4539	1573-2908				SEP	2020	19	3					275	296		10.1007/s10700-020-09322-z		APR 2020											
J								A fuzzy evaluation approach with the quasi-ordered set: evaluating the efficiency of decision making units	FUZZY OPTIMIZATION AND DECISION MAKING										Constant returns to scale; Fuzzy evaluation approach; Inequality; Decision making units (DMUs); Data envelopment analysis (DEA); Quasi-ordered set	DATA ENVELOPMENT ANALYSIS; DEA; MODEL	This work proposes an inequality approach with the quasi-ordered set to evaluate the performances of decision making units (DMUs). In real world applications, input and output data are often imprecise and fluctuated. In this case, a fuzzy inequality approach is proposed to evaluate DMUs with fuzzy data. Fuzzy inequalities consist of fuzzy expressions of the production possibility set and the line segment joining the origin to the evaluated DMU. Moreover, under constant returns to scale, the production possibility set is spanned by all the DMUs without the evaluated DMU. Fuzzy efficiency is dependent upon whether the solution set of fuzzy inequalities is empty or not. The quasi-ordered set is used to distinguish the fuzzy efficiency. Finally, numerical examples are used to illustrate the approach.																	1568-4539	1573-2908				SEP	2020	19	3					297	310		10.1007/s10700-020-09321-0		APR 2020											
J								Adversarial genetic programming for cyber security: a rising application domain where GP matters	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Genetic programming; Coevolutionary algorithms; Cyber Security	COMPETITIVE COEVOLUTION; BOTNET DETECTION; EVOLUTIONARY; BEHAVIOR; GENERATION; SEARCH	Cyber security adversaries and engagements are ubiquitous and ceaseless. We delineate Adversarial Genetic Programming for Cyber Security, a research topic that, by means of genetic programming (GP), replicates and studies the behavior of cyber adversaries and the dynamics of their engagements. Adversarial Genetic Programming for Cyber Security encompasses extant and immediate research efforts in a vital problem domain, arguably occupying a position at the frontier where GP matters. Additionally, it prompts research questions around evolving complex behavior by expressing different abstractions with GP and opportunities to reconnect to the machine learning, artificial life, agent-based modeling and cyber security communities. We present a framework called RIVALS which supports the study of network security arms races. Its goal is to elucidate the dynamics of cyber networks under attack by computationally modeling and simulating them.																	1389-2576	1573-7632															10.1007/s10710-020-09389-y		APR 2020											
J								Adaptive self-learning mechanisms for updating short-term production decisions in an industrial mining complex	JOURNAL OF INTELLIGENT MANUFACTURING										Mining complex; Production planning; Artificial intelligence; Reinforcement learning; Sensor information; Ensemble Kalman filter; Real-time; Destination policies; Deep learning	DATA ASSIMILATION; OPTIMIZATION; MODEL; SIMULATION; RECONCILIATION; CLASSIFICATION; UNCERTAINTY; INFORMATION; ORE	A mining complex is an integrated value chain where the materials extracted from a group of mineral deposits are sent to different processing streams to produce sellable products. A major short-term decision in a mining complex is to determine the flow of materials that first includes deciding which handling facilities to send the extracted materials and then determining how to utilize the processing facilities. The flow of materials through the mining complex is significantly dependent on the performance of and interaction between its different components. New digital technologies, including the development of advanced sensors and monitoring devices, have enabled a mining complex to acquire new information about the performance of its different components. This paper proposes a new continuous updating framework that combines policy gradient reinforcement learning and an extended ensemble Kalman filter to adapt the short-term flow of materials in a mining complex with incoming information. The framework first uses a new extended ensemble Kalman filter to update the uncertainty models of the different components of a mining complex with new incoming information. Then, the updated uncertainty models are fed to a neural network trained using a policy gradient reinforcement learning algorithm to adapt the short-term flow of materials in a mining complex. The proposed framework is applied to a copper mining complex and shows its ability to efficiently adapt the short-term flow of materials in an operational mining environment with new incoming information. The framework better meets the different production targets while improving the cumulative cash flow compared to industry standard approaches.																	0956-5515	1572-8145				OCT	2020	31	7			SI		1795	1811		10.1007/s10845-020-01562-5		APR 2020											
J								A deep learning-based hybrid model for recommendation generation and ranking	NEURAL COMPUTING & APPLICATIONS										Deep learning; Optimization; Side information; Hybrid model; Recommendation system; Collaborative filtering	NETWORK	A recommender system plays a vital role in information filtering and retrieval, and its application is omnipresent in many domains. There are some drawbacks such as the cold-start and the data sparsity problems which affect the performance of the recommender model. Various studies help with drastically improving the performance of recommender systems via unique methods, such as the traditional way of performing matrix factorization (MF) and also applying deep learning (DL) techniques in recent years. By using DL in the recommender system, we can overcome the difficulties of collaborative filtering. DL now focuses mainly on modeling content descriptions, but those models ignore the main factor of user-item interaction. In the proposed hybrid Bayesian stacked auto-denoising encoder (HBSADE) model, it recognizes the latent interests of the user and analyzes contextual reviews that are performed through the MF method. The objective of the model is to identify the user's point of interest, recommending products/services based on the user's latent interests. The proposed two-stage novel hybrid deep learning-based collaborative filtering method explores the user's point of interest, captures the communications between items and users and provides better recommendations in a personalized way. We used a multilayer neural network to manipulate the nonlinearities between the user and item communication from data. Experiments were to prove that our HBSADE outperforms existing methodologies over Amazon-b and Book-Crossing datasets.																	0941-0643	1433-3058															10.1007/s00521-020-04844-4		APR 2020											
J								VR Design of Public Facilities in Historical Blocks Based on BP Neural Network	NEURAL PROCESSING LETTERS										BP neural network; VR; Historic districts; Public facilities	OPTIMIZATION; INTERNET	In view of China's economy achieves rapid development, technology and science change with each passing day and gradually integrate into our life. Accustomed to the intelligent age, higher requirements caused by the design of public facilities is proposed. The public facilities of the historic block are not only the effective guarantee of people's life, but also the embodiment of the urban culture. Therefore, it is necessary to take the initiative to respond to the rapid change of the demand brought by the urban change and create a pleasant cultural space. However, the historical block is large in scale and rich in details, and the design of its public facilities requires not only a broad range of material resources, but also a plethora of manpower. In order to further decrease the design cost and significantly improve the design level, the virtual system of 3D historical block based on VR is designed, and the public facilities are designed in the virtual system. Furthermore, the design and evaluation system of public facilities based on BP neural network is designed, which simplifies the process of design evaluation and program modification through real-time design evaluation and greatly improves the design efficiency of public facilities in historical blocks.																	1370-4621	1573-773X															10.1007/s11063-020-10207-w		APR 2020											
J								Representation of De Morgan and (Semi-)Kleene Lattices	SOFT COMPUTING											DUALITY	Twist-structure representation theorems are established for De Morgan and Kleene lattices. While the former result relies essentially on the quasivariety of De Morgan lattices being finitely generated, the representation for Kleene lattices does not and can be extended to more general algebras. In particular, one can drop the double negation identity (involutivity). The resulting class of algebras, named semi-Kleene lattices by analogy with Sankappanavar's semi-De Morgan lattices, is shown to be representable through a twist-structure construction inspired by the Cornish-Fowler duality for Kleene lattices. Quasi-Kleene lattices, a subvariety of semi-Kleene, are also defined and investigated, showing that they are precisely the implication-free subreducts of the recently introduced class of quasi-Nelson lattices.																	1432-7643	1433-7479				JUN	2020	24	12			SI		8685	8716		10.1007/s00500-020-04885-w		APR 2020											
J								Kernel intuitionistic fuzzy c-means and state transition algorithm for clustering problem	SOFT COMPUTING										Clustering problem; State transition algorithm; Kernel function; Intuitionistic fuzzy set; Fuzzy c-means	K-MEANS; OPTIMIZATION	Clustering problems widely exist in machine learning, pattern recognition, image analysis and information sciences, etc. Although many clustering algorithms have been proposed, it is unpractical to find a clustering algorithm suitable for all types of datasets. Fuzzy c-means (FCM) is one of the most frequently-used fuzzy clustering algorithm for the reason that it is efficient, straightforward, and easy to implement. However, the traditional FCM taking Euclidean distance as similarity measurement can not distinguish the intersection between two clusters. Therefore, kernel function has been taken as similarity measurement to solve this issue. As a comprehensive partition criterion, intuitionistic fuzzy set which consider both membership degree and non-membership degree has been used to replace traditional fuzzy set to describe the natural attributes of objective phenomena more delicately. Thus, Kernel intuitionistic fuzzy c-means (KIFCM) has been proposed in this paper to settle clustering problem. Considering FCM is easily getting trapped in local optima due to its high sensitivity to initial centroid. State Transition Algorithm (STA) has been adopted in this study to obtain the initial centroid to enhance its stability. The proposed STA-KIFCM compared with some other clustering algorithms are implemented using five benchmark datasets. Experimental results not only show that the proposed method is efficient and can reveal encouraging results, but also indicate that the proposed method can achieve high accuracy.																	1432-7643	1433-7479				OCT	2020	24	20					15507	15518		10.1007/s00500-020-04879-8		APR 2020											
J								Predictive analysis of electronic waste for reverse logistics operations: a comparison of improved univariate grey models	SOFT COMPUTING										Electronic waste; Fractional order; Improved grey modeling; Particle Swarm Optimization; Reverse logistics; Rolling mechanism	BERNOULLI MODEL; FORECASTING-MODEL; FLOW-ANALYSIS; OPTIMIZATION; GENERATION; INDEXES; DEMAND; SYSTEM; OUTPUT	Growing rates of innovation and consumer demand resulted in rapid accumulation of waste of electrical and electronic equipment or electronic waste (e-waste). In order to build and sustain green cities, efficient management of e-waste rises as a viable response to this accumulation. Accurate e-waste predictions that municipalities can utilize to build appropriate reverse logistics infrastructures gain significance as collecting, recycling and disposing the e-waste become more complex and unpredictable. In line with its significance, the related literature presents several methodologies focusing on e-waste generation forecasting. Among these methodologies, grey modeling approach has aroused interest due to its ability to present meaningful results with small-sized or limited data. In order to improve the overall success rate of the approach, several grey modeling-based forecasting techniques have been proposed throughout the past years. The performance of these models, however, profoundly leans on the parameters used with no established consensus regarding the suitable criteria for better accuracy. To address this issue and to provide a guideline for academicians and practitioners, this paper presents a comparative analysis of most utilized grey modeling methods in the literature improved by particle swarm optimization. A case study employing e-waste data from Washington State is provided to demonstrate the comparative analysis proposed in the study.																	1432-7643	1433-7479				OCT	2020	24	20					15747	15762		10.1007/s00500-020-04904-w		APR 2020											
J								A novel plausible reasoning based on intuitionistic fuzzy propositional logic and its application in decision making	FUZZY OPTIMIZATION AND DECISION MAKING										Intuitionistic fuzzy propositional logic; Plausible reasoning; Truth table of IFLPs; Figure of equivalence of IFLPs; Reasoning theory of IFLPs	LINGUISTIC TERM SETS	Automatic reasoning based on propositional logic is considered as an important tool in machine learning, and intuitionistic fuzzy sets have turned out to deal with vague and uncertain information effectively in real world. In this paper, a novel plausible reasoning based on intuitionistic fuzzy propositional logic is proposed. On the basis of it, the categories of intuitionistic fuzzy logic proposition (IFLP) formula are discussed both considering the true degree and the false degree at the same time. Some basic operational laws and inference rules of IFLPs on the basis of closely-reasoned scientific proofs are put out. Then, we develop two classification methods of IFLPs, i.e., truth table and figure of equivalence, respectively. After that, the reasoning theory of IFLPs is introduced and three reasoning methods are further established including direct proof method, additional premise proof method and reduction to absurdity method, respectively. Finally, a case study about strategy initiatives of HBIS GROUP on Supply-side Structural Reform is presented and some discussions are provided to validate the proposed methods. As a result, the proposed methods based on the plausible reasoning offer sound structure and can improve the efficiency of logic programming.																	1568-4539	1573-2908				SEP	2020	19	3					251	274		10.1007/s10700-020-09319-8		APR 2020											
J								Regularization by Architecture: A Deep Prior Approach for Inverse Problems	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Inverse problems; Deep learning; Regularization by architecture; Deep inverse prior; Deep image prior	THRESHOLDING ALGORITHM; SPARSE	The present paper studies so-called deep image prior (DIP) techniques in the context of ill-posed inverse problems. DIP networks have been recently introduced for applications in image processing; also first experimental results for applying DIP to inverse problems have been reported. This paper aims at discussing different interpretations of DIP and to obtain analytic results for specific network designs and linear operators. The main contribution is to introduce the idea of viewing these approaches as the optimization of Tikhonov functionals rather than optimizing networks. Besides theoretical results, we present numerical verifications.																	0924-9907	1573-7683				APR	2020	62	3			SI		456	470		10.1007/s10851-019-00923-x													
J								Combining automated microfluidic experimentation with machine learning for efficient polymerization design	NATURE MACHINE INTELLIGENCE											CATALYZED PROPENE POLYMERIZATION; ZIEGLER-NATTA POLYMERIZATION; OLEFIN-POLYMERIZATION; ZIRCONOCENE CATALYSTS; METALLOCENE CATALYSTS; KINETICS; MECHANISM	Understanding polymerization reactions has challenges relating to the complexity of the systems, the hazards associated with the reagents, the environmental footprint of the operations and the highly nonlinear topologies of reaction spaces. In this work, we aim to present a new methodology for studying polymerization reactions using machine-learning-assisted automated microchemical reactors. A custom-designed rapidly prototyped microreactor is used in conjunction with automation and in situ infrared thermography for efficient, high-speed experimentation to map the reaction space of a zirconocene polymerization catalyst and obtain fundamental kinetic parameters. Chemical waste is decreased by two orders of magnitude and catalytic discovery is reduced from weeks to hours. Bayesian regularization backpropagation is used in conjunction with kinetic modelling to understand the reaction space and resultant technoeconomic topology. Here, we show that efficient microfluidic technology can be coupled with machine-learning algorithms to obtain high-fidelity datasets on a complex chemical reaction. Finding the best ratio of ingredients for polymerization reactions can be time consuming and wasteful. An automated microreactor process with integrated machine learning analysis initiates reactions, measures the resulting yield and cleans itself without human intervention. It can test concentrations of reagents systematically to find the combination with the highest production, while producing a low amount of waste.																		2522-5839				APR	2020	2	4					200	209		10.1038/s42256-020-0166-5													
J								A neural network trained for prediction mimics diverse features of biological neurons and perception	NATURE MACHINE INTELLIGENCE											MODELS; ARCHITECTURE; CODES; FIELD	Recent work has shown that convolutional neural networks (CNNs) trained on image recognition tasks can serve as valuable models for predicting neural responses in primate visual cortex. However, these models typically require biologically infeasible levels of labelled training data, so this similarity must at least arise via different paths. In addition, most popular CNNs are solely feedforward, lacking a notion of time and recurrence, whereas neurons in visual cortex produce complex time-varying responses, even to static inputs. Towards addressing these inconsistencies with biology, here we study the emergent properties of a recurrent generative network that is trained to predict future video frames in a self-supervised manner. Remarkably, the resulting model is able to capture a wide variety of seemingly disparate phenomena observed in visual cortex, ranging from single-unit response dynamics to complex perceptual motion illusions, even when subjected to highly impoverished stimuli. These results suggest potentially deep connections between recurrent predictive neural network models and computations in the brain, providing new leads that can enrich both fields. The deep convolutional recurrent neural network 'PredNet' can be trained to predict future video frames in a self-supervised manner. A surprising result is that it captures a wide array of phenomena observed in natural neuronal systems, ranging from low-level visual cortical neuron response properties to high-level perceptual illusions, hinting at potential similarities between recurrent predictive neural network models and computations in the brain.																		2522-5839				APR	2020	2	4					210	+		10.1038/s42256-020-0170-9													
J								Exploring the limit of using a deep neural network on pileup data for germline variant calling	NATURE MACHINE INTELLIGENCE												Single-molecule sequencing technologies have emerged in recent years and revolutionized structural variant calling, complex genome assembly and epigenetic mark detection. However, the lack of a highly accurate small variant caller has limited these technologies from being more widely used. Here, we present Clair, the successor to Clairvoyante, a program for fast and accurate germline small variant calling, using single-molecule sequencing data. For Oxford Nanopore Technology data, Clair achieves better precision, recall and speed than several competing programs, including Clairvoyante, Longshot and Medaka. Through studying the missed variants and benchmarking intentionally overfitted models, we found that Clair may be approaching the limit of possible accuracy for germline small variant calling using pileup data and deep neural networks. Clair requires only a conventional central processing unit (CPU) for variant calling and is an open-source project available at https://github.com/HKU-BAL/Clair. A lack of accurate and efficient variant calling methods has held back single-molecule sequencing technologies from clinical applications. The authors present a deep-learning method for fast and accurate germline small variant calling, using single-molecule sequencing data.																		2522-5839				APR	2020	2	4					220	227		10.1038/s42256-020-0167-4													
J								Deep variational network for rapid 4D flow MRI reconstruction	NATURE MACHINE INTELLIGENCE											PHASE ERROR-CORRECTION; CONTRAST MR; TIME; SENSE	Phase-contrast magnetic resonance imaging (MRI) provides time-resolved quantification of blood flow dynamics that can aid clinical diagnosis. Long in vivo scan times due to repeated three-dimensional (3D) volume sampling over cardiac phases and breathing cycles necessitate accelerated imaging techniques that leverage data correlations. Standard compressed sensing reconstruction methods require tuning of hyperparameters and are computationally expensive, which diminishes the potential reduction of examination times. We propose an efficient model-based deep neural reconstruction network and evaluate its performance on clinical aortic flow data. The network is shown to reconstruct undersampled 4D flow MRI data in under a minute on standard consumer hardware. Remarkably, the relatively low amounts of tunable parameters allowed the network to be trained on images from 11 reference scans while generalizing well to retrospective and prospective undersampled data for various acceleration factors and anatomies. 4D MRI scans can reconstruct cardiovascular flow, although they typically take many minutes, hindering real-time assessment. Vishnevskiy et al. develop a deep variational network to permit high-fidelity image reconstruction in a matter of seconds, allowing integration of 4D flow MRI into clinical workflows.																		2522-5839				APR	2020	2	4					228	+		10.1038/s42256-020-0165-6													
J								Robust Formation Control for Multiple Quadrotors With Nonlinearities and Disturbances	IEEE TRANSACTIONS ON CYBERNETICS										Vehicle dynamics; Nonlinear dynamical systems; Multi-agent systems; Stability analysis; Dynamics; Attitude control; Control systems; Formation control; multiagent system; nonlinear system; quadrotors; robust control	TIME FORMATION CONTROL; MULTIAGENT NETWORKS; SYNCHRONIZATION; VEHICLES; FEEDBACK; CONSENSUS; UAVS	In this paper, the robust formation control problem is investigated for a group of quadrotors. Each quadrotor dynamics exhibits the features of underactuation, high nonlinearities and couplings, and disturbances in both the translational and rotational motions. A distributed robust controller is developed, which consists of a position controller to govern the translational motion for the desired formation and an attitude controller to control the rotational motion of each quadrotor. Theoretical analysis and simulation studies of a formation of multiple uncertain quadrotors are presented to validate the effectiveness of the proposed formation control scheme.																	2168-2267	2168-2275				APR	2020	50	4					1362	1371		10.1109/TCYB.2018.2875559													
J								Ultrasonic Machining Process Optimization by Cuckoo Search and Chicken Swarm Optimization Algorithms	INTERNATIONAL JOURNAL OF APPLIED METAHEURISTIC COMPUTING										Chicken Swami Algorithm; Cuckoo Search Algorithm; Metaheuristic Algorithm; Optimization; Ultrasonic Machining	PARAMETERS	The ultrasonic machining (USM) process has been analyzed in the present study to obtain the desired process responses by optimizing machining parameters using cuckoo search (CS) and chicken swarm optimization (CSO), two powerful nature-inspired, population and swarm-intelligence-based metaheuristic algorithms. The CS and CSO algorithms have been compared with other non-conventional optimization techniques in terms of optimal results, convergence, accuracy, and computational time. It is found that CS and CSO algorithms predict superior single and multi-objective optimization results than gravitational search algorithms (GSAs), genetic algorithms (GAs), particle swarm optimization (PSO) algorithms, ant colony optimization (ACO) algorithms and artificial bee colony (ABC) algorithms, and gives exactly the same results as predicted by the fireworks algorithm (FWA). The CS algorithm outperforms all other algorithms namely CSO, FWA, GSA, GA, PSO, ACO, and ABC algorithms in terms of mean computational time, whereas, the CSO algorithm outperforms all other algorithms except for the CS and GSA algorithms.																	1947-8283	1947-8291				APR-JUN	2020	11	2					1	26		10.4018/IJAMC.2020040101													
J								Fast mode decision algorithm for HEVC intra coding based on texture partition and direction	JOURNAL OF REAL-TIME IMAGE PROCESSING										High efficiency video coding; Intra coding; Mode decision; Texture partition and direction	CU SIZE DECISION; EFFICIENCY; SELECTION; SPLIT; SKIP	High efficiency video coding (HEVC) is the newest video coding standard, which employs some advanced coding techniques as compared to the previous standard H.264. The flexible quad-tree partitioning of coding tree unit (CTU) and various candidate modes of prediction unit (PU) significantly promote the video compression efficiency; however, these techniques lead to a great amount of computational loads. In this paper, a fast mode decision algorithm for HEVC intra coding is proposed based on texture partition and direction. It consists of two sub-algorithms: the CTU depth range prediction (CDRP) and the intra-prediction mode selection (IPMS). The CDRP reduces the recursive partition number of coding unit (CU) based on the correlation between the CTU texture partition and the optimum CU partition, and it first calculates the texture partition flags of different-size CUs from bottom to top. Then, it employs these partition flags to predict the depth range of the current CTU and decide whether to terminate the CU partition in advance. In order to reduce the number of candidate PU modes for the Hadamard optimization, the IPMS first uses the three-step selection of the candidate modes. The first step selects the candidate modes based on the correlation between the texture directions and the optimum PU modes. The second step selects the candidate modes by using the best modes among the selected modes in the first step. The third step selects the candidate modes by using the spatial correlation of the optimum modes between the current PU and its adjacent PUs. Then, in order to reduce the number of candidate modes for the rate-distortion optimization, the IPMS utilizes the numerical relationship of the sorted Hadamard costs of above selected modes, the optimum modes of adjacent PUs and the statistical characteristics of the small-size PUs. Compared to the original algorithm in HEVC test model, the proposed overall algorithm can reduce 60% encoding time on average with only a 1.45% increase in Bjontegaard delta bit rate under the all-intra configuration. Compared to the most of state-of-the-art algorithms, the proposed overall algorithm has better computational performances and similar rate-distortion performances.																	1861-8200	1861-8219				APR	2020	17	2					275	292		10.1007/s11554-018-0766-z													
J								Efficient Weighted Semantic Score Based on the Huffman Coding Algorithm and Knowledge Bases for Word Sequences Embedding	INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS										DBpedia; Huffman Algorithm; Knowledge Bases; Semantic Representation; Word Embedding		Learning text representation is forming a core for numerous natural language processing applications. Word embedding is a type of text representation that allows words with similar meaning to have similar representation. Word embedding techniques categorize semantic similarities between linguistic items based on their distributional properties in large samples of text data. Although these techniques are very efficient, handling semantic and pragmatics ambiguity with high accuracy is still a challenging research task. In this article, we propose a new feature as a semantic score which handles ambiguities between words. We use external knowledge bases and the Huffman Coding algorithm to compute this score that depicts the semantic relatedness between all fragments composing a given text. We combine this feature with word embedding methods to improve text representation. We evaluate our method on a hashtag recommendation system in Twitter where text is noisy and short. The experimental results demonstrate that, compared with state-of-the-art algorithms, our method achieves good results.																	1552-6283	1552-6291				APR-JUN	2020	16	2					126	142		10.4018/IJSWIS.2020040107													
J								Distant Supervised Relation Extraction via DiSAN-2CNN on a Feature Level	INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS										Relation Extraction; Attention Model; Disan; Distant/Weakly Supervised		At present, the mainstream distant supervised relation extraction methods existed problems: the coarse granularity for coding the context feature information; the difficulty in capturing the long-term dependency in the sentence, and the difficulty in coding prior knowledge of structures are major issues. To address these problems, we propose a distant supervised relation extraction model via DiSAN-2CNN on feature level, in which multi-dimension self-attention mechanism is utilized to encode the features of the words and DiSAN-2CNN is used to encode the sentence to obtain the long-term dependency, the prior knowledge of the structure, the time sequence, and the entity dependence in the sentence. Experiments conducted on the NYT-Freebase benchmark dataset demonstrate that the proposed DiSAN-2CNN on a feature level model achieves better performance than the current two state-of-art distant supervised relation extraction models PCNN+ATT and ResCNN-9, and it has d generalization ability with the least artificial feature engineering.																	1552-6283	1552-6291				APR-JUN	2020	16	2					1	17		10.4018/IJSWIS.2020040101													
J								Two Ways for the Automatic Generation of Application Ontologies by Using BalkaNet	INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS										Application Ontology; BalkaNet; Food Ontology; Ontology; Oxymoron; Rhetorical Figures; Tropes; Wordnet	WORDNET	This article presents two methods for the automatic generation of application ontologies from the multilingual BalkaNet WordNets Web ontology language (OWL) representation. Both proposed methods are applied on the BalkaNet WordNets ontology for the Serbian language (SerWN). The first one uses only the SerWN, both for generating class hierarchy and instances of classes, while the other method combines the SerWN with a domain ontology. The first method was used to automatically generate the FoodOntology, whereas the second method to generate the ontology of rhetorical figures tropes. Preliminary evaluation results corroborate the soundness of the approach. Since BN consists of individual WNs for five Balkan languages and Czech, the methodology presented in this article can also be used for all these languages. The first method can also be used for other domains.																	1552-6283	1552-6291				APR-JUN	2020	16	2					18	41		10.4018/IJSWIS.2020040102													
J								Web Mining Customer Perceptions to Define Product Positions and Design Preferences	INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS										Aspect-Based Sentiment Analysis; Cluster Analysis; Market Positioning; Opinion Mining; Perceptual Maps	SENTIMENT ANALYSIS; REVIEWS	E-commerce provides a global platform supporting product transactions through the consumer purchase lifecycle including communications of perceived satisfaction and dissatisfaction. The customer feedback functions and social networks of many e-commerce websites allow for the creation of extremely large databases that can be mined to model the customers' perceptions toward online purchases. This research uses online customer reviews as the business intelligence corpus to help companies redesign products that better satisfy consumer preferences and differentiate their product offerings. After identifying the specific webpages of customer reviews, a web crawler collects review text. Computer-supported text mining, cluster analysis, and perceptual mapping are combined as a systematic analytic approach to compare products in a given domain. The study assists phone manufacturers to understand the positive and negative perceptions of customers related to their post-purchase experiences. The customer-preferred product functions, features, and price positions provide valuable strategic intelligence for new product designs and market differentiation.																	1552-6283	1552-6291				APR-JUN	2020	16	2					42	58		10.4018/IJSWIS.2020040103													
J								Combining Linked Open Data Similarity and Relatedness for Cross OSN Recommendation	INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS										Cold-Start; Linked Open Data; Online Social Network; Recommender System; Similarity measure	PERSONALIZATION	The emergence of online social networks (OSNs) and linked open data (LOD) bring up opportunities to experiment on a new generation of cross-domain recommender systems in which the true benefit of LOD can be exploited, particularly to address the new user problems. In this article, the authors explore the feasibility of combining the two axes of comparison, similarity and relatedness, in LOD space, and introduce a new LOD-based similarity measure. The reason is to take benefit more from LOD to compare general resources, which can be useful in the context of cross-OSN recommendation. Experimental evaluation demonstrates the effectiveness of the proposed approach.																	1552-6283	1552-6291				APR-JUN	2020	16	2					59	90		10.4018/IJSWIS.2020040104													
J								Context-Aware Personalized Web Search Using Navigation History	INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS										Context History; Context-Aware System; Personalization; User Profile; Web Search	INFORMATION-RETRIEVAL	It is highly desirable that web search engines know users well and provide just what the user needs. Although great effort has been devoted to achieve this dream, the commonly used web search engines still provide a "one-fit-all" results. One of the barriers is lack of an accurate representation of user search context that supports personalised web search. This article presents a method to represent user search context and incorporate this representation to produce personalised web search results based on Google search results. The key contributions are twofold: a method to build contextual user profiles using their browsing behaviour and the semantic knowledge represented in a domain ontology; and an algorithm to re-rank the original search results using these contextual user profiles. The effectiveness of proposed new techniques were evaluated through comparisons of cases with and without these techniques respectively and a promising result of 35% precision improvement is achieved.																	1552-6283	1552-6291				APR-JUN	2020	16	2					91	107		10.4018/IJSWIS.2020040105													
J								Multi-Intentional Description of Learning Semantic Web Services	INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS										E-Learning; Learning Service; Multi-Intention Service; Ontology; OWL-S; OWLS-LO; Semantic Matching; Semantic Web Services; Service Discovery; Web Services		E-learning systems use web service technology to develop distributed applications. Therefore, with the tremendous growth in the number of web services, finding the proper services while ensuring the independence and reusability of the learning objects in a different context has become an important issue and has attracted much interest. This article first proposes an extension of the Ontology Web Language for Services Learning Object (OWLS-LO) model to describe a multi-intentional learning object. This description ensures accessibility to learning objects. This research then presents a service discovery mechanism that uses the new semantic model for service matching. Experimental results show that the proposed semantic discovery mechanism using multi-intention model performs better than discovery mechanism based on single intention.																	1552-6283	1552-6291				APR-JUN	2020	16	2					108	125		10.4018/IJSWIS.2020040106													
J								Moving Vehicle Detection and Tracking Based on Video Sequences	TRAITEMENT DU SIGNAL										video sequence; vehicle tracking algorithm; vehicle detection algorithm; intelligent transportation		Vehicle detection and tracking are key aspects of intelligent transportation. However, the accuracy of traditional methods for vehicle detection and tracking is severely suppressed by the complexity of road conditions, occlusion, and illumination changes. To solve the problem, this paper initializes the background model based on multiple frames with a fixed interval, and then establishes a moving vehicle detection algorithm based on trust interval. The established algorithm can easily evaluate the background complexity based on regional information. After that, the authors pointed out that the correlation filtering, a classical vehicle tracking algorithm, cannot adapt to the scale changes of vehicles, due to the weak dependence of background information. Hence, a tracking algorithm that adapts to vehicle scale was designed based on background information. Finally, the proposed algorithms were proved feasible for detection and tracking moving vehicles in complex environments. The research provides a good reference for the application of computer vision in moving target detection.																	0765-0019	1958-5608				APR	2020	37	2					325	331		10.18280/ts.370219													
J								Idiom-Based Features in Sentiment Analysis: Cutting the Gordian Knot	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Sentiment analysis; Feature extraction; Semantics; Thesauri; Crowdsourcing; Pattern recognition; Sentiment analysis; natural language processing; text mining; knowledge engineering; feature extraction	AGREEMENT	In this paper we describe an automated approach to enriching sentiment analysis with idiom-based features. Specifically, we automated the development of the supporting lexico-semantic resources, which include (1) a set of rules used to identify idioms in text and (2) their sentiment polarity classifications. Our method demonstrates how idiom dictionaries, which are readily available general pedagogical resources, can be adapted into purpose-specific computational resources automatically. These resources were then used to replace the manually engineered counterparts in an existing system, which originally outperformed the baseline sentiment analysis approaches by 17 percentage points on average, taking the F-measure from 40s into 60s. The new fully automated approach outperformed the baselines by 8 percentage points on average taking the F-measure from 40s into 50s. Although the latter improvement is not as high as the one achieved with the manually engineered features, it has got the advantage of being more general in a sense that it can readily utilize an arbitrary list of idioms without the knowledge acquisition overhead previously associated with this task, thereby fully automating the original approach.																	1949-3045					APR-JUN	2020	11	2					189	199		10.1109/TAFFC.2017.2777842													
J								Personalized Multitask Learning for Predicting Tomorrow's Mood, Stress, and Health	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Mood; Stress; Data models; Predictive models; Meteorology; Bayes methods; Mood prediction; multitask learning; deep neural networks; multi-kernel SVM; hierarchical Bayesian model	EMOTION RECOGNITION; CLASSIFICATION; FRAMEWORK; HAPPINESS	While accurately predicting mood and wellbeing could have a number of important clinical benefits, traditional machine learning (ML) methods frequently yield low performance in this domain. We posit that this is because a one-size-fits-all machine learning model is inherently ill-suited to predicting outcomes like mood and stress, which vary greatly due to individual differences. Therefore, we employ Multitask Learning (MTL) techniques to train personalized ML models which are customized to the needs of each individual, but still leverage data from across the population. Three formulations of MTL are compared: i) MTL deep neural networks, which share several hidden layers but have final layers unique to each task; ii) Multi-task Multi-Kernel learning, which feeds information across tasks through kernel weights on feature types; and iii) a Hierarchical Bayesian model in which tasks share a common Dirichlet Process prior. We offer the code for this work in open source. These techniques are investigated in the context of predicting future mood, stress, and health using data collected from surveys, wearable sensors, smartphone logs, and the weather. Empirical results demonstrate that using MTL to account for individual differences provides large performance improvements over traditional machine learning methods and provides personalized, actionable insights.																	1949-3045					APR-JUN	2020	11	2					200	213		10.1109/TAFFC.2017.2784832													
J								Human Observer and Automatic Assessment of Movement Related Self-Efficacy in Chronic Pain: From Exercise to Functional Activity	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Pain; Observers; Muscles; Wearable sensors; Psychology; Affective computing; bodily expressions; bodily muscle activity; chronic pain; self-efficacy	LOW-BACK-PAIN; TO-STAND MOVEMENT; PERFORMANCE; MOTION; EXPRESSIONS; ORIENTATION; PREVALENCE; CONFIDENCE; SYSTEM	Clinicians tailor intervention in chronic pain rehabilitation to movement related self-efficacy (MRSE). This motivates us to investigate automatic MRSE estimation in this context towards the development of technology that is able to provide appropriate support in the absence of a clinician. We first explored clinical observer estimation, which showed that body movement behaviours, rather than facial expressions or engagement behaviours, were more pertinent to MRSE estimation during physical activity instances. Based on our findings, we built a system that estimates MRSE from bodily expressions and bodily muscle activity captured using wearable sensors. Our results (F1 scores of 0.95 and 0.78 in two physical exercise types) provide evidence of the feasibility of automatic MRSE estimation to support chronic pain physical rehabilitation. We further explored automatic estimation of MRSE with a reduced set of low-cost sensors to investigate the possibility of embedding such capabilities in ubiquitous wearable devices to support functional activity. Our evaluation for both exercise and functional activity resulted in F1 score of 0.79. This result suggests the possibility of (and calls for more studies on) MRSE estimation during everyday functioning in ubiquitous settings. We provide a discussion of the implication of our findings for relevant areas.																	1949-3045					APR-JUN	2020	11	2					214	229		10.1109/TAFFC.2018.2798576													
J								Deep Physiological Affect Network for the Recognition of Human Emotions	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Emotion recognition; Physiology; Brain modeling; Electroencephalography; Biomedical monitoring; Convolution; Sensors; Emotion recognition; affective computing; physiological signals; EEG; PPG; convolutional; LSTM; emotional lateralization; inter-hemispheric asymmetry; valence; arousal	EEG; CLASSIFICATION; NEUROSCIENCE; FRAMEWORK; SYSTEMS; MODEL; STATE	Here we present a robust physiological model for the recognition of human emotions, called Deep Physiological Affect Network. This model is based on a convolutional long short-term memory (ConvLSTM) network and a new temporal margin-based loss function. Formulating the emotion recognition problem as a spectral-temporal sequence classification problem of bipolar EEG signals underlying brain lateralization and photoplethysmogram signals, the proposed model improves the performance of emotion recognition. Specifically, the new loss function allows the model to be more confident as it observes more of specific feelings while training ConvLSTM models. The function is designed to result in penalties for the violation of such confidence. Our experiments on a public dataset show that our deep physiological learning technology significantly increases the recognition rate of state-of-the-art techniques by 15.96 percent increase in accuracy. An extensive analysis of the relationship between participants' emotion ratings and physiological changes in brain lateralization function during the experiment is also presented.																	1949-3045					APR-JUN	2020	11	2					230	243		10.1109/TAFFC.2018.2790939													
J								Emotion Recognition Based on High-Resolution EEG Recordings and Reconstructed Brain Sources	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Electroencephalography; Feature extraction; Videos; Databases; Emotion recognition; Physiology; Brain modeling; EEG; emotion recognition; source localization; functional connectivity	FEATURE-EXTRACTION; RESPONSES; DATABASE	Electroencephalography (EEG)-based emotion recognition is currently a hot issue in the affective computing community. Numerous studies have been published on this topic, following generally the same schema: 1) presentation of emotional stimuli to a number of subjects during the recording of their EEG, 2) application of machine learning techniques to classify the subjects' emotions. The proposed approaches vary mainly in the type of features extracted from the EEG and in the employed classifiers, but it is difficult to compare the reported results due to the use of different datasets. In this paper, we present a new database for the analysis of valence (positive or negative emotions), which is made publicly available. The database comprises physiological recordings and 257-channel EEG data, contrary to all previously published datasets, which include at most 62 EEG channels. Furthermore, we reconstruct the brain activity on the cortical surface by applying source localization techniques. We then compare the performances of valence classification that can be achieved with various features extracted from all source regions (source space features) and from all EEG channels (sensor space features), showing that the source reconstruction improves the classification results. Finally, we discuss the influence of several parameters on the classification scores.																	1949-3045					APR-JUN	2020	11	2					244	257		10.1109/TAFFC.2017.2768030													
J								Computational Study of Primitive Emotional Contagion in Dyadic Interactions	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Computational modeling; Mathematical model; Atmospheric measurements; Particle measurements; Synchronization; Psychology; Primitive emotional contagion; facial expressions analysis; sentiment analysis; cross-recurrence quantification analysis	VIRTUAL AGENT; DYNAMICS; BEHAVIOR; SYSTEMS; SYNCHRONY; ALIGNMENT; GESTURES; STRESS	Interpersonal human-human interaction is a dynamical exchange and coordination of social signals, feelings and emotions usually performed through and across multiple modalities such as facial expressions, gestures, and language. Developing machines able to engage humans in rich and natural interpersonal interactions requires capturing such dynamics. This paper addresses primitive emotional contagion during dyadic interactions in which roles are prefixed. Primitive emotional contagion was defined as the tendency people have to automatically mimic and synchronize their multimodal behavior during interactions and, consequently, to emotionally converge. To capture emotional contagion, a cross-recurrence based methodology that explicitly integrates short and long-term temporal dynamics through the analysis of both facial expressions and sentiment was developed. This approach is employed to assess emotional contagion at unimodal, multimodal and cross-modal levels and is evaluated on the Solid SAL-SEMAINE corpus. Interestingly, the approach is able to show the importance of the adoption of cross-modal strategies for addressing emotional contagion.																	1949-3045					APR-JUN	2020	11	2					258	271		10.1109/TAFFC.2017.2778154													
J								Generalized Two-Stage Rank Regression Framework for Depression Score Prediction from Speech	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Speech; Feature extraction; Acoustics; Indexes; Tools; Predictive models; Signal processing; Bayesian; conditional ranking functions; depression; fusion; gaussian; marginalization; paralinguistics; prediction; speech; two-stage regression	SPACE	This paper introduces a novel speech-based depression score prediction paradigm, the 2-stage ranking prediction framework, and highlights the benefits it brings to depression prediction. Conventional regression approaches aim to discern a single functional relationship between speech features and depression scores, making an implicit assumption about the existence of a single fixed relationship between the features and scores. However, as the relationship between severity of depression and the clinical score may vary over the range of the assessment scale, this style of analysis may not be suited to depression prediction. The proposed framework on the other hand, imposes a series of partitions on the feature space, with each partition corresponding to a distinct predefined range of depression scores, and predicts the score based on measures of membership to each partition. This approach provides additional flexibility by allowing different rankings to be learnt for different depression scores, and relaxes assumptions made by conventional regression approaches. Results demonstrate the framework's suitability for depression score prediction: different 2-stage implementations, based on heterogeneous feature extraction and modelling approaches, produce state-of-the-art results on the AVEC-2013 dataset. It is also demonstrated that, unlike fusion of conventional regression systems, the fusion of two-stage systems consistently improves prediction performance.																	1949-3045					APR-JUN	2020	11	2					272	283		10.1109/TAFFC.2017.2766145													
J								Singing Robots: How Embodiment Affects Emotional Responses to Non-Linguistic Utterances	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Speech; Robot sensing systems; Psychology; Computers; Visualization; Music; Human robot interaction; non-linguistic utterances; emotive sound; lab study; iterative prototyping design	SELF-ASSESSMENT MANNEQUIN; SPEECH RATE; NEGATIVE AFFECT; PITCH HEIGHT; PERCEPTION; RECOGNITION; ATTITUDES; SADNESS; AGENTS; MODEL	Robots are often envisaged as embodied agents that might be able to intelligently and expressively communicate with humans. This could be due to their physical embodiment, their animated nature, or to other factors, such as cultural associations. In this study, we investigated emotional responses of humans to affective non-linguistic utterances produced by an embodied agent, with special attention to the way that these responses depended on the nature of the embodiment and the extent to which the robot actively moved in proximity to the human. To this end, we developed a new singing robot platform, ROVER, that could interact with humans in its surroundings. We used affective sound design methods to endow ROVER with the ability to communicate through song, via musical, non-linguistic utterances that could, as we demonstrate, evoke emotional responses in humans. We indeed found that the embodiment of the computational agent had an affect on emotional responses. However, contrary to our expectations, we found singing computers to be more emotionally arousing than singing robots. Whether the robot moved or not did not affect arousal. The results may have implications for the design of affective non-speech audio displays for human-computer or human-robot interaction.																	1949-3045					APR-JUN	2020	11	2					284	295		10.1109/TAFFC.2017.2774815													
J								Analysis and Classification of Cold Speech Using Variational Mode Decomposition	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Speech; Databases; Pathology; Speech recognition; Feature extraction; Nose; Mel frequency cepstral coefficient; Cold speech; variational mode decomposition; mutual information; SVM classifier	FEATURE-SELECTION; COMMON COLD; VOICE; RECOGNITION; FEATURES; HEALTHY	This paper presents analysis and classification of a pathological speech called cold speech, which is recorded when the person is suffering from common cold. Nose and throat are affected by the common cold. As nose and throat play an important role in speech production, the speech characteristics are altered during this pathology. In this work, variational mode decomposition (VMD) is used for analysis and classification of cold speech. VMD decomposes the speech signal into a number of sub-signals or modes. These sub-signals may better exploit the pathological information for characterization of cold speech. Various statistics, mean, variance, kurtosis and skewness are extracted from each of the decomposed sub-signals. Along with those statistics, center frequency, energy, peak amplitude, spectral entropy, permutation entropy and Renyi's entropy are evaluated, and used as features. Mutual information (MI) is further employed to assign the weight values to the features. In terms of classification rates, the proposed feature outperforms the linear prediction coefficients (LPC), mel frequency cepstral coefficients (MFCC), Teager energy operator (TEO) based feature and ComParE feature sets (IS09-emotion and IS13-ComParE). The proposed feature shows an average recognition rate of 90.02 percent for IITG cold speech database and 66.84 percent for URTIC database.																	1949-3045					APR-JUN	2020	11	2					296	307		10.1109/TAFFC.2017.2761750													
J								Emotion Recognition in Simulated Social Interactions	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Avatars; Emotion recognition; Face recognition; Cognitive processes; Emotion recognition; cognitive appraisal; facial expressions; social inferences; social interaction	FACIAL EXPRESSION; GAZE DIRECTION; UNIVERSALITY; INTEGRATION; FACES	Social context plays an important role in everyday emotional interactions, and others' faces often provide contextual cues in social situations. Investigating this complex social process is a challenge that can be addressed with the use of computer-generated facial expressions. In the current research, we use synthesized facial expressions to investigate the influence of socioaffective inferential mechanisms on the recognition of social emotions. Participants judged blends of facial expressions of shame-sadness, or of anger-disgust, in a target avatar face presented at the center of a screen while a contextual avatar face expressed an emotion (disgust, contempt, and sadness) or remained neutral. The dynamics of the facial expressions and the head/gaze movements of the two avatars were manipulated in order to create an interaction in which the two avatars shared eye gaze only in the social interaction condition. Results of Experiment 1 revealed that when the avatars engaged in social interaction, target expression blends of shame and sadness were perceived as expressing more shame if the contextual face expressed disgust and more sadness when the contextual face expressed sadness. Interestingly, perceptions of shame were not enhanced when the contextual face expressed contempt. The latter finding is probably attributable to the low recognition rates for the expression of contempt observed in Experiment 2.																	1949-3045					APR-JUN	2020	11	2					308	312		10.1109/TAFFC.2018.2799593													
J								Film Mood and Its Quantitative Determinants in Different Types of Scenes	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Mood; Estimation; Speech; Visualization; Feature extraction; Cameras; Film; affect; mood; style; content-based analysis	VIDEO; AUDIO	Films elicit emotions in viewers by infusing the story they tell with an affective character or tone-in a word, a mood. Considerable effort has been made recently to develop computational methods to estimate affective content in film. However, these efforts have focused almost exclusively on style-based features while neglecting to consider different scene types separately. In this study, we investigated the quantitative determinants of film mood across scenes classified by their setting and use of sounds. We examined whether viewers could assess film mood directly in terms of hedonic tone, energetic arousal, and tense arousal; whether their mood ratings differed by scene type; and how various narrative and stylistic film attributes as well as low- and high-level computational features related to the ratings. We found that the viewers were adept at assessing film mood, that sound-based scene classification brought out differences in the mood ratings, and that the low- and high-level features related to different mood dimensions. The study showed that computational film mood estimation can benefit from scene type classification and the use of both low- and high-level features. We have made our clip assessment and annotation data as well as the extracted computational features publicly available.																	1949-3045					APR-JUN	2020	11	2					313	326		10.1109/TAFFC.2018.2791529													
J								Context Sensitivity of EEG-Based Workload Classification Under Different Affective Valence	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Electroencephalography; Brain-computer interfaces; Adaptive systems; Brain-computer interface (BCI); electroencephalography (EEG); working memory load (WML); affective valence (VAL); emoback	WORKING-MEMORY; MENTAL WORKLOAD; BRAIN; COMMUNICATION; PERFORMANCE; DYNAMICS; FLANKER; EMOTION; LOOKING; BACK	State of the art brain-computer interfaces (BCIs) largely focus on detecting single, specific, often experimentally induced or manipulated aspects of the user state. In a less controlled, more naturalistic environment, a larger variety of mental processes may be active and possibly interacting. When moving BCI applications from the lab to real-life applications, these additional unaccounted mental processes could interfere with user state decoding, thus decreasing system efficacy and decreasing real-world applicability. Here, we assess the impact of affective valence on classification of working memory load, by re-analyzing a dataset that used an affective N-back task with picture stimuli. Our analyses showed that classification of working memory load under affective valence can lead to good classification accuracies (> 70 percent), which can be further improved via data integration over time. However, positive as well as negative affective valence resulted in decreased classification accuracies, when compared to the neutral affective context. Furthermore, classifiers failed to generalize across affective contexts, highlighting the need for user state models that can account for different contexts or new, context-independent, EEG features.																	1949-3045					APR-JUN	2020	11	2					327	334		10.1109/TAFFC.2017.2775616													
J								Intensional Learning to Efficiently Build Up Automatically Annotated Emotion Corpora	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Semantics; Standards; Computational modeling; Emotion recognition; Proposals; Reliability; Affective computing; corpora annotation; sentiment analysis; textual emotion recognition	MODEL	Textual emotion detection has a high impact on business, society, politics or education with applications such as, detecting depression or personality traits, suicide prevention or identifying cases of cyber-bulling. Given this context, the objective of our research is to contribute to the improvement of emotion recognition task through an automatic technique focused on reducing both the time and cost needed to develop emotion corpora. Our proposal is to exploit a bootstrapping approach based on intensional learning for automatic annotations with two main steps: 1) an initial similarity-based categorization where a set of seed sentences is created and extended by distributional semantic similarity (word vectors or word embeddings); 2) train a supervised classifier on the initially categorized set. The technique proposed allows us an efficient annotation of a large amount of emotion data with standards of reliability according to the evaluation results.																	1949-3045					APR-JUN	2020	11	2					335	347		10.1109/TAFFC.2017.2764470													
J								Co-Clustering to Reveal Salient Facial Features for Expression Recognition	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Feature extraction; Face recognition; Face; Facial features; Support vector machines; Image recognition; Mouth; Co-clustering; facial expression recognition; feature selection; gabor wavelets; support vector machines (SVMs)	FEATURE-SELECTION; REPRESENTATION	Facial expressions are a strong visual intimation of gestural behaviors. The intelligent ability to learn these non-verbal cues of the humans is the key characteristic to develop efficient human computer interaction systems. Extracting an effective representation from facial expression images is a crucial step that impacts the recognition accuracy. In this paper, we propose a novel feature selection strategy using singular value decomposition (SVD) based co-clustering to search for the most salient regions in terms of facial features that possess a high discriminating ability among all expressions. To the best of our knowledge, this is the first known attempt to explicitly perform co-clustering in the facial expression recognition domain. In our method, Gabor filters are used to extract local features from an image and then discriminant features are selected based on the class membership in co-clusters. Experiments demonstrate that co-clustering localizes the salient regions of the face image. Not only does the procedure reduce the dimensionality but also improves the recognition accuracy. Experiments on CK plus, JAFFE and MMI databases validate the existence and effectiveness of these learned facial features.																	1949-3045					APR-JUN	2020	11	2					348	360		10.1109/TAFFC.2017.2780838													
J								Speech Synthesis for the Generation of Artificial Personality	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Speech; Speech synthesis; Robots; Psychology; Hidden Markov models; Digital signal processing; Computational modeling; Personality; automatic personality perception; automatic personality recognition; automatic personality synthesis	VOICE QUALITY; PERCEPTION; BEHAVIOR	A synthetic voice personifies the system using it. In this work we examine the impact text content, voice quality and synthesis system have on the perceived personality of two synthetic voices. Subjects rated synthetic utterances based on the Big-Five personality traits and naturalness. The naturalness rating of synthesis output did not correlate significantly with any Big-Five characteristic except for a marginal correlation with openness. Although text content is dominant in personality judgments, results showed that voice quality change implemented using a unit selection synthesis system significantly affected the perception of the Big-Five, for example tense voice being associated with being disagreeable and lax voice with lower conscientiousness. In addition a comparison between a parametric implementation and unit selection implementation of the same voices showed that parametric voices were rated as significantly less neurotic than both the text alone and the unit selection system, while the unit selection was rated as more open than both the text alone and the parametric system. The results have implications for synthesis voice and system type selection for applications such as personal assistants and embodied conversational agents where developing an emotional relationship with the user, or developing a branding experience is important.																	1949-3045					APR-JUN	2020	11	2					361	372		10.1109/TAFFC.2017.2763134													
J								Partition decision trees: representation for efficient computation of the Shapley value extended to games with externalities	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Coalitional games; Shapley value; Externalities	COALITION STRUCTURE GENERATION; COMPLEXITY; CORE	While coalitional games with externalities model a variety of real-life scenarios of interest to computer science, they pose significant game-theoretic and computational challenges. Specifically, key game-theoretic solution concepts-and the Shapley value in particular-can be extended to games with externalities in multiple, often orthogonal, ways. As for the computational challenges, while there exist two concise representations for coalitional games with externalities-called embedded MC-Nets and weighted MC-Nets-they allow the polynomial-time computation of only two of the six existing direct extensions of the Shapley values to games with externalities. In this article, inspired by the literature on endogenous coalition formation protocols, we propose to represent games with externalities in a way that mimic an intuitive process in which coalitions might form. To this end, we utilize Partition Decision Trees-rooted directed trees, where non-leaf nodes are labelled with agents' names, leaf nodes are labelled with payoff vectors, and edges indicatemembership of agents in coalitions. Interestingly, despite their apparent differences, the representation based on partition decision trees can be considered a subclass of embedded MC-Nets and weighted MC-Nets. The key advantage of this new representation is that it allows the polynomial-time computation of five out of six direct extensions of the Shapley value to games with externalities. In other words, by focusing on narrower Partition Decision Trees instead of wider embedded or weighted MC-Nets, a user is guaranteed to compute most extensions of the Shapley value in polynomial time.																	1387-2532	1573-7454				APR	2020	34	1							11	10.1007/s10458-019-09429-7													
J								Wearable sensor-based evaluation of psychosocial stress in patients with metabolic syndrome	ARTIFICIAL INTELLIGENCE IN MEDICINE										Wearable System; e-Health; HRV; Affective Computing; Neural Networks; Metabolic Syndrome	RECOGNITION; CLASSIFICATION	The prevalence of metabolic disorders has increased rapidly as such they become a major health issue recently. Despite the definition of genetic associations with obesity and cardiovascular diseases, they constitute only a small part of the incidence of disease. Environmental and physiological effects such as stress, behavioral and metabolic disturbances, infections, and nutritional deficiencies have now revealed as contributing factors to develop metabolic diseases. This study presents a multivariate methodology for the modeling of stress on metabolic syndrome (MES) patients. We have developed a supporting system to cope with MES patients' anxiety and stress by means of several biosignals such as ECG, GSR, body temperature, SpO(2), glucose level, and blood pressure that are measured by a wearable device. We employed a neural network model to classify emotions with HRV analysis in the detection of stressor moments. We have accurately recognized the stressful situations using physiological responses to stimuli by utilizing our proposed affective state detection algorithm. We evaluated our system with a dataset of 312 biosignal records from 30 participants and the results showed that our proposed method achieved an average accuracy of 92% and 89% in distinguishing stress level in MES and other groups respectively. Both being the focus of an MES group and others proved to be highly arousing experiences which were significantly reflected in the physiological signal. Exposure to the stress in MES and cardiovascular heart disease patients increases the chronic symptoms. An early stage of comprehensive intervention may reduce the risk of general cardiovascular events in these particular groups. In this context, the use of e-health applications such as our proposed system facilitates these processes.																	0933-3657	1873-2860				APR	2020	104								101824	10.1016/j.artmed.2020.101824													
J								A novel deep mining model for effective knowledge discovery from omics data	ARTIFICIAL INTELLIGENCE IN MEDICINE										Knowledge discovery; Data mining; AI; Deep learning; Omics data analysis; Predictive modelling; Precision medicine	BREAST-CANCER; BIOMARKER DISCOVERY; NEURAL-NETWORKS; REPRESENTATION; ALGORITHM; ESTROGEN	Knowledge discovery from omics data has become a common goal of current approaches to personalised cancer medicine and understanding cancer genotype and phenotype. However, high-throughput biomedical datasets are characterised by high dimensionality and relatively small sample sizes with small signal-to-noise ratios. Extracting and interpreting relevant knowledge from such complex datasets therefore remains a significant challenge for the fields of machine learning and data mining. In this paper, we exploit recent advances in deep learning to mitigate against these limitations on the basis of automatically capturing enough of the meaningful abstractions latent with the available biological samples. Our deep feature learning model is proposed based on a set of non-linear sparse Auto-Encoders that are deliberately constructed in an under-complete manner to detect a small proportion of molecules that can recover a large proportion of variations underlying the data. However, since multiple projections are applied to the input signals, it is hard to interpret which phenotypes were responsible for deriving such predictions. Therefore, we also introduce a novel weight interpretation technique that helps to deconstruct the internal state of such deep learning models to reveal key determinants underlying its latent representations. The outcomes of our experiment provide strong evidence that the proposed deep mining model is able to discover robust biomarkers that are positively and negatively associated with cancers of interest. Since our deep mining model is problem-independent and data-driven, it provides further potential for this research to extend beyond its cognate disciplines.																	0933-3657	1873-2860				APR	2020	104								101821	10.1016/j.artmed.2020.101821													
J								Modeling and processing up-to-dateness of patient information in probabilistic therapy decision support	ARTIFICIAL INTELLIGENCE IN MEDICINE										Head and neck oncology; Therapy decision model; Decision support system; Arden syntax; Medical logic modules; Decision delay	DIAGNOSTIC DELAYS; UNCERTAINTY; CANCER; HEAD; ILLNESS; HEALTH; IMPACT; RISK	Objectives: Probabilistic modeling of a patients situation with the goal of providing calculated therapy recommendations can improve the decision making of interdisciplinary teams. Relevant information entities and direct causal dependencies, as well as uncertainty, must be formally described. Possible therapy options, tailored to the patient, can be inferred from the clinical data using these descriptions. However, there are several avoidable factors of uncertainty influencing the accuracy of the inference. For instance, inaccuracy may emerge from outdated information. In general, probabilistic models, e.g. Bayesian Networks can depict the causality and relations of individual information entities, but in general cannot evaluate individual entities concerning their up-to-dateness. The goal of the work at hand is to model diagnostic up-to-dateness, which can reasonably adjust the influence of outdated diagnostic information to improve the inference results of clinical decision models. Methods and materials: We analyzed 68 laryngeal cancer cases and modeled the state of up-to-dateness of different diagnostic modalities. All cases were used for cross-validation. 55 cases were used to train the model, 13 for testing. Each diagnostic procedure involved in the decision making process of these cases was associated with a specific threshold for the time the information is considered up-to-date, i.e. reliable. Based on this threshold, outdated findings could be identified and their impact on probabilistic calculations could be reduced. We applied the model for reducing the weight of outdated patient data in the computation of TNM stagings for the 13 test cases and compared the results to the manually derived TNM stagings in the patient files. Results: With the implementation of these weights in the laryngeal cancer model, we increased the accuracy of the TNM calculation from 0.61 (8 out of 13 cases correct) to 0.76 (10 out of 13 cases correct). Conclusion: Decision delay may cause specific patient data to be outdated. This can cause contradictory or false information and impair calculations for clinical decision support. Our approach demonstrates that the accuracy of Bayesian Network models can be improved when pre-processing the patient-specific data and evaluating their up-to-dateness with reduced weights on outdated information.																	0933-3657	1873-2860				APR	2020	104								101842	10.1016/j.artmed.2020.101842													
J								Multimodal data analysis of epileptic EEG and rs-fMRI via deep learning and edge computing	ARTIFICIAL INTELLIGENCE IN MEDICINE										Autonomic computing; Deep learning; Edge computing; Multimodal analysis EEG; FMRI; Brain-computer interface; Convolutional neural network	TEMPORAL-LOBE EPILEPSY; FUNCTIONAL CONNECTIVITY; BIG DATA; SEIZURE DETECTION; CLASSIFICATION; BRAIN; NETWORKS; PERFORMANCE; FRAMEWORK; SIGNALS	Background and objective: Multimodal data analysis and large-scale computational capability is entering medicine in an accelerative fashion and has begun to influence investigational work in a variety of disciplines. It is also informing us of therapeutic interventions that will come about with such development. Epilepsy is a chronic brain disorder in which functional changes may precede structural ones and which may be detectable using existing modalities. Methods: Functional connectivity analysis using electroencephalography (EEG) and resting state-functional magnetic resonance imaging (rs-fMRI) has provided such meaningful input in cases of epilepsy. By leveraging the potential of autonomic edge computing in epilepsy, we develop and deploy both noninvasive and invasive methods for monitoring, evaluation, and regulation of the epileptic brain. First, an autonomic edge computing framework is proposed for the processing of big data as part of a decision support system for surgical candidacy. Second, a multimodal data analysis using independently acquired EEG and rs-fMRI is presented for estimation and prediction of the epileptogenic network. Third, an unsupervised feature extraction model is developed for EEG analysis and seizure prediction based on a Convolutional deep learning (CNN) structure for distinguishing preictal (pre-seizure) state from non-preictal periods by support vector machine (SVM) classifier. Results: Experimental and simulation results from actual patient data validate the effectiveness of the proposed methods. Conclusions: The combination of rs-fMRI and EEG/iEEG can reveal more information about dynamic functional connectivity. However, simultaneous fMRI and EEG data acquisition present challenges. We have proposed system models for leveraging and processing independently acquired fMRI and EEG data.																	0933-3657	1873-2860				APR	2020	104								101813	10.1016/j.artmed.2020.101813													
J								Offline identification of surgical deviations in laparoscopic rectopexy	ARTIFICIAL INTELLIGENCE IN MEDICINE										Dynamic time warping; Hidden semi-Markov model; Intraoperative event detection; Rectopexy; Surgical process model	SEGMENTATION; RECOGNITION	Objective: According to a meta-analysis of 7 studies, the median number of patients with at least one adverse event during the surgery is 14.4%, and a third of those adverse events were preventable. The occurrence of adverse events forces surgeons to implement corrective strategies and, thus, deviate from the standard surgical process. Therefore, it is clear that the automatic identification of adverse events is a major challenge for patient safety. In this paper, we have proposed a method enabling us to identify such deviations. We have focused on identifying surgeons' deviations from standard surgical processes due to surgical events rather than anatomic specificities. This is particularly challenging, given the high variability in typical surgical procedure workflows. Methods: We have introduced a new approach designed to automatically detect and distinguish surgical process deviations based on multi-dimensional non-linear temporal scaling with a hidden semi-Markov model using manual annotation of surgical processes. The approach was then evaluated using cross-validation. Results: The best results have over 90% accuracy. Recall and precision for event deviations, i.e. related to adverse events, are respectively below 80% and 40%. To understand these results, we have provided a detailed analysis of the incorrectly-detected observations. Conclusion: Multi-dimensional non-linear temporal scaling with a hidden semi-Markov model provides promising results for detecting deviations. Our error analysis of the incorrectly-detected observations offers different leads in order to further improve our method. Significance: Our method demonstrated the feasibility of automatically detecting surgical deviations that could be implemented for both skill analysis and developing situation awareness-based computer-assisted surgical systems.																	0933-3657	1873-2860				APR	2020	104								101837	10.1016/j.artmed.2020.101837													
J								Feature selection based multivariate time series forecasting: An application to antibiotic resistance outbreaks prediction	ARTIFICIAL INTELLIGENCE IN MEDICINE										Feature selection; Multi-objective evolutionary algorithms; Multivariate time series; Antibiotic resistance forecasting; Multiple criteria decision making	EVOLUTIONARY FEATURE-SELECTION; MULTIOBJECTIVE DIFFERENTIAL EVOLUTION; STAPHYLOCOCCUS-AUREUS INFECTIONS; COMMUNITY-ACQUIRED PNEUMONIA; METHICILLIN-RESISTANT; AVERAGE MODEL; INFLUENZA; ALGORITHM; EPIDEMIOLOGY; CLASSIFICATION	Antimicrobial resistance has become one of the most important health problems and global action plans have been proposed globally. Prevention plays a key role in these actions plan and, in this context, we propose the use of Artificial Intelligence, specifically Time Series Forecasting techniques, for predicting future outbreaks of Methicillin-resistant Staphylococcus aureus (MRSA). Infection incidence forecasting is approached as a Feature Selection based Time Series Forecasting problem using multivariate time series composed of incidence of Staphylococcus aureus Methicillin-sensible and MRSA infections, influenza incidence and total days of therapy of both of Levofloxacin and Oseltamivir antimicrobials. Data were collected from the University Hospital of Getafe (Spain) from January 2009 to January 2018, using months as time granularity. The main contributions of the work are the following: the applications of wrapper feature selection methods where the search strategy is based on multi-objective evolutionary algorithms (MOEA) along with evaluators based on the most powerful state-of-the-art regression algorithms. The performance of the feature selection methods has been measured using the root mean square error (RMSE) and mean absolute error (MAE) performance metrics. A novel multi-criteria decision-making process is proposed in order to select the most satisfactory forecasting model, using the metrics previously mentioned, as well as the slopes of model prediction lines in the 1, 2 and 3 steps-ahead predictions. The multi-criteria decision-making process is applied to the best models resulting from a ranking of databases and regression algorithms obtained through multiple statistical tests. Finally, to the best of our knowledge, this is the first time that a feature selection based multivariate time series methodology is proposed for antibiotic resistance forecasting. Final results show that the best model according to the proposed multi-criteria decision making process provides a RMSE = (0.1349, 0.1304, 0.1325) and a MAE = (0.1003, 0.096, 0.0987) for 1, 2, and 3 steps-ahead predictions.																	0933-3657	1873-2860				APR	2020	104								101818	10.1016/j.artmed.2020.101818													
J								Early detection of sepsis utilizing deep learning on electronic health record event sequences	ARTIFICIAL INTELLIGENCE IN MEDICINE										Sepsis; Clinical decision support systems; Machine learning; Medical informatics; Early diagnosis; Electronic health records	INTERNATIONAL CONSENSUS DEFINITIONS; PREDICTION MODELS; CURVE	Background: The timeliness of detection of a sepsis incidence in progress is a crucial factor in the outcome for the patient. Machine learning models built from data in electronic health records can be used as an effective tool for improving this timeliness, but so far, the potential for clinical implementations has been largely limited to studies in intensive care units. This study will employ a richer data set that will expand the applicability of these models beyond intensive care units. Furthermore, we will circumvent several important limitations that have been found in the literature: (1) Model evaluations neglect the clinical consequences of a decision to start, or not start, an intervention for sepsis. (2) Models are evaluated shortly before sepsis onset without considering interventions already initiated. (3) Machine learning models are built on a restricted set of clinical parameters, which are not necessarily measured in all departments. (4) Model performance is limited by current knowledge of sepsis, as feature interactions and time dependencies are hard-coded into the model. Methods: In this study, we present a model to overcome these shortcomings using a deep learning approach on a diverse multicenter data set. We used retrospective data from multiple Danish hospitals over a seven-year period. Our sepsis detection system is constructed as a combination of a convolutional neural network and a long short-term memory network. We assess model quality by standard concepts of accuracy as well as clinical usefulness, and we suggest a retrospective assessment of interventions by looking at intravenous antibiotics and blood cultures preceding the prediction time. Results: Results show performance ranging from AUROC 0.856 (3 h before sepsis onset) to AUROC 0.756 (24 h before sepsis onset). Evaluating the clinical utility of the model, we find that a large proportion of septic patients did not receive antibiotic treatment or blood culture at the time of the sepsis prediction, and the model could, therefore, facilitate such interventions at an earlier point in time. Conclusion: We present a deep learning system for early detection of sepsis that can learn characteristics of the key factors and interactions from the raw event sequence data itself, without relying on a labor-intensive feature extraction work. Our system outperforms baseline models, such as gradient boosting, which rely on specific data elements and therefore suffer from many missing values in our dataset.																	0933-3657	1873-2860				APR	2020	104								101820	10.1016/j.artmed.2020.101820													
J								Characterizing the critical features when personalizing antihypertensive drugs using spectrum analysis and machine learning methods	ARTIFICIAL INTELLIGENCE IN MEDICINE										Data mining methods; Antihypertensive drugs; Drug-related attributes; Blood pressure control; Machine learning	DECISION-SUPPORT-SYSTEM; BLOOD-PRESSURE CHANGES; DEVELOPING HYPERTENSION; RISK; CLASSIFICATION; PREDICTION; AMLODIPINE; ADHERENCE; SELECTION; MARKER	Globally, methods of controlling blood pressure in hypertension patients remain inefficient. The difficulty of prescribing appropriate drugs specific to a patient's clinical features serves as one of the most important factors. Characterizing the critical drug-related features, just like that of the antibacterial spectrum (where each item is sensitive to the targeted drug's effectiveness or a specified indication), may help a doctor easily prescribe appropriate drugs by matching a patient's attributes with drug-related features, and effectiveness of the selected drugs would also be ascertained. In this study, we aimed to apply data mining methods to obtain the clinical characteristics spectrum or important clinical features of five frequently used drugs (Irbesartan, Metoprolol, Felodipine, Amlodipine, and Levamlodipine) for hypertension control by comparing successful and unsuccessful cases. Spectrum analysis based on a statistical method and five algorithms based on machine learning were used to extract the critical clinical features. A visualized relative weight matrix was then achieved by combining the results from the characteristic spectrum and machine learning-based methods. Our results indicated that the five targeted antihypertension agents had different importance orders of the 15 relative clinical features. Clinical analysis showed that the extracted important clinical attributes of the five drugs were both reasonable and meaningful in the selection of hypertension treatment. Therefore, our study provided a data-driven reference for the personalization of clinical antihypertensive drugs.																	0933-3657	1873-2860				APR	2020	104								101841	10.1016/j.artmed.2020.101841													
J								A recurrent neural network approach to predicting hemoglobin trajectories in patients with End-Stage Renal Disease	ARTIFICIAL INTELLIGENCE IN MEDICINE										Hemoglobin prediction; Recurrent neural network; ESA dosing; Iron dosing; Anemia	HEMODIALYSIS-PATIENTS; ANEMIA MANAGEMENT; PHARMACOLOGY; EPOETIN; ALPHA; IRON	The most severe form of kidney disease, End-Stage Renal Disease (ESRD) is treated with various forms of dialysis - artificial blood cleansing. Dialysis patients suffer many health burdens including high mortality and hospitalization rates, and symptomatic anemia: a low red blood cell count as indicated by a low hemoglobin (Hgb) level. ESRD-induced anemia is treated, with variable patient response, by erythropoiesis stimulating agents (ESAs): expensive injectable medications typically administered during dialysis sessions. The dosing protocol is typically a population level protocol based on original clinical trials, the use of which often results in Hgb cycling. This cycling phenomenon occurs primarily due to the mismatch in the time between dosing decisions and the time it takes for the effects of a dosing change to be fully realized. In this paper we develop a recurrent neural network approach that uses historic data together with future ESA and iron dosing data to predict the 1, 2, and 3 month Hgb levels of patients with ESRD-induced anemia. The results of extensive experimentation indicate that this approach generates predictions that are clinically relevant: the mean absolute error of the predictions is comparable to estimates of the infra-individual variability of the laboratory test for Hgb.																	0933-3657	1873-2860				APR	2020	104								101823	10.1016/j.artmed.2020.101823													
J								Efficient treatment of outliers and class imbalance for diabetes prediction	ARTIFICIAL INTELLIGENCE IN MEDICINE										Outlier detection; Imbalanced data; Machine learning; Data preprocessing; Oversampling; SMOTE	LIFE-STYLE INTERVENTION; CLASSIFICATION; DIAGNOSIS; ARTMAP; IDENTIFICATION; PREVENTION; DISEASE; SYSTEM	Learning from outliers and imbalanced data remains one of the major difficulties for machine learning classifiers. Among the numerous techniques dedicated to tackle this problem, data preprocessing solutions are known to be efficient and easy to implement. In this paper, we propose a selective data preprocessing approach that embeds knowledge of the outlier instances into artificially generated subset to achieve an even distribution. The Synthetic Minority Oversampling TEchnique (SMOTE) was used to balance the training data by introducing artificial minority instances. However, this was not before the outliers were identified and oversampled (irrespective of class). The aim is to balance the training dataset while controlling the effect of outliers. The experiments prove that such selective oversampling empowers SMOTE, ultimately leading to improved classification performance.																	0933-3657	1873-2860				APR	2020	104								101815	10.1016/j.artmed.2020.101815													
J								A neutrosophic-entropy based adaptive thresholding segmentation algorithm: A special application in MR images of Parkinson's disease	ARTIFICIAL INTELLIGENCE IN MEDICINE										Neutrosophic set (NS) theory; Neutrosophic entropy information (NEI); Parkinson's disease (PD); Image segmentation; Magnetic resonance (MR) images	FUZZY C-MEANS; GRAY-MATTER; SPINAL-CORD; NUMBERS	Brain MR images are composed of three main regions such as gray matter, white matter and cerebrospinal fluid. Radiologists and medical practitioners make decisions through evaluating the developments in these regions. Study of these MR images suffers from two major issues such as: (a) the boundaries of their gray matter and white matter regions are ambiguous and unclear in nature, and (b) their regions are formed with unclear inhomogeneous gray structures. These two issues make the diagnosis of critical diseases very complex. To solve these issues, this study presented a method of image segmentation based on the neutrosophic set (NS) theory and neutrosophic entropy information (NEI). By nature, the proposed method is adaptive to select the threshold value and is entitled as neutrosophic-entropy based adaptive thresholding segmentation algorithm (NEATSA). In this study, experimental results were provided through the segmentation of Parkinson's disease (PD) MR images. Experimental results, including statistical analyses showed that NEATSA can segment the main regions of MR images very clearly compared to the well-known methods of image segmentation available in literature of pattern recognition and computer vision domains.																	0933-3657	1873-2860				APR	2020	104								101838	10.1016/j.artmed.2020.101838													
J								Optimisation and control of the supply of blood bags in hemotherapic centres via Markov decision process with discounted arrival rate	ARTIFICIAL INTELLIGENCE IN MEDICINE										OR in health services; Perishable inventory; Blood; Stochastic modeling; Blood management	INVENTORY MANAGEMENT; POLICIES; CHAIN; PRODUCTS; SYSTEMS; DONATION; TRANSSHIPMENT; COMPETITION; PLATELETS; IMPACTS	Running a cost-effective human blood transfusion supply chain challenges decision makers in blood services world-wide. In this paper, we develop a Markov decision process with the objective of minimising the overall costs of internal and external collections, storing, producing and disposing of blood bags, whilst explicitly considering the probability that a donated blog bag will perish before demanded. The model finds an optimal policy to collect additional bags based on the number of bags in stock rather than using information about the age of the oldest item. Using data from the literature, we validate our model and carry out a case study based on data from a large blood supplier in South America. The study helped achieve an overall increase of 4.5% in blood donations in one year.																	0933-3657	1873-2860				APR	2020	104								101791	10.1016/j.artmed.2020.101791													
J								An improved multi-swarm particle swarm optimizer for optimizing the electric field distribution of multichannel transcranial magnetic stimulation	ARTIFICIAL INTELLIGENCE IN MEDICINE										Multi-swarm particle swarm optimizer; Multichannel transcranial magnetic stimulation; Information exchange strategy; Leaning strategy; Mutation strategy	ALGORITHM; DESIGN; DEEP	Multichannel transcranial magnetic stimulation (mTMS) is a therapeutic method to improve psychiatric diseases, which has a flexible working pattern used to different applications. In order to make the electric field distribution in the brain meet the treatment expectations, we have developed a novel multi-swam particle swarm optimizer (NMSPSO) to optimize the current configuration of double layer coil array. To balance the exploration and exploitation abilities, three novel improved strategies are used in NMSPSO based on multi-swarm particle swarm optimizer. Firstly, a novel information exchange strategy is achieved by individual exchanges between sub-swarms. Secondly, a novel leaning strategy is used to control knowledge dissemination in the population, which not only increases the diversity of the particles but also guarantees the convergence. Finally, a novel mutation strategy is introduced, which can help the population jump out of the local optimum for better exploration ability. The method is examined on a set of well-known benchmark functions and the results show that NMSPSO has better performance than many particle swarm optimization variants. And the superior electric field distribution in mTMS can be obtained by NMSPSO to optimize the current configuration of the double layer coil array.																	0933-3657	1873-2860				APR	2020	104								101790	10.1016/j.artmed.2020.101790													
J								Detecting potential signals of adverse drug events from prescription data	ARTIFICIAL INTELLIGENCE IN MEDICINE										Adverse drug events (ADEs); Prescription data; Logistic regression; Case-crossover	SEQUENCE SYMMETRY ANALYSIS; CAUSAL INFERENCE; MEDICATION; SELECTION	Adverse drug events (ADEs) may occur and lead to severe consequences for the public, even though clinical trials are conducted in the stage of pre-market. Computational methods are still needed to fulfil the task of pharmacosurveillance. In post-market surveillance, the spontaneous reporting system (SRS) has been widely used to detect suspicious associations between medicines and ADEs. However, the passive mechanism of SRS leads to the hysteresis in ADE detection by SRS based methods, not mentioning the acknowledged problem of under-reporting and duplicate reporting in SRS. Therefore, there is a growing demand for other complementary methods utilising different types of healthcare data to assist with global pharmacosurveillance. Among those data sources, prescription data is of proved usefulness for pharmacosurveillance. However, few works have used prescription data for signalling ADEs. In this paper, we propose a data-driven method to discover medicines that are responsible for a given ADE purely from prescription data. Our method uses a logistic regression model to evaluate the associations between up to hundreds of suspected medicines and an ADE spontaneously and selects the medicines possessing the most significant associations via Lasso regularisation. To prepare data for training the logistic regression model, we adapt the design of the case-crossover study to construct case time and control time windows for the extraction of medicine use information. While the case time window can be readily determined, we propose several criteria to select the suitable control time windows providing the maximum power of comparisons. In order to address confounding situations, we have considered diverse factors in medicine utilisation in terms of the temporal effect of medicine and the frequency of prescription, as well as the individual effect of patients on the occurrence of an ADE. To assess the performance of the proposed method, we conducted a case study with a real-world prescription dataset. Validated by the existing domain knowledge, our method successfully traced a wide range of medicines that are potentially responsible for the ADE. Further experiments were also carried out according to a recognised gold standard, our method achieved a sensitivity of 65.9% and specificity of 96.2%.																	0933-3657	1873-2860				APR	2020	104								101839	10.1016/j.artmed.2020.101839													
J								A Computation Model for Senior Citizen Health Self-Care	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Senior citizen health self-care; patient monitoring; multi-level computation cycles; events; event icons; slow intelligence		A computation model is proposed for senior citizen health self-care through adaptive multi-level computation cycles according to the principles of slow intelligence. An experimental system, called the TDR system, is being implemented on a smart phone to serve as the test bed for the proposed approach. In this paper, we describe the computation model, the basic concepts and the prototype experimental system. The main theoretical concept is centered on adaptive multilevel computation cycles, events and event icons. Further experimental as well as theoretical investigations of the proposed computation model are discussed.																	0218-1940	1793-6403				APR	2020	30	4			SI		483	501		10.1142/S021819402040001X													
J								Mutual Inference Model for User Roles and Urban Functional Zones	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										User roles; functional zones; user mobility; situational elements; context factors		Users engage in various activities in different regions of the city, how to infer their roles through the area of the user's activity, and how to infer the possible geographical structure of the user's role through the user's role, it is the case of the challenging issues in the user's mobility field. The necessary context elements are extracted from the user's microblogging, such as user ID, checkins and GPS coordinates, the mutual inference model of the user's role and the urban functional zones is established for the user's movement law. The experimental results show that the model has achieved good performance in terms of using the user role to infer the urban geographical structure, or in the use of the urban functional zones to infer the user roles.																	0218-1940	1793-6403				APR	2020	30	4			SI		503	521		10.1142/S0218194020400021													
J								An Intelligent Service Middleware Based on Sensors in IoT Environments	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Intelligent middleware; sensor-based middleware; ontology; inference system		In Internet of Things (IoT) environment, a number of sensors and devices exist and they have various and heterogeneous characteristics. Applications which provide a variety of services based on the sensors also have different service requirements. Therefore, a middleware that is located between sensors and application systems is needed for integrating two layers. This paper proposes a general purpose middleware for providing intelligent services based on heterogeneous sensors existing in IoT environment. The proposed middleware acquires and manages sensing data in real time. The middleware stores and manages heterogeneous sensors, node, and network metadata. Especially, the middleware in this paper includes a component for providing intelligent services based on inferencing the situation based on ontologies and rules.																	0218-1940	1793-6403				APR	2020	30	4			SI		523	536		10.1142/S0218194020400033													
J								Smart Street Litter Detection and Classification Based on Faster R-CNN and Edge Computing	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Smart city; street cleanliness; deep leaning; edge computing	CLEANLINESS; NETWORKS	Cleanliness of city streets has an important impact on city environment and public health. Conventional street cleaning methods involve street sweepers going to many spots and manually confirming if the street needs to be clean. However, this method takes a substantial amount of manual operations for detection and assessment of street's cleanliness which leads to a high cost for cities. Using pervasive mobile devices and AI technology, it is now possible to develop smart edge-based service system for monitoring and detecting the cleanliness of streets at scale. This paper explores an important aspect of cities - how to automatically analyze street imagery to understand the level of street litter. A vehicle (i.e. trash truck) equipped with smart edge station and cameras is used to collect and process street images in real time. A deep learning model is developed to detect, classify and analyze the diverse types of street litters such as tree branches, leaves, bottles and so on. In addition, two case studies are reported to show its strong potential and effectiveness in smart city systems.																	0218-1940	1793-6403				APR	2020	30	4			SI		537	553		10.1142/S0218194020400045													
J								Winning Strategy Tree Construction for BDD-Based ATL Model Checkers	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Formal verification; model checking; alternating-time temporal logic (ATL); binary decision diagrams (BDD); winning strategy	CHECKING; COUNTEREXAMPLES; ALGORITHMS	The Alternating-time Temporal Logic (ATL) is a temporal logic for formal verification of open systems, which supports selective quantification over paths. The open system can be represented as a game where the system and the environment pick their moves in turn or simultaneously. The ATL model checker Mocha has been developed by using Binary Decision Diagrams (BDDs), and successfully employed for open system verification. However, since Mocha symbolically manipulates a set of states by a BDD, it is hard to generate a winning strategy as a witness or a counter-example. In this paper, we propose a novel algorithm to efficiently construct a winning strategy tree and report that our technique has been successfully implemented on Mocha.																	0218-1940	1793-6403				APR	2020	30	4			SI		555	573		10.1142/S0218194020500199													
J								An Automated Approach for Constructing Framework Instantiation Documentation	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Program comprehension; reverse engineering; documentation; frameworks; empirical assessment		A substantial effort, in general, is required for understanding APIs of application frameworks. High-quality API documentation may alleviate the effort, but the production of such documentation still poses a major challenge for modern frameworks. To facilitate the production of framework instantiation documentation, we hypothesize that the framework code itself and the code of existing instantiations provide useful information. However, given the size and complexity of existent code, automated approaches are required to assist the documentation production. Our goal is to assess an automated approach for constructing relevant documentation for framework instantiation based on source code analysis of the framework itself and of existing instantiations. The criterion for defining whether documentation is relevant would be to compare the documentation with an traditional framework documentation, considering the time spent and correctness during instantiation activities, information usefulness, complexity of the activity, navigation, satisfaction, information localization and clarity. We propose an automated approach for constructing relevant documentation for framework instantiation based on source code analysis of the framework itself and of existing instantiations. The proposed approach generates documentation in a cookbook style, where the recipes are programming activities using the necessary API elements driven by the framework features. We performed an empirical study, consisting of three experiments with 44 human subjects executing real framework instantiations aimed at comparing the use of the proposed cookbooks to traditional manual framework documentation (baseline). Our empirical assessment shows that the generated cookbooks performed better or, at least, with non-significant difference when compared to the traditional documentation, evidencing the effectiveness of the approach.																	0218-1940	1793-6403				APR	2020	30	4			SI		575	601		10.1142/S0218194020500205													
J								Identification of Apnea-Hypopnea Index Subgroups Based on Multifractal Detrended Fluctuation Analysis and Nasal Cannula Airflow Signals	TRAITEMENT DU SIGNAL										obstructive sleep apnea hypopnea syndrome (OSAHS); positive airway pressure (pap); apnea-hypopnea index (AHI); multifractal detrended fluctuation analysis; nasal cannula airflow signals; feature extraction; feature selection; random forest	RANDOM FOREST ALGORITHM; SLEEP-APNEA; GENDER DETERMINATION; AUTOMATIC DETECTION; FEATURES; EVENTS; PRESSURE	The diagnosis of obstructive sleep apnea hypopnea syndrome (OSASH) and making decision of treatment necessity with positive airway pressure (PAP) therapy are time consuming and costly processes. There were different approaches in literature to accomplish these processes successfully and as soon as possible by using physiological signals with selected feature extraction and machine learning techniques. To reach fastest and true result, selection of optimal physiological signal(s), feature extraction and learning techniques is important. This study aimed to identify apnea hypopnea index (AHI) subgroups of 120 subjects and thus diagnose of OSASH and determine the need for PAP therapy by applying Multifractal Detrended Fluctuation Analysis (MDFA) as a feature extraction technique to only single channel nasal cannula airflow signals. After the extracted features from airflow signals with MDFA were gone through feature selection phase, the selected features were evaluated in Random Forest classifier. With the implementation of all processes, OSAHS patients were discriminated from healthy subjects with 95.83% accuracy, 96.88% sensitivity and 93.75% specificity. 93.75% sensitivities and 93.75%, 100% and 96.88% specificities were obtained for 15 <= AHI (PAP therapy necessary), 5 <= AHI<15 (require additional information for PAP therapy decision) and AHI <5 (not require PAP therapy) subgroups, respectively.																	0765-0019	1958-5608				APR	2020	37	2					145	156		10.18280/ts.370201													
J								Adaptive and Feature-Preserving Bilateral Filters for Three-Dimensional Models	TRAITEMENT DU SIGNAL										bilateral filtering; mesh denoising; scale parameters; feature preservation		Bilateral filtering is a well-known tool to denoise or smooth one-dimensional (1D) signals, two-dimensional (2D) images, and three-dimensional (3D) models. The bilateral weights help preserve the edges or features more effectively than unilateral weights. However, it is immensely difficult to configure the scale parameters of the convolutional kernel functions. To overcome the difficulty, this paper proposes adaptive, feature-preserving bilateral filters for 3D models by introducing automatic, adaptive scale parameters. Firstly, the feature scale was defined on 3D models, bridging up the gap between feature scale and scale parameter of the Gaussian functions. Next, the scale descriptor was proposed to adaptively configure the scale parameter for each face of the target 3D model, changing the traditional approach of adopting the same scale parameter for all faces. On this basis, a feature-preserving local filter was designed by introducing the adaptive scale parameters to the iterative local scheme, and a modified global filter, which is robust to irregular sampling during denoising, was designed based on the adaptive scale parameters. The excellence of our filters was proved through experiments on multiple synthetic and real-world noisy models, in comparison to the state-of-the-art filters. The research results lay a solid basis for feature preservation and noise removal of 3D models.																	0765-0019	1958-5608				APR	2020	37	2					157	168		10.18280/ts.370202													
J								New Thresholding Technique in DCT Domain for Interference Mitigation in GNSS Receivers	TRAITEMENT DU SIGNAL										GNSS interference mitigation; DSSS; Discrete cosine transform; Universal threshold; statistical sampling theory; Tukey window; narraw band interference (NBI)	EXCISION; SYSTEMS	The presented method is a DCT mitigation thresholding technique (DCT-MTT) for narrowband interference reduction in Global Navigation Satellite System (GNSS) receivers. First, the received signal immersed in an Additive White Gaussian Noise (AWGN), is multiplied, in time domain, (sample by sample) by a Tukey window of the same length. Then, the DCT transform is applied. Next, the transformed signal is divided in non-overlapped packets. Each one can be viewed as a non-interfered packet (if it has roughly the same variance as the estimated unknown variance of the AWGN) that must be conserved, or considered as an interference packet (if its variance is significantly greater than the variance of the AWGN) that should be thresholded. The conservation (or inversely the thresholding) of a packet is achieved by the use of DONOHO' s Universal-Threshold apart from that the variance is estimated based on the statistical sampling theory. The Final step consists in the application of the inverse DCT to obtain a good approximation of the received interference-less signal. The results obtained from several simulations confirm that the suggested strategy outperforms, in term of signal quality restoration, the conventional methods.																	0765-0019	1958-5608				APR	2020	37	2					169	180		10.18280/ts.370203													
J								Detection of Skin Cancer Image by Feature Selection Methods Using New Buzzard Optimization (BUZO) Algorithm	TRAITEMENT DU SIGNAL										skin cancer; skin lesion; Dermoscopy images; shape and color features; Buzzard Optimization (BUZO) algorithm; feature selection	PARTICLE SWARM OPTIMIZATION; OBJECT-BASED CLASSIFICATION; IMPERIALIST COMPETITIVE ALGORITHM; HYBRID NEURAL-NETWORK; GENETIC ALGORITHM; SYSTEM; SEGMENTATION; DIAGNOSIS; SEARCH	Feature selection is used in machine learning as well as in statistical pattern recognition. This is important in many applications, such as classification. There are so many extracted features in these applications which are either useless or do not have much information. If not removing these features, make raises the computational burden for the main application. In different methods of feature selection, a subset is selected as the answer, which can optimize the value of an evaluation function. In this study, a new algorithm for classification of Dermoscopy images into two types of malignant and benign are presented. To develop the general skin cancer detection system, at first a pre-processing step is applied to enhance image quality. Then the lesion area is removed from the healthy areas using the Otsu threshold method. Nine shape feature and nine color features are extracted from the segmented image using different optimization schema. At the end of the operation, classification was done by SVM, KNN and Decision Tree methods. The results show that combination of buzzard optimization algorithm for feature extraction and SVM classifier accuracy is 94.3%. This result shows the high potential of buzzard optimization algorithm for feature extraction.																	0765-0019	1958-5608				APR	2020	37	2					181	194		10.18280/ts.370204													
J								A Novel Image Fusion Method for Water Body Extraction Based on Optimal Band Combination	TRAITEMENT DU SIGNAL										water body extraction; Enhanced Thematic Mapper Plus (ETM plus ); Phased Array type L-band Synthetic Aperture Radar (PALSAR); optimal band combination (OBC)	LANDSAT; SAR; ALGORITHM; RADAR; AREAS	This paper attempts to design an image fusion method that facilitates the extraction of water bodies from remote sensing images, namely, the images taken by Enhanced Thematic Mapper Plus (ETM+) of Landsat 7 and those taken by Phased Array type L-band Synthetic Aperture Radar (PALSAR) of Advanced Land Observation Satellite (ALOS). Firstly, the water body information was extracted from ETM+ data and PALSAR data, and combined into a benchmark image. Next, several traditional image fusion methods were separately adopted to merge the ETM+453 image and the PALSAR HH image, and the water bodies extracted from the fused images were compared in details. The selected methods include principal component analysis (PCA), Brovey transform (BT), intensity-hue-saturation (IHS) transform, discrete wavelet transform (DWT), and high-pass filter (HPF). After that, a new image fusion method was designed based on optimal band combination (OBC), and the water bodies extracted by the method were compared with the benchmark image and those extracted by the traditional methods. The results show that the ALOS HH image alone achieved higher accuracy in water body extraction than the ETM+ image alone; the traditional image fusion methods, namely, PCA, BT, IHS, HPF and DWT, were more accurate than the ETM+ image alone in water body extraction, and less accurate than the ALOS HH image alone. The OBC-based image fusion method greatly outperformed all the traditional methods. The research results provide a good reference for image fusion and extraction tasks in similar cases.																	0765-0019	1958-5608				APR	2020	37	2					195	207		10.18280/ts.370205													
J								Hand-Crafted Features vs Deep Learning for Pedestrian Detection in Moving Camera	TRAITEMENT DU SIGNAL										deep learning; handcrafted features; intelligent transport systems; moving camera; pedestrian detection	OBJECT DETECTION	Detecting pedestrians and other objects in images taken from moving platforms is an essential task needed for many applications such as smart surveillance systems and intelligent transportation systems. However, most detectors in this domain still rely on handcrafted features to separate the foreground objects from the background. While these types of methods have presented good results with good response times, they still have some weaknesses to overcome. In recent years, alternative object detection methods are being proposed, with deep learning based approaches rising in popularity thanks to their promising results. In this paper, we propose two pedestrian detectors for use in images taken from a moving vehicle: The first detector uses a block matching algorithm and handcraft features for pedestrian detection, and the second uses a Faster R-CNN deep detector. We also compare both systems' performances to other state-of-the-art pedestrian detectors. Our results show that although handcraft feature-based approach achieves good results within acceptable detection times, it suffers from a high false positive rate. However, we found that Faster R-CNN detector performs better in terms of precision and recall, but these improved results come at a cost of detection time.																	0765-0019	1958-5608				APR	2020	37	2					209	216		10.18280/ts.370206													
J								A Multi-class SVM Based Content Based Image Retrieval System Using Hybrid Optimization Techniques	TRAITEMENT DU SIGNAL										CBIT; CS-SCHT; exact Legendre moments; HSV color quantization; differential evolution; multi-class SVM; firefly algorithm	FEATURE-SELECTION; TEXTURE; COLOR	Due to the increasing usage of multimedia and storage devices accessible, searching for large image databases has become imperative. Furthermore, the handiness of high-speed internet has escalated the exchange of images by users enormously. Content-Based Image Retrieval is proposed in this work, taking features based on Exact Legendre Moments, HVS color quantization with dc coefficient and statistical properties such as variance, mean, and skew of Conjugate Symmetric Sequency Complex Hadamard Transform (CS-SCHT). In most of the machine learning tasks, the quality of the learning process depends on dimensionality. High dimensional datasets can influence the classification outcome and training time. To overcome this problem, we use DE (Differential Evolution) to generate the optimal feature subsets. The features scaled by weights derived from the firefly algorithm, which fed to Multi-Class SVM. The fitness function taken for the firefly algorithm is the classification error of SVM. By minimizing fitness function, optimum weights are obtained. When these optimal weights are applied to SVM, the proposed algorithm exhibits better precision, recall, and accuracy when compared to some of the existing algorithms in the literature.																	0765-0019	1958-5608				APR	2020	37	2					217	226		10.18280/ts.370207													
J								Prediction of Graphic Interaction Time of Virtual Reality System Based on Improved Fitts' Law	TRAITEMENT DU SIGNAL										virtual reality (VR); human computer interaction (HCI); Fitts' law; arbitrary shape		This paper attempts to evaluate the click efficiency of different graphic designs of the virtual reality (VR) system for shipbuilding in a shipyard. For this purpose, a prediction method for the completion time of pointing tasks in a VR was proposed based on the probabilistic Fitts' law, and a selection model was constructed for targets in arbitrary shape in VR. According to the design requirements of VR interfaces, the authors considered the influence of target shape on task completion time, constructed the relationship between hit probability and the index of difficulty (ID) of the task, and took the target center as the center point of the function to be integrated, thus defining the probabilistic Fitts' model in VR scenes. Next, Experiment 1 was designed to compute the constant terms of probability function P(HIT) in the improved probabilistic Fitts' model; Experiment 2 was designed to calculate the constant terms of prediction function in the improved model. Int his way, the improved probabilistic Fitts' model was completed. Finally, our model was validated and evaluated by the actual pointing task of the shipbuilding VR system of a shipyard. The results show that our model can predict the task completion time well under VR scenes. The research provides effective guidance for designers to optimize the interface layout in a VR environment, and optimize the user experience of interface interaction.																	0765-0019	1958-5608				APR	2020	37	2					227	234		10.18280/ts.370208													
J								Automatic Detection of Schizophrenia by Applying Deep Learning over Spectrogram Images of EEG Signals	TRAITEMENT DU SIGNAL										schizophrenia; CNN; deep learning; spectrogram		This study presents a method that aims to automatically diagnose Schizophrenia (SZ) patients by using EEG recordings. Unlike many literature studies, the proposed method does not manually extract features from EEG recordings, instead it transforms the raw EEG into 2D by using Short-time Fourier Transform (STFT) in order to have a useful representation of frequency-time features. This work is the first in the relevant literature in using 2D time-frequency features for the purpose of automatic diagnosis of SZ patients. In order to extract most useful features out of all present in the 2D space and classify samples with high accuracy, a state-of-art Convolutional Neural Network architecture, namely VGG-16, is trained. The experimental results show that the method presented in the paper is successful in the task of classifying SZ patients and healthy controls with a classification accuracy of 95% and 97% in two datasets of different age groups. With this performance, the proposed method outperforms most of the literature methods. The experiments of the study also reveal that there is a relationship between frequency components of an EEG recording and the SZ disease. Moreover, Grad-CAM images presented in the paper clearly show that mid-level frequency components matter more while discriminating a SZ patient from a healthy control.																	0765-0019	1958-5608				APR	2020	37	2					235	244		10.18280/ts.370209													
J								Satellite Image Enhancement Using an Ameliorated Balance Contrast Enhancement Technique	TRAITEMENT DU SIGNAL										ABCETP; contrast enhancement; image enhancement; satellite imaging	CUCKOO SEARCH ALGORITHM; KNEE TRANSFER-FUNCTION; BRIGHTNESS ENHANCEMENT; GAMMA CORRECTION; HISTOGRAM EQUALIZATION; QUALITY ASSESSMENT	The quality of satellite images is frequently degraded by the low-contrast effect. Therefore, different research works have been developed to deal with this undesirable effect. Still, no conclusive verdicts have been reached and the problem remains open for research. Hence, an ameliorated balance contrast enhancement technique using a parabolic function (ABCETP) is proposed in this article to improve the quality in terms of brightness, contrast, and colors. The original BCETP works by determining a parabolic function with three coefficients. However, the proposed ABCETP works by computing a modified parabolic function with one modified and two unmodified coefficients, as well as, it utilizes two additional methods to produce adequate-quality results. The proposed ABCETP is examined with numerous real contrast-distorted images, compared against different contrast enhancement methods and the results are assessed with two specialized quality appraisal metrics. Empirical results obtained from conducting various comparisons and experiments revealed the favorability of ABCETP, wherein it outputted images with better-perceived quality and outperformed the comparative methods in several aspects.																	0765-0019	1958-5608				APR	2020	37	2					245	254		10.18280/ts.370210													
J								Fusion Between Shape Prior and Graph Cut for Vehicle Image Segmentation	TRAITEMENT DU SIGNAL										shape prior; graph cut; image segmentation; vehicle images	NORMALIZED CUTS	In vehicle image segmentation, the traditional graph cut algorithm often have errors, when the original image contains shadows or complex background. To overcome the errors, this paper introduces the shape prior to graph cut algorithm. Our algorithm firstly maps the vehicle image to a weighted undirected graph, and obtains the regional energy and boundary energy functions. Then, the shape prior was added to constrain the image segmentation, and create a new energy function. The star convex was selected as the shape prior, and manually marked. Experimental results show that our algorithm segmented complex background vehicle images, shadowed vehicle images, and incomplete vehicle images more effectively than Otsu's method and graph cut algorithm. The details of vehicles were preserved and extracted efficiently by our algorithm. The research results provide new insights into vehicle image segmentation.																	0765-0019	1958-5608				APR	2020	37	2					255	262		10.18280/ts.370211													
J								Muscle Noise Cancellation from ECG Signal Using Self Correcting Leaky Normalized Least Mean Square Adaptive Filter under Varied Step Size and Leakage Coefficient	TRAITEMENT DU SIGNAL										ECG signal; EMG noise; noise canceller; step size; leakage coefficient; normalized least square; self correcting filter	ALGORITHM	The electrocardiogram (ECG) signal is exposed to many types of noise due to its sensitivity, especially those high-frequency noises, the most important of which is the electromyogram (EMG) noise whose spectrum overlaps with the spectrum of the ECG signal, which impedes the correct diagnosis of heart diseases. In this paper, a new adaptive filter for EMG noise removing from corrupted ECG signal is proposed; the filter is based on self correcting leaky normalized least mean square algorithm SC-LNLMS with varied step size and varied leakage coefficient, the corrupted signal passes through noise canceller filter based on leaky normalized least square algorithm using multiple stages and under adjusting both step size and leakage coefficient. Testing was performed using noise free ECG signals from the MIT-BIH Arrhythmia database and the noise from Noise Stress Test database (nstdb). The experimental results show that the proposed denoising filter achieves better output signal-to-noise ratio (SNR), improvement SNR and lower mean square error (MSE) when compared to other existing techniques. besides preserving the original shape of the signal without causing distortions in the low-amplitude. Using the proposed denoising filter, the average output SNR varies from 16.53 to 28.56 dB, and the MSE less than 0.00000045.																	0765-0019	1958-5608				APR	2020	37	2					263	269		10.18280/ts.370212													
J								An Improved Semantic Segmentation Method for Remote Sensing Images Based on Neural Network	TRAITEMENT DU SIGNAL										remote sensing images; pixel-level method; residual network (ResNet); dilated spatial pyramid pooling (SPP); sub pixel up-sampling; semantic segmentation		Traditional semantic segmentation methods cannot accurately classify high-resolution remote sensing images, due to the difficulty in acquiring the correlations between geophysical objects in these images. To solve the problem, this paper proposes an improved semantic segmentation method for remote sensing images based on neural network. Based on residual network, the proposed algorithm changes the dilated convolution kernels in the dilated spatial pyramid pooling (SPP) module before extracting the correlations between geophysical objects, thus improving the accuracy of segmentation. Next, the high resolution of the input image was maintained through deconvolution, and the semantic segmentation was realized by the pixel-level method. To enhance the robustness of our algorithm, the dataset was expanded through random cropping and stitching of images. Finally, our algorithm was trained and tested on the Potsdam dataset provided by the International Society for Photogrammetry and Remote Sensing (ISPRS). The results show that our algorithm was 1.4% more accurate than the DeepLab v3 Plus. The research results shed new light on the semantic segmentation of high-resolution remote sensing images.																	0765-0019	1958-5608				APR	2020	37	2					271	278		10.18280/ts.370213													
J								Comparison of Classification Models Using Entropy Based Features from Sub-bands of EEG	TRAITEMENT DU SIGNAL										EEG classification; approximate entropy; sample entropy; fuzzy approximate entropy; random forest; AdaBoost; gradient boosting; naive Bayes; linear discriminant analysis; quadratic discriminant analysis	EPILEPTIC SEIZURE; APPROXIMATE ENTROPY; ELECTROENCEPHALOGRAM; TRANSFORM	The purpose of this study is to distinguish between different epileptic states automatically in an EEG. The work focuses on distinguishing activity of a controlled patient from interictal and ictal activity and also from each other. Publically available Bonn database is used in this study. Seven such cases are considered. For this study three entropy features: approximate entropy, sample entropy and fuzzy approximate entropy are extracted from frequency sub-bands and are used with six classification algorithms which are Naive Bayes, LDA (Linear Discriminant Analysis), QDA(Quadratic Discriminant Analysis) from the generative group and RF(Random Forest), GB(Gradient Boosting) and Ada Boost from the ensemble group. The performance is evaluated on basis ofClassification accuracy, Sensitivity and Specificity. The results obtained direct that LDA as a classifier from the generative class and Ada boost from the ensemble group has outperformed other classifiers achieving the highest classification accuracies for three cases each respectively. Evaluating the results from sub-bands, we find out that D2 (21.7-43.4 Hz) sub-band clearly outperformed all the bands. Among the entropies used as features from sub bands, sample entropy outperforms the other entropies. From the results obtained it is established that frequency features from higher sub-band such as D2 (21.7-43.4 Hz) contain substantial information which can be used for identification of epileptic discharges which are however missed during visual analysis. This shows the impact automated methods can make in the field of identification of ictal and inter-ictal activity.																	0765-0019	1958-5608				APR	2020	37	2					279	289		10.18280/ts.370214													
J								Brain Tumor Diagnosis in MRI Images Using Image Processing Techniques and Pixel-Based Clustering	TRAITEMENT DU SIGNAL										brain tumor; super pixel; spectral clustering; filter Gabor	SEGMENTATION	Brain diseases are common causes of death and burns such as cancerous tumors. Nowadays, the use of automated computer techniques is quite common for faster extraction and better identification of tumor locations. The present study examines the diagnosis of brain tumors in MRI imaging through a super pixel-based clustering technique. In the proposed method, additional regions of MRI images were removed by pre-processing operations to eliminate noise and skull removal to increase the speed of tumor detection. Then, the super pixels were calculated by dividing the image into even blocks. Spectral clustering was performed on the ROI containing the tumor tissue information. Finally, adjacent blocks were identified by Filter Gabor to identify brain tumors in MRI images. Based on the results, the proposed method has shown better performance in terms of accuracy, sensitivity, and specificity in comparison to other methods. The function of brain tumor diagnosis can be useful in helping physicians identify more rapidly.																	0765-0019	1958-5608				APR	2020	37	2					291	300		10.18280/ts.370215													
J								Key Technologies for Dynamic Imaging of Disaster-Causing Concealed Water Bodies in Underground Coalmines Based on Transient Electromagnetic Method	TRAITEMENT DU SIGNAL										underground coal mine; high power; transient electromagnetic method (TEM); dynamic imaging	DEPTH	The concealed water bodies are potential sources of disasters in underground coalmines However, it is difficult to detect these water bodies in an accurate manner To overcome the difficulty, this paper designs a high-power imaging system for transient electromagnetic method (TEM) in coalmines Specifically, the shutdown period of emission current was reduced to 12 mu s with high-current reverse clamp inductance technology; the secondary field signals were collected and the background noise was reduced to 1 mu V through equivalent sampling, segmented amplification, and superimposed denoising; the least squares imaging and attitude angle was fused for the first time to realize the rapid imaging of the disaster-causing concealed water bodies in front of the heading face in underground coalmines The proposed system was applied to detect and continuously track the water-rich worked-out sections on the same layer and on the roof, as well as the water in collapse columns in the excavation direction and in the heading face. The results show that our system can accurately detect 74.1% of disaster-causing concealed water bodies. This research provides new insights into the prevention and control of water disasters in underground coalmines.																	0765-0019	1958-5608				APR	2020	37	2					301	306		10.18280/ts.370216													
J								Cross-Recurrence Plots and Quantification of Glottal Signal for Pathological Voice Assessment	TRAITEMENT DU SIGNAL										assessment; cross recurrence quantification analysis; glottal signal; vocal folds	CLASSIFICATION	Non-linear signal processing techniques know an extensively challenge use in the field of pathological voice diagnosis and assessment. This paper contributes the application of an important tool, Cross Recurrence Quantification Analysis (CRQA) for showing how it can be adapted to compare, assess and quantify the articulation changes of Vocal Folds (VFs) after a voice therapy. To achieve this aim, four types of pathological voice: VFs paralysis, VFs nodules, VFs polyps and spasmodic dysphonia have been considered in which the normal voice was considered as a reference. The glottal signal was firstly extracted using an inverse filtering algorithm, in the next step; the CRQA was performed on the Cross Recurrent Plot (CRP) structure of the glottal signal. Eight (08) CRQA measures were extracted to evaluate the improvement degree of voice quality after a voice therapy operation. By comparing these parameters variation before and after therapy to the normophonic voices, pathological voice stages were easily discriminated. For the test validation, a local collection database has been recorded in three different Algerian EarNose-Throat services, where the long vowel [a:] was selected. A number of 60 pathological samples was achieved of the two genders adults male and female uttered in different stages before and after voice therapy. The obtained results shown the effectiveness of CRQA applied to the glottal signal. Indeed, it presented an effective tool to assess the improvement of voice quality after therapy.																	0765-0019	1958-5608				APR	2020	37	2					307	317		10.18280/ts.370217													
J								An Automatic Method for Unsupervised Feature Selection of Hyperspectral Images Based on Fuzzy Clustering of Bands	TRAITEMENT DU SIGNAL										hyperspectral classification; band selection; statistical attributes; fuzzy c-means clustering; virtual dimensionality; principal component analysis	FEATURE-EXTRACTION; TRANSFORMATION; REDUCTION	Hyperspectral sensors collect spectral data in numerous adjacent spectral bands which are usually redundant and cause some challenges such as Hughes phenomenon. In this study, an automatic unsupervised method is presented for feature selection from hyperspectral images. To do so, a new statistical feature space is introduced in which each band is regarded as a sample point. This feature space is originated from the statistical attributes of image bands while these attributes are extracted from different partitions of the entire image. A fuzzy clustering of bands, performed in the PCA transformed space of previously mentioned statistical feature space, leads to band clusters with similar characteristics. The proposed band selection technique chooses a representative band in each cluster and removes the other redundant ones. The proposed method is investigated in terms of the classification accuracy of the Pavia University hyperspectral image. Obtained results, which are compared to two recent states of art methods, prove its efficiency.																	0765-0019	1958-5608				APR	2020	37	2					319	324		10.18280/ts.370218													
J								Depth Perception in a Single RGB Camera Using Body Dimensions and Centroid Property	TRAITEMENT DU SIGNAL										stereo imaging; anthropometric; perspective errors; body dimensions; centroid; surveillance; vision		Infrastructure Supervision is a compelling need for buildings and open areas. It is facilitated through the joint use of stereo vision cameras, techniques and algorithms. This Stereoscopic assessment helps monitoring systems to reconstruct people's visible surface and also provides a robust estimation of the position and posture of the person that allows 3D scene activities and interactions. In practice, in occluded fields, the correspondence between pixels and pixels interferes with the flow of data in surveillance. Structured light ToF imaging and Light Field imaging sensors came into being considering the restriction. These techniques, however failed in addressing the inaccuracies and noise introduced in the phase of profound capture. Based on the Human Anthropometric research, we suggested a technique for estimating the depth of an individual from a single RGB camera. As we deal with moving objects in a scene, also consideration is given to centroid ownership. The system is trained by feeding stature, body width and centroid as inputs to estimate a person's actual height using gradient boosting model. And a person's further anticipated height and actual height are used to predict distance. After taking actual depth (camera to person distance) and real height as ground truth, the suggested model is validated and it is inferred that the camera to person distance anticipated (Pred(dist)) from estimated Real height is 95% correlated with actual Camera to Person distance (or depth) at a confidence level of 99.9% with RMSE of 0.092.																	0765-0019	1958-5608				APR	2020	37	2					333	340		10.18280/ts.370220													
J								A Crop Disease Image Recognition Algorithm Based on Feature Extraction and Image Segmentation	TRAITEMENT DU SIGNAL										image recognition; image segmentation; feature extraction; crop diseases		Due to the complex background of the field, it is a highly complex and flexible task to recognize and diagnose diseases from crop images. Image processing and machine vision can adapt to the complex and changing natural scenes, laying the basis for recognition and diagnosis of crop diseases. This paper designs and verifies an image segmentation method and a disease recognition method for crop disease images under complex background. The segmentation method was developed by improving graph-cut segmentation algorithm with saliency map and excess-green method, while the recognition method was designed based on a single hidden-layer forward neural network (NN). Experimental results show that our segmentation method outperformed he traditional graph-cut algorithm, and fuzzy c-means (FCM) clustering in segmenting the fine-grained disease images, and that our recognition method could accurately identify typical leaf diseases with high stability. The research results provide a good reference for the application of image processing and machine vision in disease image processing.																	0765-0019	1958-5608				APR	2020	37	2					341	346		10.18280/ts.370221													
J								A branch & bound algorithm to determine optimal cross-splits for decision tree induction	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Branch & bound; Decision trees; Classification; Cross-splits	DIAGNOSIS; SYSTEM	State-of-the-art decision tree algorithms are top-down induction heuristics which greedily partition the attribute space by iteratively choosing the best split on an individual attribute. Despite their attractive performance in terms of runtime, simple examples, such as the XOR-Problem, point out that these heuristics often fail to find the best classification rules if there are strong interactions between two or more attributes from the given datasets. In this context, we present a branch and bound based decision tree algorithm to identify optimal bivariate axis-aligned splits according to a given impurity measure. In contrast to a univariate split that can be found in linear time, such an optimal cross-split has to consider every combination of values for every possible selection of pairs of attributes which leads to a combinatorial optimization problem that is quadratic in the number of values and attributes. To overcome this complexity, we use a branch and bound procedure, a well known technique from combinatorial optimization, to divide the solution space into several sets and to detect the optimal cross-splits in a short amount of time. These cross splits can either be used directly to construct quaternary decision trees or they can be used to select only the better one of the individual splits. In the latter case, the outcome is a binary decision tree with a certain sense of foresight for correlated attributes. We test both of these variants on various datasets of the UCI Machine Learning Repository and show that cross-splits can consistently produce smaller decision trees than state-of-the-art methods with comparable accuracy. In some cases, our algorithm produces considerably more accurate trees due to the ability of drawing more elaborate decisions than univariate induction algorithms.																	1012-2443	1573-7470				APR	2020	88	4					291	311		10.1007/s10472-019-09684-0													
J								Sufficient conditions for the existence of a sample mean of time series under dynamic time warping	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Time series; Dynamic time warping; Frechet function; Sample mean	LEARNING VECTOR QUANTIZATION; SELF-ORGANIZING MAPS; AVERAGING METHOD; CLASSIFICATION; MANIFOLDS	Time series averaging is an important subroutine for several time series data mining tasks. The most successful approaches formulate the problem of time series averaging as an optimization problem based on the dynamic time warping (DTW) distance. The existence of an optimal solution, called sample mean, is an open problem for more than four decades. Its existence is a necessary prerequisite to formulate exact algorithms, to derive complexity results, and to study statistical consistency. In this article, we propose sufficient conditions for the existence of a sample mean. A key result for deriving the proposed sufficient conditions is the Reduction Theorem that provides an upper bound for the minimum length of a sample mean.																	1012-2443	1573-7470				APR	2020	88	4					313	346		10.1007/s10472-019-09682-2													
J								Multiple k -opt evaluation multiple k -opt moves with GPU high performance local search to large-scale traveling salesman problems	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Parallel 2-opt; Massive 2-opt moves; Parallel 3-opt; Massive 3-opt moves; TSP; GPU; High performance GPU local search; 68	ALGORITHM	The 2-opt, 3-opt or k-opt heuristics are classical local search algorithms for traveling salesman problems (TSP) in combinatorial optimization area, while sequential k-opt complete neighborhood examination takes polynomial time complexity which is timeconsuming to approach large scale TSP instances. This paper introduces a reasonable methodology called "multiple k-opt evaluation, multiple k-opt moves" that allows to simultaneously execute, without interference, massive k -opt moves that are globally found on the same TSP tour, as well as keep high performance GPU (Graphics Processing Unit) parallel 2-/3-opt evaluation with characteristic of "data parallelism, decentralized control and O(1) local memory for each GPU thread". The methodology is reasonable since intervention of a sequential O(N) time complexity tour reversal operation is unavoidable for each k -opt move when using array of ordered coordinates as TSP tour data structure for high performance GPU k -opt local search that considers coalesced memory access and usage of limited on-chip shared memory. Innovation work includes two parts, a sequential non-interacted k-opt moves' set partition algorithm that takes linear time complexity; a new TSP tour representation, array of ordered coordinates-index, that unveils how to combine the advantages of using doubly linked list and array of ordered coordinates data structures for iterative parallel k -opt local search based on GPU CUDA. We test this methodology on 22 national TSP instances with up to 71009 cities and with brute initial tour solution. Average maximum 997 non-interacted 2-opt moves are found and executed on the same tour of ch71009.tsp instance after one iteration of complete N*(N-1)2 2-opt checks working in parallel on GPU. And the proposed iterative GPU parallel 2-opt methodology executes average 306631 2-opt moves while only iterates 786 tour reversal operations, in comparison with methods that have to execute tour reversal operation after each 2-opt move. Experimental comparisons show that our proposed methodology gets huge acceleration over both classical sequential and a current state-of-the-art GPU parallel 2-opt implementation.																	1012-2443	1573-7470				APR	2020	88	4					347	365		10.1007/s10472-019-09679-x													
J								Orientation Measurement for Objects with Planar Surface Based on Monocular Microscopic Vision	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Microscopic vision; micro assembly; orientation measurement; optimization; analytical computation	POSE ESTIMATION; SYSTEM	Orientation measurement of objects is vital in micro assembly. In this paper, we present a novel method based on monocular microscopic vision for 3-D orientation measurement of objects with planar surfaces. The proposed methods aim to measure the orientation of the object, which does not require calibrating the intrinsic parameters of microscopic camera. In our methods, the orientation of the object is firstly measured with analytical computation based on feature points. The results of the analytical computation are coarse because the information about feature points is not fully used. In order to improve the precision, the orientation measurement is converted into an optimization process base on the relationship between deviations in image space and in Cartesian space under microscopic vision. The results of the analytical computation are used as the initial values of the optimization process. The optimized variables are the three rotational angles of the object and the pixel equivalent coefficient. The objective of the optimization process is to minimize the coordinates differences of the feature points on the object. The precision of the orientation measurement is boosted effectively. Experimental and comparative results validate the effectiveness of the proposed methods.																	1476-8186	1751-8520				APR	2020	17	2					247	256		10.1007/s11633-019-1202-y													
J								No-reference mesh visual quality assessment via ensemble of convolutional neural networks and compact multi-linear pooling	PATTERN RECOGNITION										Blind mesh quality assessment; Convolutional neural network; Fine-tuning; Compact multi-linear pooling; Visual saliency	ERROR; METRICS; COMPRESSION; ATTENTION; MODEL	Blind or No reference quality evaluation is a challenging issue since it is done without access to the original content. In this work, we propose a method based on deep learning for the mesh visual quality assessment without reference. For a given 3D model, we first compute its mesh saliency. Then, we extract views from the 3D mesh and the corresponding mesh saliency. After that, the views are split into small patches that are filtered using a saliency threshold. Only the salient patches are selected and used as input data. After that, three pre-trained deep convolutional neural networks are employed for feature learning: VGG, AlexNet, and ResNet. Each network is fine-tuned and produces a feature vector. The Compact Multi-linear Pooling (CMP) is used afterward to fuse the retrieved vectors into a global feature representation. Finally, fully connected layers followed by a regression module are used to estimate the quality score. Extensive experiments are executed on four mesh quality datasets and comparisons with existing methods demonstrate the effectiveness of our method in terms of correlation with subjective scores. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107174	10.1016/j.patcog.2019.107174													
J								CARs-Lands: An associative classifier for large-scale datasets	PATTERN RECOGNITION										Classification association rules (CARs); Associative classifier; Big data; Large-scale datasets; Evolutionary algorithms	BIG DATA; ALGORITHM; RULES; RECOGNITION	Associative classifiers are one of the most efficient classifiers for large datasets. However, they are unsuitable to be directly used in large-scale data problems. Associative classifiers discover frequent/rare rules or both in order to produce an efficient classifier. Discovery rules need to explore a large solution space in a well-organized manner; hence, learning of the associative classification methods of large datasets is not suitable on large-scale datasets because of memory and time-complexity constraints. The proposed method, CARs-Lands, presents an efficient distributed associative classifier. In CARs-Lands, first, a modified dataset is generated. This new dataset has sub-datasets that are completely appropriate to produce classification association rules (CARs) in a parallel manner. The produced dataset by CARs-Lands contains two types of instances: main instances and neighbor instances. Main instances can be either real instances of training dataset or meta-instances, which are not in the training dataset; each main instance has several neighbor instances from the training dataset, which together form a sub-dataset. These sub-datasets are used for parallel local association rule mining. In CARs-Lands, local association rules lead to more accurate prediction, because each test instance is classified by the association rules of their nearest neighbors in the training datasets. The proposed approach is evaluated in terms of accuracy on six real-world large-scale datasets against five recent and well-known methods. Experiment results show that the proposed classification method has high prediction accuracy and is highly competitive when compared to other classification methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107128	10.1016/j.patcog.2019.107128													
J								A repeatable and robust local reference frame for 3D surface matching	PATTERN RECOGNITION										Local reference frame; 3D Surface description; Feature transformation; Local characteristics; Scale strategy	OBJECT RECOGNITION; PERFORMANCE EVALUATION; REGISTRATION; REPRESENTATION; SIGNATURES; ALGORITHM; FEATURES	Local reference frames (LRFs) have been widely used for 3D local surface description. In this work, we propose a repeatable LRF with strong robustness to different nuisances. Different from existing LRF methods, the proposed LRF uses a part of neighboring points within the support region to calculate the z-axis, and performs an effective feature transformation on the neighboring points to define the x-axis. Specifically, feature transformation is applied to the data on a projection plane based on three point distribution characteristics via weighted strategies. These characteristics include the z-height, the distance to the center and the average length to 1-ring neighbors, covariance analysis is then applied to the transformed points to obtain the eigenvector with the largest eigenvalue, which points towards the maximum variance direction. Using a sign disambiguation technique, the modified eigenvector is used to define the final x-axis. Furthermore, a scale strategy is proposed to improve the robustness of the LRF with respect to mesh decimation. The proposed LRF was rigorously tested on six public benchmark datasets consisting of three different application contexts, i.e., 3D shape retrieval, 3D object recognition and registration. Experiments show that our method achieves significantly higher repeatability and stronger robustness than the state-of-the-arts under Gaussian noise, shot noise and mesh resolution variation. Finally, the descriptor matching results on four typical datasets further demonstrate the effectiveness of our LRF. (C) 2019 Published by Elsevier Ltd.																	0031-3203	1873-5142				APR	2020	100								107186	10.1016/j.patcog.2019.107186													
J								A robust statistics approach for plane detection in unorganized point clouds	PATTERN RECOGNITION										Plane detection; Region growing; Robust statistics; Unorganized point clouds	HOUGH TRANSFORM; RECONSTRUCTION	Plane detection is a key component for many applications, such as industrial reverse engineering and self-driving cars. However, existing plane-detection techniques are sensitive to noise and to user-defined parameters. We introduce a fast deterministic technique for plane detection in unorganized point clouds that is robust to noise and virtually independent of parameter tuning. It is based on a novel planarity test drawn from robust statistics and on a split and merge strategy. Its parameter values are automatically adjusted to fit the local distribution of samples in the input dataset, thus leading to good reconstruction of even small planar regions. We demonstrate the effectiveness of our solution on several real datasets, comparing its performance to state-of-art plane detection techniques, and showing that it achieves better accuracy, while still being one of the fastest. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107115	10.1016/j.patcog.2019.107115													
J								Connectivity-based cylinder detection in unorganized point clouds	PATTERN RECOGNITION										Cylinder detection; Unorganized point clouds; Reverse engineering; Industrial sites	RECONSTRUCTION; EXTRACTION	Cylinder detection is an important step in reverse engineering of industrial sites, as such environments often contain a large number of cylindrical pipes and tanks. However, existing techniques for cylinder detection require the specification of several parameters which are difficult to adjust because their values depend on the noise level of the input point cloud. Also, these solutions often expect the cylinders to be either parallel or perpendicular to the ground. We present a cylinder-detection technique that is robust to noise, contains parameters which require little to no fine-tuning, and can handle cylinders with arbitrary orientations. Our approach is based on a robust linear-time circle-detection algorithm that naturally discards outliers, allowing our technique to handle datasets with various density and noise levels while using a set of default parameter values. It works by projecting the point cloud onto a set of directions over the unit hemisphere and detecting circular projections formed by samples defining connected components in 3D. The extracted cylindrical surfaces are obtained by fitting a cylinder to each connected component. We compared our technique against the state-of-the-art methods on both synthetic and real datasets containing various densities and noise levels, and show that it outperforms existing techniques in terms of accuracy and robustness to noise, while still maintaining a competitive running time. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107161	10.1016/j.patcog.2019.107161													
J								Encoding features robust to unseen modes of variation with attentive long short-term memory	PATTERN RECOGNITION										Long short-term memory; Recurrent neural networks; Attention; Robust features; Modes of variation; Facial expression recognition; Human action recognition		Long short-term memory (LSTM) is a type of recurrent neural networks that is efficient for encoding spatio-temporal features in dynamic sequences. Recent work has shown that the LSTM retains information related to the mode of variation in the input dynamic sequence which reduces the discriminability of the encoded features. To encode features robust to unseen modes of variation, we devise an LSTM adaptation named attentive mode variational LSTM. The proposed attentive mode variational LSTM utilizes the concept of attention to separate the input dynamic sequence into two parts: (1) task-relevant dynamic sequence features and (2) task-irrelevant static sequence features. The task-relevant dynamic features are used to encode and emphasize the dynamics in the input sequence. The task-irrelevant static sequence features are utilized to encode the mode of variation in the input dynamic sequence. Finally, the attentive mode variational LSTM suppresses the effect of mode variation with a shared output gate and results in a spatio-temporal feature robust to unseen variations. The effectiveness of the proposed attentive mode variational LSTM has been verified using two tasks: facial expression recognition and human action recognition. Comprehensive and extensive experiments have verified that the proposed method encodes spatio-temporal features robust to variations unseen during the training. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107159	10.1016/j.patcog.2019.107159													
J								ExprADA: Adversarial domain adaptation for facial expression analysis	PATTERN RECOGNITION										Visual domain adaptation; Facial expression recognition; Adversarial learning		We propose a deep neural network based image-to-image translation for domain adaptation, which aims at finding translations between image domains. Despite recent GAN based methods showing promising results in image-to-image translation, they are prone to fail at preserving semantic information and maintaining image details during translation, which reduces their practicality on tasks such as facial expression synthesis. In this paper, we learn a framework with two training objectives: first, we propose a multidomain image synthesis model, yielding a better recognition performance compared to other GAN based methods, with a focus on the data augmentation process; second, we explore the use of domain adaptation to transform the visual appearance of the images from different domains, with the detail of face characteristics (e.g., identity) well preserved. Doing so, the expression recognition model learned from the source domain can be generalized to the translated images from target domain, without the need for re-training a model for new target domain. Extensive experiments demonstrate that ExprADA shows significant improvements in facial expression recognition accuracy compared to state-of-the-art domain adaptation methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107111	10.1016/j.patcog.2019.107111													
J								Wavelet-based segmentation on the sphere	PATTERN RECOGNITION										Image segmentation; Wavelets; Curvelets; Tight frame; Sphere	CONTINUOUS CURVELET TRANSFORM; SPARSE IMAGE-RECONSTRUCTION; SAMPLING THEOREM; MODEL; SCALE	Segmentation, a useful/powerful technique in pattern recognition, is the process of identifying object outlines within images. There are a number of efficient algorithms for segmentation in Euclidean space that depend on the variational approach and partial differential equation modelling. Wavelets have been used successfully in various problems in image processing, including segmentation, inpainting, noise removal, super-resolution image restoration, and many others. Wavelets on the sphere have been developed to solve such problems for data defined on the sphere, which arise in numerous fields such as cosmology and geophysics. In this work, we propose a wavelet-based method to segment images on the sphere, accounting for the underlying geometry of spherical data. Our method is a direct extension of the tight-frame based segmentation method used to automatically identify tube-like structures such as blood vessels in medical imaging. It is compatible with any arbitrary type of wavelet frame defined on the sphere, such as axisymmetric wavelets, directional wavelets, curvelets, and hybrid wavelet constructions. Such an approach allows the desirable properties of wavelets to be naturally inherited in the segmentation process. In particular, directional wavelets and curvelets, which were designed to efficiently capture directional signal content, provide additional advantages in segmenting images containing prominent directional and curvilinear features. We present several numerical experiments, applying our wavelet-based segmentation method, as well as the common K-means method, on real-world spherical images, including an Earth topographic map, a light probe image, solar data-sets, and spherical retina images. These experiments demonstrate the superiority of our method and show that it is capable of segmenting different kinds of spherical images, including those with prominent directional features. Moreover, our algorithm is efficient with convergence usually within a few iterations. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107081	10.1016/j.patcog.2019.107081													
J								Semi-supervised robust deep neural networks for multi-label image classification	PATTERN RECOGNITION										Multi-label classification; Semi-supervised learning; Ramp loss; Image classification; Deep learning		This paper introduces a robust method for semi-supervised training of deep neural networks for multi-label image classification. To this end, a ramp loss is utilized since it is more robust against noisy and incomplete image labels compared to the classic hinge loss. The proposed method allows for learning from both labeled and unlabeled data in a semi-supervised setting. This is achieved by propagating labels from the labeled images to their unlabeled neighbors in the feature space. Using a robust loss function becomes crucial here, as the initial label propagations may include many errors, which degrades the performance of non-robust loss functions. In contrast, the proposed robust ramp loss restricts extreme penalties from the samples with incorrect labels, and the label assignment improves in each iteration and contributes to the learning process. The proposed method achieves state-of-the-art results in semi-supervised learning experiments on the CIFAR-10 and STL-10 datasets, and comparable results to the state-of the-art in supervised learning experiments on the NUS-WIDE and MS-COCO datasets. Experimental results also verify that our proposed method is more robust against noisy image labels as expected. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107164	10.1016/j.patcog.2019.107164													
J								Noise-robust dictionary learning with slack block-Diagonal structure for face recognition	PATTERN RECOGNITION										Face recognition; Low-rank representation; Noise-robust dictionary learning; Slack block-diagonal structure	SHARED DICTIONARY; ALGORITHM	Strict '0-1' block-diagonal structure has been widely used for learning structured representation in face recognition problems. However, it is questionable and unreasonable to assume the within-class representations are the same. To circumvent this problem, in this paper, we propose a slack block-diagonal (SBD) structure for representation where the target structure matrix is dynamically updated, yet its blockdiagonal nature is preserved. Furthermore, in order to depict the noise in face images more precisely, we propose a robust dictionary learning algorithm based on mixed-noise model by utilizing the above SBD structure ((SBDL)-L-2). (SBDL)-L-2 considers that there exists two forms of noise in data which are drawn from Laplacian and Gaussion distribution, respectively. Moreover, SBD2L introduces a low-rank constraint on the representation matrix to enhance the dictionary's robustness to noise. Extensive experiments on four benchmark databases show that the proposed (SBDL)-L-2 can achieve better classification results than several state-of-the-art dictionary learning methods. (C) 2019 Published by Elsevier Ltd.																	0031-3203	1873-5142				APR	2020	100								107118	10.1016/j.patcog.2019.107118													
J								Dynamic imposter based online instance matching for person search	PATTERN RECOGNITION										Person search; Pedestrian detection; Person re-identification; Dynamic pseudo-label	REIDENTIFICATION; NETWORK	Person search aims to locate the target person matching a given query from a list of unconstrained whole images. It is a challenging task due to the unavailable bounding boxes of pedestrians, limited samples for each labeled identity and large amount of unlabeled persons in existing datasets. To address these issues, we propose a novel end-to-end learning framework for person search. The proposed framework settles pedestrian detection and person re-identification concurrently. To achieve the goal of co-learning and utilize the information of unlabeled persons, a novel yet extremely efficient Dynamic Imposter based Online Instance Matching (DI-OIM) loss is formulated. The DI-OIM loss is inspired by the observation that pedestrians appearing in the same image obviously have different identities. Thus we assign the unlabeled persons with dynamic pseudo-labels. The pseudo-labeled persons along with the labeled persons can be used to learn powerful feature representations. Experiments on CUHK-SYSU and PRW datasets demonstrate that our method outperforms other state-of-the-art algorithms. Moreover, it is superior and efficient in terms of memory capacity comparing with existing methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107120	10.1016/j.patcog.2019.107120													
J								Structured self-attention architecture for graph-level representation learning	PATTERN RECOGNITION										Neural self-attention mechanism; Graph neural networks; Graph classification	RETRIEVAL; NETWORKS; KERNEL	Recently, graph neural networks (GNNs) have shown to be effective in learning representative graph features. However, current pooling-based strategies for graph classification lack efficient utilization of graph representation information in which each node and layer have the same contribution to the output of graph-level representation. In this paper, we develop a novel architecture for extracting an effective graph representation by introducing structured multi-head self-attention in which the attention mechanism consists of three different forms, i.e., node-focused, layer-focused and graph-focused. In order to make full use of the information of graphs, the node-focused self-attention firstly aggregates neighbor node features with a scaled dot-product manner, and then the layer-focused and graph-focused self-attention serve as readout module to measure the importance of different nodes and layers to the model's output. Moreover, it is able to improve the performance on graph classification tasks by combining these two self-attention mechanisms with base node-level GNNs. The proposed Structured Self-attention Architecture is evaluated on two kinds of graph benchmarks: bioinformatics datasets and social network datasets. Extensive experiments have demonstrated superior performance improvement to existing methods on predictive accuracy. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107084107084	10.1016/j.patcog.2019.107084													
J								Combining deep generative and discriminative models for Bayesian semi-supervised learning	PATTERN RECOGNITION										Probabilistic models; Semi-supervised learning; Variational autoencoders; Predictive uncertainty		Generative models can be used for a wide range of tasks, and have the appealing ability to learn from both labelled and unlabelled data. In contrast, discriminative models cannot learn from unlabelled data, but tend to outperform their generative counterparts in supervised tasks. We develop a framework to jointly train deep generative and discriminative models, enjoying the benefits of both. The framework allows models to learn from labelled and unlabelled data, as well as naturally account for uncertainty in predictive distributions, providing the first Bayesian approach to semi-supervised learning with deep generative models. We demonstrate that our blended discriminative and generative models outperform purely generative models in both predictive performance and uncertainty calibration in a number of semi-supervised learning tasks. (C) 2019 The Authors. Published by Elsevier Ltd.																	0031-3203	1873-5142				APR	2020	100								107156	10.1016/j.patcog.2019.107156													
J								Deformable face net for pose invariant face recognition	PATTERN RECOGNITION										Pose-invariant face recognition; Displacement consistency loss; Pose-triplet loss		Unconstrained face recognition still remains a challenging task due to various factors such as pose, expression, illumination, partial occlusion, etc. In particular, the most significant appearance variations are stemmed from poses which leads to severe performance degeneration. In this paper, we propose a novel Deformable Face Net (DFN) to handle the pose variations for face recognition. The deformable convolution module attempts to simultaneously learn face recognition oriented alignment and identity-preserving feature extraction. The displacement consistency loss (DCL) is proposed as a regularization term to enforce the learnt displacement fields for aligning faces to be locally consistent both in the orientation and amplitude since faces possess strong structure. Moreover, the identity consistency loss (ICL) and the pose-triplet loss (PTL) are designed to minimize the intra-class feature variation caused by different poses and maximize the inter-class feature distance under the same poses. The proposed DFN can effectively handle pose invariant face recognition (PIFR). Extensive experiments show that the proposed DFN outperforms the state-of-the-art methods, especially on the datasets with large poses. (C) 2019 Published by Elsevier Ltd.																	0031-3203	1873-5142				APR	2020	100								107113	10.1016/j.patcog.2019.107113													
J								Active emulation of computer codes with Gaussian processes Application to remote sensing	PATTERN RECOGNITION										Active learning; Gaussian process; Emulation; Design of experiments; Computer code; Remote sensing; Radiative transfer model	EXPERIMENTAL-DESIGN; SEQUENTIAL DESIGNS; SEGMENTATION; MODELS	Many fields of science and engineering rely on running simulations with complex and computationally expensive models to understand the involved processes in the system of interest. Nevertheless, the high cost involved hamper reliable and exhaustive simulations. Very often such codes incorporate heuristics that ironically make them less tractable and transparent. This paper introduces an active learning methodology for adaptively constructing surrogate models, i.e. emulators, of such costly computer codes in a multi-output setting. The proposed technique is sequential and adaptive, and is based on the optimization of a suitable acquisition function. It aims to achieve accurate approximations, model tractability, as well as compact and expressive simulated datasets. In order to achieve this, the proposed Active Multi-Output Gaussian Process Emulator (AMOGAPE) combines the predictive capacity of Gaussian Processes (GPs) with the design of an acquisition function that favors sampling in low density and fluctuating regions of the approximation functions. Comparing different acquisition functions, we illustrate the promising performance of the method for the construction of emulators with toy examples, as well as for a widely used remote sensing transfer code. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107103	10.1016/j.patcog.2019.107103													
J								Transfer learning-based discriminative correlation filter for visual tracking	PATTERN RECOGNITION										Visual tracking; Discriminative correlation filter; Instance-Transfer; Probability-Transfer		Most Correlation Filter (CF)-based tracking methods can hardly handle occlusion or severe deformation, due to the lack of effective utilization of previous target information. To overcome this, we propose a novel Transfer Learning-based Discriminative Correlation Filter (TLDCF), which extracts knowledge from multiple previous tracking tasks and applies the knowledge for a new tracking task through Instance-Transfer Learning (ITL) and Probability-Transfer Learning (PTL). ITL applies knowledge of Gaussian Mixture Modelling (GMM) target representations and multi-channel filters learned in previous frames to directly train a new correlation filter. This improves the robustness of tracker for heavy occlusion and large appearance variations. Meanwhile, PTL encodes the spatio-temporal relationship predicted by Kalman Filter (KF) into a shared Gaussian prior to suppress huge location drift caused by similar targets. For optimization, we develop an efficient Alternating Direction Method of Multipliers (ADMM) based algorithm to calculate CFs on each independent channel in real time. Extensive experiments on OTB-2013 and OTB-2015 datasets well demonstrate the effectiveness of the proposed method. In particular, our method improves AUC score of the two datasets by 5.5% and 3.9% respectively compared to baseline, and achieves competitive performance against recent state-of-the-art deep trackers. (C) 2019 The Author(s). Published by Elsevier Ltd.																	0031-3203	1873-5142				APR	2020	100								107157	10.1016/j.patcog.2019.107157													
J								Automatic processing of Historical Arabic Documents: A comprehensive Survey	PATTERN RECOGNITION										Historical Arabic Documents; Writer identification; Data retrieval; Text analysis; Text recognition; Survey on Historical Arabic Documents	LAYOUT ANALYSIS; SEGMENTATION; EXTRACTION; OPTIMIZATION; RECOGNITION; ALGORITHM	Nowadays, there is a huge amount of Historical Arabic Documents (HAD) in the national libraries and archives around the world. Analyzing this type of data manually is a difficult and costly task. Thus, an automatic process is required to exploit these documents more rapidly. Processing historical documents is a recent research subject that has seen a remarkable growth in the last years. Processing Historical Arabic Documents is a particularly challenging problem. First, due to complicated nature of Arabic script compared to other scripts and second because the documents are ancient. This paper focuses on this difficult problem and provides a comprehensive survey of existing research work. First, we describe in detail the challenges making the automatic processing of Historical Arabic Documents a difficult task. Second, we classify this task into four applications of automatic processing of HAD: i) Analyze the document to extract the main text ii) Identify the writer of the document iii) Recognize some words or parts of the document in a reference dataset and iv) Retrieve and extract specific data from the document. For each application, existing approaches are surveyed and qualitatively described. Finally, we focus on available datasets and describe how they can be used in each application. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107144	10.1016/j.patcog.2019.107144													
J								Transferable heterogeneous feature subspace learning for JPEG mismatched steganalysis	PATTERN RECOGNITION										Mismatched steganalysis; Heterogeneous subspace; Domain-independent features; Domain-related features; Transfer learning	DOMAIN ADAPTATION; ALGORITHM	Steganalysis is a technique that detects the presence of secret information in multimedia data. Many steganalysis algorithms have been proposed with high detection accuracy; however, the difference in statistical distribution between training and testing sets can cause mismatch problems, which will degrade the performance of traditional steganalysis algorithms. To solve this problem, we propose a transferable heterogeneous feature subspace learning (THFSL) algorithm for JPEG mismatched steganalysis. Our approach considers the feature space in each domain as a combination of the domain-independent features and the domain-related features. We use the transformation matrix to transfer both the domain-independent and domain-related features from the source and target domains to a common feature subspace, where each target sample can be better represented by a combination of source samples. By imposing low-rank constraints on the domain-independent features, the structures of data can be preserved, which can capture the intrinsic structures for discriminating cover and stego images. Our method can avoid a potentially negative transfer by using a sparse matrix to model the domain-related features and, thus, is more robust to different domain changes in mismatched steganalysis. Extensive experiments on various mismatched steganalysis tasks show the superiority of the proposed method over the state-of-the art methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107105	10.1016/j.patcog.2019.107105													
J								AI-GAN: Asynchronous interactive generative adversarial network for single image rain removal	PATTERN RECOGNITION										Feature-wise disentanglement; Asynchronous and interactive; Single image deraining; Complementary adversarial training	STREAKS; DEPTH	Single image rain removal plays an important role in numerous multimedia applications. Existing algorithms usually tackle the deraining problem by the way of signal removal, which lead to over-smoothness and generate unexpected artifacts in de-rained images. This paper addresses the deraining problem from a completely different perspective of feature-wise disentanglement, and introduces the interactions and constraints between two disentangled latent spaces. Specifically, we propose an Asynchronous Interactive Generative Adversarial Network (AI-GAN) to progressively disentangle the rainy image into background and rain spaces in feature level through a two-branch structure. Each branch employs a two-stage synthesis strategy and interacts asynchronously by exchanging feed-forward information and sharing feedback gradients, achieving complementary adversarial optimization. This 'adversarial' is not only the 'adversarial' between the generator and the discriminator, but also means that the two generators are entangled, and interact with each other in the optimization process. Extensive experimental results demonstrate that AI-GAN outperforms state-of-the-art deraining methods and benefits various typical multimedia applications such as Image/Video Coding, Action Recognition, and Person Re-identification. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107143	10.1016/j.patcog.2019.107143													
J								A dual-cue network for multispectral photometric stereo	PATTERN RECOGNITION										Multispectral photometric stereo; Normal estimation; Deep neural networks; Networks fusion	RECONSTRUCTION; SHAPE	Estimating pixel-wise surface normal from a single image is a challenging task but offers great values to computer vision and robotics applications. By using the spectrally and spatially variant illumination, multispectral photometric stereo can produce pixel-wise surface normal from just one image. But multispectral photometric stereo methods may encounter the tangle problem of illumination, surface reflectance and camera response, which lead to an under-determined system. Existing approaches rely on either extra depth information or material calibration strategies, assuming a Lambertian surface condition which limits their application in practical systems. Previous learning-based methods employ fully-connected or CNN architectures to estimate surface normal. Compared with fully-connected framework, CNN takes advantage of the information embedded in the neighborhood of a surface point, but losing high-frequency surface normal details. In this paper, we present a new method that addresses this task by designing two stacked deep network. We first apply a CNN-based structural cue network to approximate coarse surface normal on small patches. Then, we use a pixel level fully-connected photometric cue network to further refine surface normal details and correct errors from the first step. The fused network is robust to non-Lambertian surfaces and complex illumination environments, such as ambient light and variant light directions. Experimental results show that our dual-cue fused network outperforms existing methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107162	10.1016/j.patcog.2019.107162													
J								Video super-resolution via pre-frame constrained and deep-feature enhanced sparse reconstruction	PATTERN RECOGNITION										Video super resolution; Sparse representation; Deep features; Temporal coherence	IMAGE SUPERRESOLUTION; KERNEL REGRESSION; PHOTOGRAPHY; RESOLUTION; ALGORITHM; FACES; FLASH	This paper presents a new video super-resolution (SR) method that can generate high-quality and temporally coherent high-resolution (HR) videos. Starting from the traditional sparse reconstruction framework that works well for image SR, we improve it significantly from the following aspects to obtain an effect video SR method. Firstly, to enhance the temporal coherence between adjacent HR frames, once a HR frame is estimated, we use it to guide the sparse reconstruction of the next low-resolution frame. Secondly, instead of using just hand-craft features, we further incorporate deep features generated by VGG16 into our sparse reconstruction based video SR method. Thirdly, we constantly update the dictionary, which is the core of the sparse reconstruction, by making use of the previously estimated HR frame. Finally, after the HR video is reconstructed, we use a joint bilateral filter to post-process it to remove artifacts and transfer image details. Experiments demonstrate that the proposed four strategies effectively improve our final results. In most of the experiments of this paper, our results are better than those produced by the latest deep learning based approaches. (C) 2019 Published by Elsevier Ltd.																	0031-3203	1873-5142				APR	2020	100								107139	10.1016/j.patcog.2019.107139													
J								Statistical bootstrap-based principal mode component analysis for dynamic background subtraction	PATTERN RECOGNITION										Background modeling; Video surveillance; Principal Component analysis; Statistical mode	MATRIX FACTORIZATION; TRACKING; SPARSE	Background subtraction is needed to extract foreground information from a video sequence for further processing in many applications, such as surveillance tracking. However, due to the presence of a dynamic background and noise, extracting foreground accurately from a video sequence remains challenging. A novel projection method, namely Principal Mode Component Analysis (PMCA), is proposed to capture the most repetitive patterns of a video sequence, which is one of the key characteristics of the video background. The patterns are captured by applying the bootstrapping method together with the statistic mode measure. The bootstrapping method can model the distribution of almost any statistic of the dynamic background and complicated noise. This is different from current methods, which restrict the distribution to a closed-form function. We introduce a mathematical relaxation that can formulate the statistical mode measure for a continuous video data. A fast exhaustive search method is proposed to find the global optimal solution for the PMCA. This fast method adopts a simplification procedure that makes the optimization procedure independent of the video size. The proposed method is computationally much more traceable than existing ones. We compare the proposed method with 10 different methods, including several state-of-the-art techniques, for 19 different real-world video sequences from two popular datasets. Experiment results show that the proposed method performs the best in 16 cases and second best in 2 cases. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107153	10.1016/j.patcog.2019.107153													
J								UNIC: A fast nonparametric clustering	PATTERN RECOGNITION										Cluster analysis; Hard (conventional crisp) clustering; Nonparametric algorithms; Data mining; Big data	ALGORITHM	Clustering is among the tools for exploring, analyzing, and deriving information from data. In the case of large data sets, the real burden to the application of clustering algorithms can be their complexity and demand of control parameters. We present a new fast nonparametric clustering algorithm, UNIC, to address these challenges. To identify clusters, the algorithm evaluates the distances between selected points and other points in the set. While assessing these distances, it employs methods of robust statistics to identify the cluster borders. The performance of the proposed algorithm is assessed in an experimental study and compared with several existing clustering methods over a variety of benchmark data sets. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107117	10.1016/j.patcog.2019.107117													
J								Semi-supervised cross-modal image generation with generative adversarial networks	PATTERN RECOGNITION										Multi-modality; Semi-supervised learning; Semantic networks; Generative adversarial networks; Multi-label learning	MULTIMODAL FUSION	Cross-modal image generation is an important aspect of the multi-modal learning. Existing methods usually use the semantic feature to reduce the modality gap. Although these methods have achieved notable progress, there are still some limitations: (1) they usually use single modality information to learn the semantic feature; (2) they require the training data to be paired. To overcome these problems, we propose a novel semi-supervised cross-modal image generation method, which consists of two semantic networks and one image generation network. Specifically, in the semantic networks, we use image modality to assist non-image modality for semantic feature learning by using a deep mutual learning strategy. In the image generation network, we introduce an additional discriminator to reduce the image reconstruction loss. By leveraging large amounts of unpaired data, our method can be trained in a semi-supervised manner. Extensive experiments demonstrate the effectiveness of the proposed method. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107085	10.1016/j.patcog.2019.107085													
J								Complex Contourlet-CNN for polarimetric SAR image classification	PATTERN RECOGNITION										Complex Contourlet-CNN; Multiscale deep Contourlet filter banks; Polarimetric SARimage classification	NEURAL-NETWORK; TRANSFORM; APPROXIMATION; SEGMENTATION	A complex multiscale network named complex Contourlet convolutional neural network (complex Contourlet-CNN) is proposed for polarimetric synthetic aperture radar (PolSAR) image classification in this paper. In order to make full use of the phase information of PolSAR image, we redefine the conventional operations of CNN in complex field, and the data sets and parameters are always expressed through the complex matrixes in complex CNN. Moreover, the multiscale deep Contourlet filter banks are constructed to extract more robust discriminative features with multidirection, multiscale, and multiresolution properties, which can improve the performance of complex CNN by replacing filters of the first complex convolutional layer with the multiscale deep Contourlet filter banks. The Contourlet transform is used for helping the complex CNN network to capture abstract features in a certain direction and frequency band. Furthermore, the proposed network based on the multiscale geometric properties of Contourlet transform can retrieve the information in the region and direction corresponding to the extracted features. Experiments on different spatial resolutions and land coverings of Flevoland, San Francisco Bay, and Germany PolSAR images show that less training data is required and the performance of the proposed explainable deep learning method is comparable to that of the existing state-of-the-art methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107110	10.1016/j.patcog.2019.107110													
J								Deep label refinement for age estimation	PATTERN RECOGNITION										Age estimation; Deep learning; Convolutional neural networks; Label distribution learning	REGRESSION	Age estimation of unknown persons is a challenging pattern analysis task due to the lack of training data and various ageing mechanisms for different individuals. Label distribution learning-based methods usually make distribution assumptions to simplify age estimation. However, since different genders, races and/or any other characteristics may influence facial ageing, age-label distributions are often complicated and difficult to model parametrically. In this paper, we propose a label refinery network (LRN) with two concurrent processes: label distribution refinement and slack regression refinement. The label refinery network aims to learn age-label distributions progressively in an iterative manner. In this way, we can adaptively obtain the specific age-label distributions for different facial images without making strong assumptions on the fixed distribution formulations. To further utilize the correlations among age labels, we propose a slack regression refinery to convert the age-label regression model into an age-interval regression model. Extensive experiments on three popular datasets, namely, MORPH Album2, ChaLearn15 and MegaAge-Asian, demonstrate the superiority of our method. (C) 2019 Published by Elsevier Ltd.																	0031-3203	1873-5142				APR	2020	100								107178	10.1016/j.patcog.2019.107178													
J								Correlation classifiers based on data perturbation: New formulations and algorithms	PATTERN RECOGNITION										Correlation classifiers; data perturbation; phi divergence; PMMO; Data classification	SUPPORT VECTOR MACHINE; ROBUST; REGULARIZATION	This paper develops a novel framework for a family of correlation classifiers that are reconstructed from uncertain convex programs under data perturbation. Under this framework, correlation classifiers are exploited from the pessimistic viewpoint under possible perturbation of data, and the max-min optimization problem is formulated by simplifying the original model in terms of adaptive uncertainty regions. The proposed model can be formulated as a minimization problem under proper conditions. The proximal majorization-minimization optimization (PMMO) based on Bregman divergences is devised to solve the proposed model that may be nonconvex or nonsmooth. It is found that using PMMO to solve the proposed model can exploit the convergence rate of the solution sequence in the nonconvex case. In the case of specific functions we can use the accelerated versions of first-order methods to solve the proposed model with convexity in order to make them have fast convergence rates in terms of the objective function. Extensive experiments on some data sets are conducted to demonstrate the feasibility and validity of the proposed model. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107106	10.1016/j.patcog.2019.107106													
J								Discovering influential factors in variational autoencoders	PATTERN RECOGNITION										Variational autoencoder; Mutual information; Generative model	INDEPENDENT COMPONENT ANALYSIS; REPRESENTATION; NETWORK	In the field of machine learning, it is still a critical issue to identify and supervise the learned representation without manually intervening or intuition assistance to extract useful knowledge or serve for the downstream tasks. In this work, we focus on supervising the influential factors extracted by the variational autoencoder (VAE). The VAE is proposed to learn independent low dimension representation while facing the problem that sometimes pre-set factors are ignored. We argue that the mutual information of the input and each learned factor of the representation plays a necessary indicator of discovering the influential factors. We find the VAE objective inclines to induce mutual information sparsity in factor dimension over the data intrinsic dimension and therefore result in some non-influential factors whose function on data reconstruction could be ignored. We show mutual information also influences the lower bound of VAE's reconstruction error and downstream classification task. To make such indicator applicable, we design an algorithm for calculating the mutual information for VAE and prove its consistency. Experimental results on MNIST, CelebA and DEAP datasets show that mutual information can help determine influential factors, of which some are interpretable and can be used to further generation and classification tasks, and help discover the variant that connects with emotion on DEAP dataset. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107166	10.1016/j.patcog.2019.107166													
J								MDFN: Multi-scale deep feature learning network for object detection	PATTERN RECOGNITION										Deep feature learning; Multi-scale; Semantic and contextual information; Small and occluded objects		This paper proposes an innovative object detector by leveraging deep features learned in high-level layers. Compared with features produced in earlier layers, the deep features are better at expressing semantic and contextual information. The proposed deep feature learning scheme shifts the focus from concrete features with details to abstract ones with semantic information. It considers not only individual objects and local contexts but also their relationships by building a multi-scale deep feature learning network (MDFN). MDFN efficiently detects the objects by introducing information square and cubic inception modules into the high-level layers, which employs parameter-sharing to enhance the computational efficiency. MDFN provides a multi-scale object detector by integrating multi-box, multi-scale and multi-level technologies. Although MDFN employs a simple framework with a relatively small base network (VGG-16), it achieves better or competitive detection results than those with a macro hierarchical structure that is either very deep or very wide for stronger ability of feature extraction. The proposed technique is evaluated extensively on KITTI, PASCAL VOC, and COCO datasets, which achieves the best results on KITTI and leading performance on PASCAL VOC and COCO. This study reveals that deep features provide prominent semantic information and a variety of contextual contents, which contribute to its superior performance in detecting small or occluded objects. In addition, the MDFN model is computationally efficient, making a good trade-off between the accuracy and speed. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107149	10.1016/j.patcog.2019.107149													
J								Training data independent image registration using generative adversarial networks and domain adaptation	PATTERN RECOGNITION										Registration; Domain adaptation; GANs; X-ray; MRI	REPRESENTATION	Medical image registration is an important task in automated analysis of multimodal images and temporal data involving multiple patient visits. Conventional approaches, although useful for different image types, are time consuming. Of late, deep learning (DL) based image registration methods have been proposed that outperform traditional methods in terms of accuracy and time. However, DL based methods are heavily dependent on training data and do not generalize well when presented with images of different scanners or anatomies. We present a DL based approach that can perform medical image registration of one image type despite being trained with images of a different type. This is achieved by unsupervised domain adaptation in the registration process and allows for easier application to different datasets without extensive retraining. To achieve our objective we train a network that transforms the given input image pair to a latent feature space vector using autoencoders. The resultant encoded feature space is used to generate the registered images with the help of generative adversarial networks (GANs). This feature transformation ensures greater invariance to the input image type. Experiments on chest X-ray, retinal and brain MR images show that our method, trained on one dataset gives better registration performance for other datasets, outperforming conventional methods that do not incorporate domain adaptation. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107109	10.1016/j.patcog.2019.107109													
J								Dissimilarity-based representations for one-class classification on time series	PATTERN RECOGNITION										Dissimilarity-based representations; One-class classification; Time series	SUPPORT VECTOR MACHINES; PROTOTYPE SELECTION; FAULT-DETECTION; DISCRIMINATION	In several real-world classification problems it can be impractical to collect samples from classes other than the one of interest, hence the need for classifiers trained on a single class. There is a rich literature concerning binary and multi-class time series classification but less concerning one-class learning. In this study, we investigate the little-explored one-class time series classification problem. We represent time series as vectors of dissimilarities from a set of time series referred to as prototypes. Based on this approach, we evaluate a Cartesian product of 12 dissimilarity measures, and 8 prototype methods (strategies to select prototypes). Finally, a one-class nearest neighbor classifier is used on the dissimilarity-based representations (DBR). Experimental results show that DBR are competitive overall when compared with a strong baseline on the data-sets of the UCR/UEA archive. Additionally, DBR enable dimensionality reduction, and visual exploration of data-sets. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107122	10.1016/j.patcog.2019.107122													
J								Human activity recognition from UAV-captured video sequences	PATTERN RECOGNITION										Scene stabilization; Human detection; Human activity recognition; Deep learning; Convolutional neural networks; UAV	GRADIENTS; FEATURES; TRACKING; MOTION	This research paper introduces a new approach for human activity recognition from UAV-captured video sequences. The proposed approach involves two phases: an offline phase and an inference phase. A scene stabilization step is performed together with these two phases. The offline phase aims to generate the human/non-human model as well as a human activity model using a convolutional neural network. The inference phase makes use of the already generated models in order to detect humans and recognize their activities. Our main contribution lies in adapting the convolutional neural networks, normally dedicated to the classification task, to detect humans. In addition, the classification of human activities is carried out according to two scenarios: An instant classification of video frames and an entire classification of the video sequences. Relying on an experimental evaluation of the proposed methods for human detection and human activity classification on the UCF-ARG dataset, we validated not only these contributions but also the performance of our methods compared to the existing ones. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107140	10.1016/j.patcog.2019.107140													
J								A paired sparse representation model for robust face recognition from a single sample	PATTERN RECOGNITION										Face recognition; Sparse representation-based classification; Face synthesis; Generic learning; Simultaneous sparsity; Video surveillance		Sparse representation-based classification (SRC) has been shown to achieve a high level of accuracy in face recognition (FR). However, matching faces captured in unconstrained video against a gallery with a single reference facial still per individual typically yields low accuracy. For improved robustness to intraclass variations, SRC techniques for FR have recently been extended to incorporate variational information from an external generic set into an auxiliary dictionary. Despite their success in handling linear variations, non-linear variations (e.g., pose and expressions) between probe and reference facial images cannot be accurately reconstructed with a linear combination of images in the gallery and auxiliary dictionaries because they do not share the same type of variations. In order to account for non-linear variations due to pose, a paired sparse representation model is introduced allowing for joint use of variational information and synthetic face images. The proposed model, called synthetic plus variational model, reconstructs a probe image by jointly using (1) a variational dictionary and (2) a gallery dictionary augmented with a set of synthetic images generated over a wide diversity of pose angles. The augmented gallery dictionary is then encouraged to pair the same sparsity pattern with the variational dictionary for similar pose angles by solving a newly formulated simultaneous sparsity-based optimization problem. Experimental results obtained on Chokepoint and COX-S2V datasets, using different face representations, indicate that the proposed approach can outperform state-of-the-art SRC-based methods for still-to-video FR with a single sample per person. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107129	10.1016/j.patcog.2019.107129													
J								Deep ensemble network using distance maps and body part features for skeleton based action recognition	PATTERN RECOGNITION										Human action recognition; Distance maps; Part features; Convolutional neural networks; Long short term memory	MOTION	Human action recognition is a hot research topic in the field of computer vision. The availability of low cost depth sensors in the market made the extraction of reliable skeleton maps of human objects easier. This paper proposes three subnets, referred to as SNet, TNet, and BodyNet to capture diverse spatiotemporal dynamics for action recognition task. Specifically, SNet is used to capture pose dynamics from the distance maps in the spatial domain. The second subnet (TNet) captures the temporal dynamics along the sequence. The third net (BodyNet) extracts distinct features from the fine-grained body parts in the temporal domain. With the motivation of ensemble learning, a hybrid network, referred to as HNet, is modeled using two subnets (TNet and BodyNet) to capture robust temporal dynamics. Finally, SNet and HNet are fused as one ensemble network for action classification task. Our method achieves competitive results on three widely used datasets: UTD MHAD, UT Kinect and NTU RGB+D. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107125	10.1016/j.patcog.2019.107125													
J								Patched-tube unitary transform for robust tensor completion	PATTERN RECOGNITION										Unitary transform; Patched-tubes; Robust tensor completion; Transformed tensor singular value decomposition	REMOTE-SENSING IMAGES; RECOVERY; FACTORIZATION; SPARSE	The aim of the robust tensor completion problem for third-order tensors is to recover a low-rank tensor from incomplete and/or corrupted observations. In this paper, we develop a patched-tubes unitary transform method for robust tensor completion. The proposed method is to extract similar patched-tubes to form a third-order sub-tensor, and then a transformed tensor singular value decomposition is employed to recover such low-rank incomplete and/or corrupted sub-tensor. Here the unitary transform matrix for transformed tensor singular value decomposition is constructed by using left singular vectors of the unfolding matrix arising from such sub-tensor. Moreover, we establish the perturbation results of the transformed tensor singular value decomposition for patched-tubes tensor completion. Extensive numerical experiments on hyperspectral, video and face data sets are presented to demonstrate the superior performance of the proposed patched-tubes unitary transform method over testing state-of-the-art robust tensor completion methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107181	10.1016/j.patcog.2019.107181													
J								A novel classification-selection approach for the self updating of template-based face recognition systems	PATTERN RECOGNITION										Self-update; Face recognition; Adaptive systems		The boosting on the need of security notably increased the amount of possible facial recognition applications, especially due to the success of the Internet of Things (IoT) paradigm. However, although handcrafted and deep learning-inspired facial features reached a significant level of compactness and expressive power, the facial recognition performance still suffers from intra-class variations such as ageing, facial expressions, lighting changes, and pose. These variations cannot be captured in a single acquisition and require multiple acquisitions of long duration, which are expensive and need a high level of collaboration from the users. Among others, self-update algorithms have been proposed in order to mitigate these problems. Self-updating aims to add novel templates to the users' gallery among the inputs submitted during system operations. Consequently, computational complexity and storage space tend to be among the critical requirements of these algorithms. The present paper deals with the above problems by a novel template-based self-update algorithm, able to keep over time the expressive power of a limited set of templates stored in the system database. The rationale behind the proposed approach is in the working hypothesis that a dominating mode characterises the features' distribution given the client. Therefore, the key point is to select the best templates around that mode. We propose two methods, which are tested on systems based on handcrafted features and deep-learning-inspired autoencoders at the state-of-the-art. Three benchmark data sets are used. Experimental results confirm that, by effective and compact feature sets which can support our working hypothesis, the proposed classification-selection approaches overcome the problem of manual updating and, in case, stringent computational requirements. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107121	10.1016/j.patcog.2019.107121													
J								Correlation-aware adversarial domain adaptation and generalization	PATTERN RECOGNITION										Domain adaptation; Domain generalization; Correlation-alignment; Adversarial learning		Domain adaptation (DA) and domain generalization (DG) have emerged as a solution to the domain shift problem where the distribution of the source and target data is different. The task of DG is more challenging than DA as the target data is totally unseen during the training phase in DG scenarios. The current state-of-the-art employs adversarial techniques, however, these are rarely considered for the DG problem. Furthermore, these approaches do not consider correlation alignment which has been proven highly beneficial for minimizing domain discrepancy. In this paper, we propose a correlation-aware adversarial DA and DG framework where the features of the source and target data are minimized using correlation alignment along with adversarial learning. Incorporating the correlation alignment module along with adversarial learning helps to achieve a more domain agnostic model due to the improved ability to reduce domain discrepancy with unlabeled target data more effectively. Experiments on benchmark datasets serve as evidence that our proposed method yields improved state-of-the-art performance. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107124	10.1016/j.patcog.2019.107124													
J								No-reference stereoscopic image quality assessment using a multi-task CNN and registered distortion representation	PATTERN RECOGNITION										No-reference stereoscopic image quality assessment; Multi-task learning; Convolutional neural network; Image registration	PREDICTION	Scene discrepancy between the left and right views presents more challenges for image quality assessment (IQA) of stereoscopic images as opposed to monocular ones. Existing no-reference stereoscopic IQA (NR-SIQA) metrics cannot achieve a good performance on asymmetrically distorted stereoscopic images. In this paper, we propose an NR-SIQA index that first addresses scene discrepancy by means of image registration. It then uses a registered distortion representation based on the left and registered right views to represent the distortion in the stereoscopic image. Because different distortion types influence image quality differently, a multi-task convolutional neural network (CNN) is employed to learn image quality prediction and distortion-type identification simultaneously. We first design a one-column multi-task CNN model, that learns from the registered distortion representation. Then, we extend the one-column model to a three-column model, which also learns from the left and right views. Our experimental results validate the effectiveness of the proposed registered distortion representation and multi-task CNN architecture. The proposed one- and three-column models outperform the state-of-the-art NR-SIQA metrics, especially for asymmetrically distorted stereoscopic images. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107168	10.1016/j.patcog.2019.107168													
J								A domain adaptation approach to solve inverse problems in imaging via coupled deep dictionary learning	PATTERN RECOGNITION										Dictionary learning; Deep learning; Domain adaptation; Inverse problems	SUPERRESOLUTION; REPRESENTATIONS; AUTOENCODER; ALGORITHM; SPARSE	In this work, we focus on solving four standard inverse problems in imaging - denoising, deblurring, super-resolution, and reconstruction. All these problems are usually associated with image acquisition. Traditionally, signal processing techniques are used to solve such problems. However, such techniques are computationally expensive. In many applications today, when the requirement is to compute on the edge, such expensive inversion techniques are not viable solutions. Thus, one resorts to machine learning based solutions. In the past few years, variants of neural networks were developed to 'learn' the inversion operation. Although the learning was computationally expensive, the learnt model was light-weight and portable; suitable for deployment on leaner platforms. This work is based on the same concept, however, instead of using neural networks we treat inversion as a domain adaptation problem - a transfer from corrupted space to clean space. Our work is based on the developing field of coupled representation learning. We propose a deep version of the well known coupled dictionary learning technique; but instead of learning a single layer of dictionary, we learn multiple layers. Experimental results on standard datasets against state-of-the-art solutions for each problem, show that our method yields the best results both in terms of accuracy and speed. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107163	10.1016/j.patcog.2019.107163													
J								F-measure curves: A tool to visualize classifier performance under imbalance	PATTERN RECOGNITION										Pattern classification; Class imbalance; Performance metrics; F-measure; Visualization tools; Video face recognition	FACE RECOGNITION; ADAPTIVE ENSEMBLES; SURVEILLANCE	Learning from imbalanced data is a challenging problem in many real-world machine learning applications due in part to the bias of performance in most classification systems. This bias may exist due to three reasons: (1) Classification systems are often optimized and compared using performance measurements that are unsuitable for imbalance problems; (2) most learning algorithms are designed and tested on a fixed imbalance level of data, which may differ from operational scenarios; (3) the preference of correct classification of classes is different from one application to another. This paper investigates specialized performance evaluation metrics and tools for imbalance problem, including scalar metrics that assume a given operating condition (skew level and relative preference of classes), and global evaluation curves or metrics that consider a range of operating conditions. We focus on the case in which the scalar metric F-measure is preferred over other scalar metrics, and propose a new global evaluation space for the F-measure that is analogous to the cost curves for expected cost. In this space, a classifier is represented as a curve that shows its performance over all of its decision thresholds and a range of possible imbalance levels for the desired preference of true positive rate to precision. Curves obtained in the F-measure space are compared to those of existing spaces (ROC, precision-recall and cost) and analogously to cost curves. The proposed F-measure space allows to visualize and compare classifiers' performance under different operating conditions more easily than in ROC and precision-recall spaces. This space allows us to set the optimal decision threshold of a soft classifier and to select the best classifier among a group. This space also allows to empirically improve the performance obtained with ensemble learning methods specialized for class imbalance, by selecting and combining the base classifiers for ensembles using a modified version of the iterative Boolean combination algorithm that is optimized using the F-measure instead of AUC. Experiments on a real-world dataset for video face recognition show the advantages of evaluating and comparing different classifiers in the F-measure space versus ROC, precision-recall, and cost spaces. In addition, it is shown that the performance evaluated using the F-measure of Bagging ensemble method can improve considerably by using the modified iterative Boolean combination algorithm. (C) 2019 Published by Elsevier Ltd.																	0031-3203	1873-5142				APR	2020	100								107146	10.1016/j.patcog.2019.107146													
J								Joint temporal context exploitation and active learning for video segmentation	PATTERN RECOGNITION										Video segmentation; Deep learning; Computer vision		The segmentation of video, or separating out objects in the foreground, is an important application of pattern recognition and computer vision. Segmentation errors in pattern recognition approaches mainly come from difficulties in selecting maximally informative frames for learning. In this paper, we develop an approach to video segmentation that relies on temporal features by modeling the uncertainty of the distribution of different feature mask forms. We use those uncertainty values for unsupervised active learning. We evaluate our approach on the DAVIS16 annotated video data set and Shining3D dental video data set, and the results show our approach to be more accurate than other video segmentation approaches. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107158	10.1016/j.patcog.2019.107158													
J								Ensemble Selection based on Classifier Prediction Confidence	PATTERN RECOGNITION										Ensemble method; Multiple classifier system; Ensemble selection; Classifier selection; Artificial bee colony	BEE COLONY ALGORITHM; ROTATION FOREST; COMPETENCE	Ensemble selection is one of the most studied topics in ensemble learning because a selected subset of base classifiers may perform better than the whole ensemble system. In recent years, a great many ensemble selection methods have been introduced. However, many of these lack flexibility: either a fixed subset of classifiers is pre-selected for all test samples (static approach), or the selection of classifiers depends upon the performance of techniques that define the region of competence (dynamic approach). In this paper, we propose an ensemble selection method that takes into account each base classifier's confidence during classification and the overall credibility of the base classifier in the ensemble. In other words, a base classifier is selected to predict for a test sample if the confidence in its prediction is higher than its credibility threshold. The credibility thresholds of the base classifiers are found by minimizing the empirical 0-1 loss on the entire training observations. In this way, our approach integrates both the static and dynamic aspects of ensemble selection. Experiments on 62 datasets demonstrate that the proposed method achieves much better performance in comparison to some ensemble methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107104	10.1016/j.patcog.2019.107104													
J								Generalized support vector data description for anomaly detection	PATTERN RECOGNITION										Anomaly detection; Bayesian statistics; Multimode process; Support vector data description	CLASSIFICATION; CLASSIFIERS; KERNEL	Traditional anomaly detection procedures assume that normal observations are obtained from a single distribution. However, due to the complexities of modern industrial processes, the observations may belong to multiple operating modes with different distributions. In such cases, traditional anomaly detection procedures may trigger false alarms while the process is indeed in another normally operating mode. We propose a generalized support vector-based anomaly detection procedure called generalized support vector data description which can be used to determine the anomalies in multimodal processes. The proposed procedure constructs hyperspheres for each class in order to include as many observations as possible and keep other class observations as far apart as possible. In addition, we introduce a generalized Bayesian framework which does not only consider the prior information from each mode, but also highlights the relationships among the modes. The effectiveness of the proposed procedure is demonstrated through various simulation studies and real-life applications. (C) 2019 Published by Elsevier Ltd.																	0031-3203	1873-5142				APR	2020	100								107119	10.1016/j.patcog.2019.107119													
J								Deep cascaded cross-modal correlation learning for fine-grained sketch-based image retrieval	PATTERN RECOGNITION										Fine-grained Sketch-based Image Retrieval (FG-SBIR); Deep Cascaded Cross-modal Correlation Learning; Deep Multimodal Representation; Deep Multimodal Embedding; Deep Triplet Ranking		Fine-grained Sketch-based Image Retrieval (FG-SBIR), which utilizes hand-drawn sketches to search the target object images, has recently drawn much attention. It is a challenging task because sketches and images belong to different modalities and sketches are highly abstract and ambiguous. Existing solutions to this problem either focus on visual comparisons between sketches and images and ignore the multimodal characteristics of annotated images, or treat the retrieval as a one-time process. In this paper, we formulate FG-SBIR as a coarse-to-fine process, and propose a Deep Cascaded Cross-modal Ranking Model (DCCRM) that can exploit all the beneficial multimodal information in sketches and annotated images and improve both the retrieval efficiency and the top-K ranked effectiveness. Our goal concentrates on constructing deep representations for sketches, images, and descriptions, and learning the optimized deep correlations across such different domains. Thus for a given query sketch, its relevant images with fine-grained instance-level similarities in a specific category can be returned, and the strict requirement of the instance-level retrieval for FG-SBIR is satisfied. Very positive results have been obtained in our experiments by using a large quantity of public data. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107148	10.1016/j.patcog.2019.107148													
J								Writer-aware CNN for parsimonious HMM-based offline handwritten Chinese text recognition	PATTERN RECOGNITION										Offline handwritten; Chinese text recognition; Writer-aware CNN; Parsimonious HMM; State tying; Adaptation; Hybrid language model	DISCRIMINATIVE LINEAR-REGRESSION; CHARACTER-RECOGNITION; ADAPTATION; ONLINE; NETWORK; MODEL; CLASSIFICATION; FEATURES	Recently, the hybrid convolutional neural network hidden Markov model (CNN-HMM) has been introduced for offline handwritten Chinese text recognition (HCTR) and has achieved state-of-the-art performance. However, modeling each of the large vocabulary of Chinese characters with a uniform and fixed number of hidden states requires high memory and computational costs and makes the tens of thousands of HMM state classes confusing. Another key issue of CNN-HMM for HCTR is the diversified writing style, which leads to model strain and a significant performance decline for specific writers. To address these issues, we propose a writer-aware CNN based on parsimonious HMM (WCNN-PHMM). First, PHMM is designed using a data-driven state-tying algorithm to greatly reduce the total number of HMM states, which not only yields a compact CNN by state sharing of the same or similar radicals among different Chinese characters but also improves the recognition accuracy due to the more accurate modeling of tied states and the lower confusion among them. Second, WCNN integrates each convolutional layer with one adaptive layer fed by a writer-dependent vector, namely, the writer code, to extract the irrelevant variability in writer information to improve recognition performance. The parameters of writer-adaptive layers are jointly optimized with other network parameters in the training stage, while a multiple-pass decoding strategy is adopted to learn the writer code and generate recognition results. Validated on the ICDAR 2013 competition of CASIA-HWDB database, the more compact WCNN-PHMM of a 7360-class vocabulary can achieve a relative character error rate (CER) reduction of 16.6% over the conventional CNN-HMM without considering language modeling. By adopting a powerful hybrid language model (N-gram language model and recurrent neural network language model), the CER of WCNN-PHMM is reduced to 3.17%. Moreover, the state-tying results of PHMM explicitly show the information sharing among similar characters and the confusion reduction of tied state classes. Finally, we visualize the learned writer codes and demonstrate the strong relationship with the writing styles of different writers. To the best of our knowledge, WCNN-PHMM yields the best results on the ICDAR 2013 competition set, demonstrating its power when enlarging the size of the character vocabulary. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107102	10.1016/j.patcog.2019.107102													
J								Prototype learning and collaborative representation using Grassmann manifolds for image set classification	PATTERN RECOGNITION										Image set classification; Collaborative representation; Prototype learning; Grassmann manifolds	FACE RECOGNITION; NEAREST POINTS	Image set classification using manifolds is becoming increasingly more attractive since it considers non-Euclidean geometry. However, with the success of dictionary learning for image set classification using manifolds, how to learn an over-complete dictionary is still challenging. This paper proposes a novel prototype subspace learning method, in which a set of images is represented by a linear subspace and then mapped onto a Grassmann manifold. With this subspace representation, class prototypes and intra-class differences can be represented as principal components and variation subspaces, respectively. Isometric mapping further maps the manifolds into the symmetrical space via collaborative representation, which permits a closed-term solution. The proposed method is evaluated for face recognition, object recognition and action recognition. Extensive experimental results on the Honda, Extended YaleB, ETH-80 and Cambridge-Gesture datasets verify the superiority of the proposed method over the state-of-the-art methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107123	10.1016/j.patcog.2019.107123													
J								Deep morphological simplification network (MS-Net) for guided registration of brain magnetic resonance images	PATTERN RECOGNITION										Deformable image registration; Deep learning; Anatomical complexity	DEFORMABLE REGISTRATION; ALGORITHMS	Deformable brain MR image registration is challenging due to large inter-subject anatomical variation. For example, the highly complex cortical folding pattern makes it hard to accurately align corresponding cortical structures of individual images. In this paper, we propose a novel deep learning way to simplify the difficult registration problem of brain MR images. Specifically, we train a morphological simplification network (MS-Net), which can generate a simple image with less anatomical details based on the complex input. With MS-Net, the complexity of the fixed image or the moving image under registration can be reduced gradually, thus building an individual (simplification) trajectory represented by MS-Net outputs. Since the generated images at the ends of the two trajectories (of the fixed and moving images) are so simple and very similar in appearance, they are easy to register. Thus, the two trajectories can act as a bridge to link the fixed and the moving images, and guide their registration. Our experiments show that the proposed method can achieve highly accurate registration performance on different datasets (i.e., NIREP, LPBA, IBSR, CUMC, and MGH). Moreover, the method can be also easily transferred across diverse image datasets and obtain superior accuracy on surface alignment. We propose MS-Net as a powerful and flexible tool to simplify brain MR images and their registration. To our knowledge, this is the first work to simplify brain MR image registration by deep learning, instead of estimating deformation field directly. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107171	10.1016/j.patcog.2019.107171													
J								Concatenation hashing: A relative position preserving method for learning binary codes	PATTERN RECOGNITION										Unsupervised hashing; Approximate nearest neighbor search; Clustering	NEAREST-NEIGHBOR; CLUSTERING-ALGORITHM; QUANTIZATION; IMAGE; DENSITY	Hashing methods perform the efficient nearest neighbor search by mapping high-dimensional data to binary codes. Compared to projection-based hashing methods, hashing methods that adopt the clustering technique can encode the complex relationship of the data into binary codes. However, their search performance is affected by the boundary of the cluster. Two similar data points may be assigned to two different clusters and then encoded into two much different binary codes. In this paper, we propose a new hashing method based on the clustering technique and it can alleviate the effect from the cluster boundary. It is from an observation that the relative positions of any two close data points to each cluster center are close. An alternating optimization is developed to simultaneously discover the cluster structures of the data and learn the hash functions to preserve the relative positions of the data to each cluster center. To integrate the information in each cluster, the corresponding binary code of each data point is obtained by concatenating the substrings learnt by the hash functions in each cluster. The experiments show that our method is competitive to or better than the state-of-the-art hashing methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107151	10.1016/j.patcog.2019.107151													
J								Graph regularized low-rank representation for submodule clustering	PATTERN RECOGNITION										Clustering; Kernel methods; Manifold regularization; Submodule clustering; Tensor nuclear norm; Union of free submodules	COMPONENT ANALYSIS; SUBSPACE; FACTORIZATION; ROBUST	In this paper, a new submodule clustering method for imaging (2-D) data is proposed. Unlike most existing clustering methods that first convert such data into vectors as preprocessing, the proposed method arranges the data samples as lateral slices of a third-order tensor. Our algorithm is based on the union-of-free-submodules model and the samples are represented using t-product in the third-order tensor space. First, we impose a low-rank constraint on the representation tensor to capture the principle information of data. By incorporating manifold regularization into the tensor factorization, the proposed method explicitly exploits the local manifold structure of data. Meanwhile, a segmentation dependent term is employed to integrate the two pipeline steps of affinity learning and spectral clustering into a unified optimization framework. The proposed method can be efficiently solved based on the alternating direction method of multipliers and spectral clustering. Finally, a nonlinear extension is proposed to handle data drawn from a mixture of nonlinear manifolds. Extensive experimental results on five real-world image datasets confirm the effectiveness of the proposed methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107145	10.1016/j.patcog.2019.107145													
J								MeMu: Metric correlation Siamese network and multi-class negative sampling for visual tracking	PATTERN RECOGNITION										Visual tracking; Siamese network; Metric correlation filter; Multi-class negative samples		Despite the great success in the computer vision field, visual tracking is still a challenging task. The main obstacle is that the target object often suffers from interference, such as occlusion. As most Siamese network-based trackers mainly sample image patches of target objects for training, the tracking algorithm lacks sufficient information about the surrounding environment. Besides, many Siamese network-based tracking algorithms build a regression only with the target object samples without considering the relationship between target and background, which may deteriorate the performance of trackers. In this paper, we propose a metric correlation Siamese network and multi-class negative sampling tracking method. For the first time, we explore a sampling approach that includes three different kinds of negative samples: virtual negative samples for pre-learning the potential occlusion situation, boundary negative samples to cope with potential tracking drift, and context negative samples to cope with potential incorrect positioning. With the three kinds of negative samples, we also propose a metric correlation method to train a correlation filter that contains metric information for better discrimination. Furthermore, we design a Siamese network-based architecture to embed the metric correlation filter module mentioned above in order to benefit from the powerful representation ability of deep learning. Extensive experiments on challenging OTB100 and VOT2017 datasets demonstrate the competitive performance of the proposed algorithm performs favorably compared with state-of-the-art approaches. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107170	10.1016/j.patcog.2019.107170													
J								Handwriting posture prediction based on unsupervised model	PATTERN RECOGNITION										Pupils; Writing posture prediction; Features extracting; Neural network model; Unsupervised learning; Data analysis		Writing is an important basic skill for humans. To acquire such a skill, pupils often have to practice writing for several hours each day. However, different pupils usually possess distinct writing postures. Bad postures not only affect the speed and quality of writing, but also severely harm the healthy development of pupils' spine and eyesight. Therefore, it is of key importance to identify or predict pupils' writing postures and accordingly correct bad ones. In this paper, we formulate the problem of handwriting posture prediction for the first time. Further, we propose a neural network constructed with small convolution kernels to extract features from handwriting, and incorporate unsupervised learning and handwriting data analysis to predict writing postures. Extensive experiments reveal that our approach achieves an accuracy rate of 93.3%, which is significantly higher than the 76.67% accuracy of human experts. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107093	10.1016/j.patcog.2019.107093													
J								A fusion network for road detection via spatial propagation and spatial transformation	PATTERN RECOGNITION										Road detection; Fusion; Spatial propagation; Spatial transformation; Joint anisotropic diffusion	POINT	In this paper, we address the fusion of image and point cloud data for road detection. To take advantage of both deep network and multi-modal data fusion, we propose an end-to-end road segmentation network called SPSTFN (Spatial Propagation and Spatial Transformation Fusion Network). Our method considers the model-level fusion and dual-view fusion in the network simultaneously for the first time. Specifically, the proposed SPSTFN contains three parts: the point cloud branch, the image branch, and the fusion block. Firstly, we design a simple but efficient lightweight network to handle the unordered and sparse point cloud to obtain a coarse representation of the road area. Then, an equal-resolution convolutional block is adopted to capture the low-level features of the image which are used to produce the heat diffusion coefficients of the joint anisotropic diffusion based spatial propagation model. Thirdly, we conduct the diffusion process on the coarse representation under the guidance of the learned low-level image features, both in the perspective and bird views, via the spatial transformation in the network. Finally, the diffusion results of the two views are then integrated to generate the final refined representation of the road area. The proposed fusion method is totally data-driven and parameter-free, and the whole fusion network can be trained with the standard BP (Back Propagation) algorithm. Without any additional process steps and pre-training, the proposed method obtains competitive results on the KITTI Road Benchmark. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107141	10.1016/j.patcog.2019.107141													
J								Deep reinforcement hashing with redundancy elimination for effective image retrieval	PATTERN RECOGNITION										Deep reinforcement hashing; Redundancy elimination; Image retrieval; Block-wise hash code inference; Hash code mapping; Hash code de-redundancy		Hashing is one of the most promising techniques in approximate nearest neighbor search due to its time efficiency and low cost in memory. Recently, with the help of deep learning, deep supervised hashing can perform representation learning and compact hash code learning jointly in an end-to-end style, and obtains better retrieval accuracy compared to non-deep methods. However, most deep hashing methods are trained with a pair-wise loss or triplet loss in a mini-batch style, which makes them inefficient at data sampling and cannot preserve the global similarity information. Besides that, many existing methods generate hash codes with redundant or even harmful bits, which is a waste of space and may lower the retrieval accuracy. In this paper, we propose a novel deep reinforcement hashing model with redundancy elimination called Deep Reinforcement De-Redundancy Hashing (DRDH), which can fully exploit large-scale similarity information and eliminate redundant hash bits with deep reinforcement learning. DRDH conducts hash code inference in a block-wise style, and uses Deep Q Network (DQN) to eliminate redundant bits. Very promising results have been achieved on four public datasets, i.e., CIFAR-10, NUS-WIDE, MS-COCO, and Open-Images-V4, which demonstrate that our method can generate highly compact hash codes and yield better retrieval performance than those of state-of-the-art methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107116	10.1016/j.patcog.2019.107116													
J								Locality-constrained affine subspace coding for image classification and retrieval	PATTERN RECOGNITION										Bag of visual words; Locality-constrained affine subspace coding; Image classification; Image retrieval	FEATURES; NETWORK	Feature coding is a key component of the bag of visual words (BoVW) model, which is designed to improve image classification and retrieval performance. In the feature coding process, each feature of an image is nonlinearly mapped via a dictionary of visual words to form a high-dimensional sparse vector. Inspired by the well-known locality-constrained linear coding (LLC), we present a locality-constrained affine subspace coding (LASC) method to address the limitation whereby LLC fails to consider the local geometric structure around visual words. LASC is distinguished from all the other coding methods since it constructs a dictionary consisting of an ensemble of affine subspaces. As such, the local geometric structure of a manifold is explicitly modeled by such a dictionary. In the process of coding, each feature is linearly decomposed and weighted to form the first-order LASC vector with respect to its top-k neighboring subspaces. To further boost performance, we propose the second-order LASC vector based on information geometry. We use the proposed coding method to perform both image classification and image retrieval tasks and the experimental results show that the method achieves superior or competitive performance in comparison to state-of-the-art methods. (C) 2019 Published by Elsevier Ltd.																	0031-3203	1873-5142				APR	2020	100								107167	10.1016/j.patcog.2019.107167													
J								Deep eigen-filters for face recognition: Feature representation via unsupervised multi-structure filter learning	PATTERN RECOGNITION										Deep eigen-filters; Convolution kernels; Face recognition; Convolutional neural networks; Feature representation	PATTERNS	Training deep convolutional neural networks (CNNs) often requires high computational cost and a large number of learnable parameters. To overcome this limitation, one solution is computing predefined convolution kernels from training data. In this paper, we propose a novel three-stage approach for filter learning alternatively. It learns filters in multiple structures including standard filters, channel-wise filters and point-wise filters which are inspired from variations of CNNs' convolution operations. By analyzing the linear combination between learned filters and original convolution kernels in pre-trained CNNs, the reconstruction error is minimized to determine the most representative filters from the filter bank. These filters are used to build a network followed by HOG-based feature extraction for feature representation. The proposed approach shows competitive performance on color face recognition compared with other deep CNNs-based methods. Besides, it provides a perspective of interpreting CNNs by introducing the concepts of advanced convolutional layers to unsupervised filter learning. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107176	10.1016/j.patcog.2019.107176													
J								Non-rigid object tracking via deep multi-scale spatial-temporal discriminative saliency maps	PATTERN RECOGNITION										Deep neural network; Non-rigid object tracking; Salient object detection; Spatial-temporal consistency	NET	In this paper, we propose a novel effective non-rigid object tracking framework based on the spatial-temporal consistent saliency detection. In contrast to most existing trackers that utilize a bounding box to specify the tracked target, the proposed framework can extract accurate regions of the target as tracking outputs. It achieves a better description of the non-rigid objects and reduces the background pollution for the tracking model. Furthermore, our model has several unique characteristics. First, a tailored fully convolutional neural network (TFCN) is developed to model the local saliency prior for a given image region, which not only provides the pixel-wise outputs but also integrates the semantic information. Second, a novel multi-scale multi-region mechanism is proposed to generate local saliency maps that effectively consider visual perceptions with different spatial layouts and scale variations. Subsequently, the local saliency maps are fused via a weighted entropy method, resulting in a discriminative saliency map. Finally, we present a non-rigid object tracking algorithm based on the predicted saliency maps. By utilizing a spatial-temporal consistent saliency map (STCSM), we conduct the target-background classification and use an online fine-tuning scheme for model updating. Extensive experiments demonstrate that the proposed algorithm achieves competitive performance in both saliency detection and visual tracking, especially outperforming other related trackers on the non-rigid object tracking datasets. Source codes and compared results are released at https://github.com/Pchank/TFCNTracker. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107130	10.1016/j.patcog.2019.107130													
J								Sign consistency for the linear programming discriminant rule	PATTERN RECOGNITION										High-dimensional linear discriminant analysis; Sign consistency; Irrepresentability condition; Linear programming	MODEL SELECTION CONSISTENCY; CLASSIFICATION	Linear discriminant analysis (LDA) is an important conventional model for data classification. Classical theory shows that LDA is Bayes consistent for a fixed data dimensionality p and a large training sample size n. However, in high-dimensional settings when p >> n, LDA is difficult due to the inconsistent estimation of the covariance matrix and the mean vectors of populations. Recently, a linear programming discriminant (LPD) rule was proposed for high-dimensional linear discriminant analysis, based on the sparsity assumption over the discriminant function. It is shown that the LPD rule is Bayes consistent in high-dimensional settings. In this paper, we further show that the LPD rule is sign consistent under the sparsity assumption. Such sign consistency ensures the LPD rule to select the optimal discriminative features for high-dimensional data classification problems. Evaluations on both synthetic and real data validate our result on the sign consistency of the LPD rule. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107083	10.1016/j.patcog.2019.107083													
J								Monocular pedestrian orientation estimation based on deep 2D-3D feedforward	PATTERN RECOGNITION										Information feedforward; Logic relationship; Monocular vision; Orientation estimation		Accurate pedestrian orientation estimation of autonomous driving helps the ego vehicle obtain the intentions of pedestrians in the related environment, which are the base of safety measures such as collision avoidance and prewarning. However, because of relatively small sizes and high-level deformation of pedestrians, common pedestrian orientation estimation models fail to extract sufficient and comprehensive information from them, thus having their performance restricted, especially monocular ones which fail to obtain depth information of objects and related environment. In this paper, a novel monocular pedestrian orientation estimation model, called FFNet, is proposed. Apart from camera captures, the model adds the 2D and 3D dimensions of pedestrians as two other inputs according to the logic relationship between orientation and them. The 2D and 3D dimensions of pedestrians are determined from the camera captures and further utilized through two feedforward links connected to the orientation estimator. The feedforward links strengthen the logicality and interpretability of the network structure of the proposed model. Experiments show that the proposed model has at least 1.72% AOS increase than most state-of-the-art models after identical training processes. The model also has competitive results in orientation estimation evaluation on KITTI dataset. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107182	10.1016/j.patcog.2019.107182													
J								Disentangled representation learning and residual GAN for age-invariant face verification	PATTERN RECOGNITION										Face recognition; Age-invariant face verification; Representation learning; Generative adversarial network	RECOGNITION; VIDEO	One of the challenges in cross-age face recognition and verification is to effectively model the facial aging process. Despite the rapid advances in face-related areas, it is still very difficult for existing methods to simultaneously achieve accurate facial feature preservation and reliable aging during aging modeling. Aiming to address this issue, we introduce a Disentangled Representation learning and Residual Generative Adversarial Network (DR-RGAN) that represents the facial features without age interference, which is achieved by explicitly disentangling the facial features and age variation. An encoder-decoder structured generator produces aging images from unstructured facial representations by using the age characteristics provided separately. It can thus take the disentangled representation to preserve personal identity for face verification. Considering that pixel-based errors may cause a loss of detail, a VGG based content loss is further equipped to preferably preserve facial features. The discriminator is trained to distinguish the real from generated faces, carry out identification prediction, and leverage an age estimator to boost the aging accuracy. It is beneficial for obtaining more photorealistic and desirable aging effects, as well as more consistent face verification results. Extensive experiments on the CACD and LFW datasets demonstrate that our DR-RGAN generates pleasing aging imageries and achieves a high accuracy of face verification. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107097	10.1016/j.patcog.2019.107097													
J								MobileFAN: Transferring deep hidden representation for face alignment	PATTERN RECOGNITION										Face alignment; Knowledge distillation; Lightweight model	NETWORK	Facial landmark detection is a crucial prerequisite for many face analysis applications. Deep learning-based methods currently dominate the approach of addressing the facial landmark detection. However, such works generally introduce a large number of parameters, resulting in high memory cost. In this paper, we aim for a lightweight as well as effective solution to facial landmark detection. To this end, we propose an effective lightweight model, namely Mobile Face Alignment Network (MobileFAN), using a simple backbone MobileNetV2 as the encoder and three deconvolutional layers as the decoder. The proposed MobileFAN, with only 8% of the model size and lower computational cost, achieves superior or equivalent performance compared with state-of-the-art models. Moreover, by transferring the geometric structural information of a face graph from a large complex model to our proposed MobileFAN through feature-aligned distillation and feature-similarity distillation, the performance of MobileFAN is further improved in effectiveness and efficiency for face alignment. Extensive experiment results on three challenging facial landmark estimation benchmarks including COFW, 300W and WFLW show the superiority of our proposed MobileFAN against state-of-the-art methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107114	10.1016/j.patcog.2019.107114													
J								BLAN: Bi-directional ladder attentive network for facial attribute prediction	PATTERN RECOGNITION										Deep facial attribute prediction; Bi-directional ladder attentive network (BLAN); Residual dual attention module (RDAM); Local mutual information maximization (LMIM); Adaptive score fusion	FACE; REPRESENTATION; INFORMATION	Deep facial attribute prediction has received considerable attention with a wide range of real-world applications in the past few years. Existing works almost extract abstract global features at high levels of deep neural networks to make predictions. However, local features at low levels, which contain detailed local attribute information, are not well exploited. In this paper, we propose a novel Bi-directional Ladder Attentive Network (BLAN) to learn hierarchical representations, covering the correlations between feature hierarchies and attribute characteristics. BLAN adopts layer-wise bi-directional connections based on the autoencoder framework from low to high levels. In this way, hierarchical features with local and global attribute characteristics could be correspondingly interweaved at each level via multiple designed Residual Dual Attention Modules (RDAMs). Besides, we derive a Local Mutual Information Maximization (LMIM) loss to further incorporate the locality of facial attributes to high-level representations at each hierarchy. Multiple attribute classifiers receive hierarchical representations to produce local and global decisions, followed by a proposed adaptive score fusion module to merge these decisions for yielding the final prediction result. Extensive experiments on two facial attribute datasets, CelebA and LFWA, demonstrate that our BLAN outperforms state-of-the-art methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107155	10.1016/j.patcog.2019.107155													
J								Clustering social audiences in business information networks	PATTERN RECOGNITION										Machine learning; Clustering; Business information networks; Social networks	NONNEGATIVE MATRIX FACTORIZATION; ALGORITHMS	Business information networks involve diverse users and rich content and have emerged as important platforms for enabling business intelligence and business decision making. A key step in an organizations business intelligence process is to cluster users with similar interests into social audiences and discover the roles they play within a business network. In this article, we propose a novel machine-learning approach, called CBIN, that co-clusters business information networks to discover and understand these audiences. The CBIN framework is based on co-factorization. The audience clusters are discovered from a combination of network structures and rich contextual information, such as node interactions and node-content correlations. Since what defines an audience cluster is data-driven, plus they often overlap, predetermining the number of clusters is usually very difficult. Therefore, we have based CBIN on an overlapping clustering paradigm with a hold-out strategy to discover the optimal number of clusters given the underlying data. Experiments validate an outstanding performance by CBIN compared to other state-of-the-art algorithms on 13 real-world enterprise datasets. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107126	10.1016/j.patcog.2019.107126													
J								Few-shot traffic sign recognition with clustering inductive bias and random neural network	PATTERN RECOGNITION										Traffic sign recognition; Few-shot learning; Clustering; Randomization		Reliable and fast traffic sign recognition (TSR) that locates the traffic sign from an image and then estimates its category is a crucial perception function for Advanced Driver Assistance Systems (ADAS) of autonomous vehicles. Most of the popular deep convolutional neural networks (DCNNs) based TSR techniques advocate discriminative feature learning for traffic signs against their appearance variability. However, such feature learning scheme may suffer from the diversity of traffic signs categories, especially when samples within each category are limited for model training (i.e., few-shot learning). Here, we present a generative feature learning based TSR network with well generalization capacity and high computational efficiency. Instead of relying on large amounts of supervision to learn discriminative features, our method devotes to learn common but unique properties of class-specific traffic signs with few training samples. Specifically, we combine clustering inductive bias with a random neural network, and then exploit computational advantages offered by a fast random projection algorithm. Experiments on two TSR benchmarks illustrate that our method achieves comparable or higher recognition accuracy than state-of-the-art DCNN-based methods with less training data and inference time consumption. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				APR	2020	100								107160	10.1016/j.patcog.2019.107160													
J								Beyond Newton: A New Root-Finding Fixed-Point Iteration for Nonlinear Equations	ALGORITHMS										Newton-Raphson method; Newton's iteration; nonlinear equations; iterative solution; gradient-based methods		Finding roots of equations is at the heart of most computational science. A well-known and widely used iterative algorithm is Newton's method. However, its convergence depends heavily on the initial guess, with poor choices often leading to slow convergence or even divergence. In this short note, we seek to enlarge the basin of attraction of the classical Newton's method. The key idea is to develop a relatively simple multiplicative transform of the original equations, which leads to a reduction in nonlinearity, thereby alleviating the limitation of Newton's method. Based on this idea, we derive a new class of iterative methods and rediscover Halley's method as the limit case. We present the application of these methods to several mathematical functions (real, complex, and vector equations). Across all examples, our numerical experiments suggest that the new methods converge for a significantly wider range of initial guesses. For scalar equations, the increase in computational cost per iteration is minimal. For vector functions, more extensive analysis is needed to compare the increase in cost per iteration and the improvement in convergence of specific problems.																		1999-4893				APR	2020	13	4							78	10.3390/a13040078													
J								Success History-Based Position Adaptation in Fuzzy-Controlled Ensemble of Biology-Inspired Algorithms	ALGORITHMS										optimization; co-operation; biology-inspired algorithms; external archive; probabilistic distribution	SWARM OPTIMIZATION; POWERFUL; LOGIC	In this study, a new modification of the meta-heuristic approach called Co-Operation of Biology-Related Algorithms (COBRA) is proposed. Originally the COBRA approach was based on a fuzzy logic controller and used for solving real-parameter optimization problems. The basic idea consists of a cooperative work of six well-known biology-inspired algorithms, referred to as components. However, it was established that the search efficiency of COBRA depends on its ability to keep the exploitation and exploration balance when solving optimization problems. The new modification of the COBRA approach is based on other method for generating potential solutions. This method keeps a historical memory of successful positions found by individuals to lead them in different directions and therefore to improve the exploitation and exploration capabilities. The proposed technique was applied to the COBRA components and to its basic steps. The newly proposed meta-heuristic as well as other modifications of the COBRA approach and components were evaluated on three sets of various benchmark problems. The experimental results obtained by all algorithms with the same computational effort are presented and compared. It was concluded that the proposed modification outperformed other algorithms used in comparison. Therefore, its usefulness and workability were demonstrated.																		1999-4893				APR	2020	13	4							89	10.3390/a13040089													
J								Deterministic Coresets for k-Means of Big Sparse Data	ALGORITHMS										coreset; clustering; KMeans; big data; streaming		Let P be a set of n points in R-d, k >= 1 be an integer and epsilon is an element of(0,1) be a constant. An epsilon-coreset is a subset C subset of P with appropriate non-negative weights (scalars), that approximates any given set Q subset of R-d of k centers. That is, the sum of squared distances over every point in P to its closest point in Q is the same, up to a factor of 1 +/-epsilon to the weighted sum of C to the same k centers. If the coreset is small, we can solve problems such as k-means clustering or its variants (e.g., discrete k-means, where the centers are restricted to be in P, or other restricted zones) on the small coreset to get faster provable approximations. Moreover, it is known that such coreset support streaming, dynamic and distributed data using the classic merge-reduce trees. The fact that the coreset is a subset implies that it preserves the sparsity of the data. However, existing such coresets are randomized and their size has at least linear dependency on the dimension d. We suggest the first such coreset of size independent of d. This is also the first deterministic coreset construction whose resulting size is not exponential in d. Extensive experimental results and benchmarks are provided on public datasets, including the first coreset of the English Wikipedia using Amazon's cloud.																		1999-4893				APR	2020	13	4							92	10.3390/a13040092													
J								A Survey of Low-Rank Updates of Preconditioners for Sequences of Symmetric Linear Systems	ALGORITHMS										preconditioners; conjugate gradient method; sparse matrices; low-rank updates	INEXACT INVERSE ITERATION; SPECTRAL PRECONDITIONERS; CONJUGATE GRADIENTS; JACOBI-DAVIDSON; DEFLATION; CONVERGENCE; SOLVERS; GMRES	The aim of this survey is to review some recent developments in devising efficient preconditioners for sequences of symmetric positive definite (SPD) linear systems A(k)x(k) = b(k), k=1, ... arising in many scientific applications, such as discretization of transient Partial Differential Equations (PDEs), solution of eigenvalue problems, (Inexact) Newton methods applied to nonlinear systems, rational Krylov methods for computing a function of a matrix. In this paper, we will analyze a number of techniques of updating a given initial preconditioner by a low-rank matrix with the aim of improving the clustering of eigenvalues around 1, in order to speed-up the convergence of the Preconditioned Conjugate Gradient (PCG) method. We will also review some techniques to efficiently approximate the linearly independent vectors which constitute the low-rank corrections and whose choice is crucial for the effectiveness of the approach. Numerical results on real-life applications show that the performance of a given iterative solver can be very much enhanced by the use of low-rank updates.																		1999-4893				APR	2020	13	4							100	10.3390/a13040100													
J								Numerical Simulation of Non-Linear Models of Reaction-Diffusion for a DGT Sensor	ALGORITHMS										reaction; diffusion; DGT; numerical simulation; PDE	THIN-FILMS; GRADIENTS; COMPLEXES; LABILITY; KINETICS; METALS	In this work, we present a novel strategy for the numerical solution of a coupled system of partial differential equations that describe reaction-diffusion processes of a mixture of metals and ligands that can be absorbed by a sensor or a microorganism, in an aqueous medium. The novelty introduced in this work consisted of an adequate database management in conjunction with a direct iterative schema, which allowed the construction of simple, fast and efficient algorithms. Except in really adverse conditions, the calculation is converging and satisfactory solutions were reached. Computing times showed to be better than those obtained with some commercial programs. Although we concentrate on the solution for a particular system (Diffusive Gradients in Thin Films [DGT] sensors), the proposed algorithm does not require major modifications to consider new theoretical or experimental configurations. Since the quality of numerical simulations of reaction-diffusion problems often faces some drawbacks as the values of reaction rate constants increase, some additional effort has been invested in obtaining proper solutions in those cases.																		1999-4893				APR	2020	13	4							98	10.3390/a13040098													
J								On Classical Solutions for A Kuramoto-Sinelshchikov-Velarde-Type Equation	ALGORITHMS										existence; uniqueness; stability; Kuramoto-Sinelshchikov-Velarde-type equation; Cauchy problem	BENARD-MARANGONI CONVECTION; TRAVELING-WAVE SOLUTIONS; WELL-POSEDNESS; NONLINEAR SATURATION; SIVASHINSKY EQUATION; CONVERGENCE; STABILITY; DIFFUSION; STABILIZATION; INSTABILITY	The Kuramoto-Sinelshchikov-Velarde equation describes the evolution of a phase turbulence in reaction-diffusion systems or the evolution of the plane flame propagation, taking into account the combined influence of diffusion and thermal conduction of the gas on the stability of a plane flame front. In this paper, we prove the well-posedness of the classical solutions for the Cauchy problem.																		1999-4893				APR	2020	13	4							77	10.3390/a13040077													
J								Research and Study of the Hybrid Algorithms Based on the Collective Behavior of Fish Schools and Classical Optimization Methods	ALGORITHMS										evolutionary optimization; swarm intelligence; fish school search; gradient descent; hybrid algorithm; Newton's algorithm; neural network training; hyper parameter optimization; distributed computations	PARTICLE SWARM OPTIMIZATION; EVOLUTIONARY ALGORITHMS; GENETIC ALGORITHM; GRADIENT DESCENT; NEURAL-NETWORKS; SEARCH	Inspired by biological systems, swarm intelligence algorithms are widely used to solve multimodal optimization problems. In this study, we consider the hybridization problem of an algorithm based on the collective behavior of fish schools. The algorithm is computationally inexpensive compared to other population-based algorithms. Accuracy of fish school search increases with the increase of predefined iteration count, but this also affects computation time required to find a suboptimal solution. We propose two hybrid approaches, intending to improve the evolutionary-inspired algorithm accuracy by using classical optimization methods, such as gradient descent and Newton's optimization method. The study shows the effectiveness of the proposed hybrid algorithms, and the strong advantage of the hybrid algorithm based on fish school search and gradient descent. We provide a solution for the linearly inseparable exclusive disjunction problem using the developed algorithm and a perceptron with one hidden layer. To demonstrate the effectiveness of the algorithms, we visualize high dimensional loss surfaces near global extreme points. In addition, we apply the distributed version of the most effective hybrid algorithm to the hyperparameter optimization problem of a neural network.																		1999-4893				APR	2020	13	4							85	10.3390/a13040085													
J								Performance Assessment of Predictive Control-A Survey	ALGORITHMS										advanced process control; model predictive control; control performance assessment; benchmark; model-free methods; model-based approach; DMC; GPC	CONTROL MPC PERFORMANCE; CONTROL-SYSTEMS; ECONOMIC-PERFORMANCE; MODEL; CONSTRAINT; DIAGNOSIS; CHALLENGES; STATE; CLASSIFICATION; OPTIMIZATION	Model Predictive Control constitutes an important element of any modern control system. There is growing interest in this technology. More and more advanced predictive structures have been implemented. The first applications were in chemical engineering, and now Model Predictive Control can be found in almost all kinds of applications, from the process industry to embedded control systems or for autonomous objects. Currently, each implementation of a control system requires strict financial justification. Application engineers need tools to measure and quantify the quality of the control and the potential for improvement that may be achieved by retrofitting control systems. Furthermore, a successful implementation of predictive control must conform to prior estimations not only during commissioning, but also during regular daily operations. The system must sustain the quality of control performance. The assessment of Model Predictive Control requires a suitable, often specific, methodology and comparative indicators. These demands establish the rationale of this survey. Therefore, the paper collects and summarizes control performance assessment methods specifically designed for and utilized in predictive control. These observations present the picture of the assessment technology. Further generalization leads to the formulation of a control assessment procedure to support control application engineers.																		1999-4893				APR	2020	13	4							97	10.3390/a13040097													
J								Experiments-Based Comparison of Different Power Controllers for a Solid Oxide Fuel Cell Against Model Imperfections and Delay Phenomena	ALGORITHMS										SOFC; power control; experimental performance analysis; nonlinear control; PI control; sliding-mode control; internal model control; linear state-feedback control	SYSTEM	Solid oxide fuel cell systems such as those presented in this paper are not only applicable for a pure supply with electric energy, they can typically also be used in decentralized power stations, i.e., as micro-cogeneration systems for houses, where both electric and thermal energy are required. For that application, obviously, the electric power need is not constant but rather changes over time. In such a way, it essentially depends on the user profiles of said houses which can refer to e.g., private households as well as offices. The power use is furthermore not predefined. For an optimal operation of the fuel cell, we want to adjust the power, to match the need with sufficiently small time constants without the implementation of mid- or long-term electrical storage systems such as battery buffers. To adapt the produced electric power a simple, however, sufficiently robust feedback controller regulating the hydrogen mass flow into the cells is necessary. To achieve this goal, four different controllers, namely, a PI output-feedback controller combined with a feedforward control, an internal model control (IMC) approach, a sliding-mode (SM) controller and a state-feedback controller, are developed and compared in this paper. As the challenge is to find a controller ensuring steady-state accuracy and good tracking behavior despite the nonlinearities and uncertainties of the plant, the comparison was done regarding these requirements. Simulations and experiments show that the IMC outperforms the alternatives with respect to steady-state accuracy and tracking behavior.																		1999-4893				APR	2020	13	4							76	10.3390/a13040076													
J								Practical Grammar Compression Based on Maximal Repeats	ALGORITHMS										lossless data compression; RePair; maximal repeat	LINE TEXT COMPRESSION	This study presents an analysis of RePair, which is a grammar compression algorithm known for its simple scheme, while also being practically effective. First, we show that the main process of RePair, that is, the step by step substitution of the most frequent symbol pairs, works within the corresponding most frequent maximal repeats. Then, we reveal the relation between maximal repeats and grammars constructed by RePair. On the basis of this analysis, we further propose a novel variant of RePair, called MR-RePair, which considers the one-time substitution of the most frequent maximal repeats instead of the consecutive substitution of the most frequent pairs. The results of the experiments comparing the size of constructed grammars and execution time of RePair and MR-RePair on several text corpora demonstrate that MR-RePair constructs more compact grammars than RePair does, especially for highly repetitive texts.																		1999-4893				APR	2020	13	4							103	10.3390/a13040103													
J								Decision Support System for Fitting and Mapping Nonlinear Functions with Application to Insect Pest Management in the Biological Control Context	ALGORITHMS										Nonlinear regression; interactive platform; component-based approach; software architecture; Eclipse-RCP (Rich Client Platform); spatial prediction	METARHIZIUM-ANISOPLIAE; TEMPERATURE; VIRULENCE; PATHOGENICITY; HUMIDITY; ISOLATE; LOCUSTS; MODELS; APHID	The process of moving from experimental data to modeling and characterizing the dynamics and interactions in natural processes is a challenging task. This paper proposes an interactive platform for fitting data derived from experiments to mathematical expressions and carrying out spatial visualization. The platform is designed using a component-based software architectural approach, implemented in R and the Java programming languages. It uses experimental data as input for model fitting, then applies the obtained model at the landscape level via a spatial temperature grid data to yield regional and continental maps. Different modules and functionalities of the tool are presented with a case study, in which the tool is used to establish a temperature-dependent virulence model and map the potential zone of efficacy of a fungal-based biopesticide. The decision support system (DSS) was developed in generic form, and it can be used by anyone interested in fitting mathematical equations to experimental data collected following the described protocol and, depending on the type of investigation, it offers the possibility of projecting the model at the landscape level.																		1999-4893				APR	2020	13	4							104	10.3390/a13040104													
J								Ensemble Deep Learning for Multilabel Binary Classification of User-Generated Content	ALGORITHMS										ensemble learning; sentiment analysis; multilabel classification; deep neural networks; pure emotion; Semeval 2018 Task 1; toxic comment classification	SENTIMENT ANALYSIS; DIFFERENTIAL EVOLUTION; NEURAL-NETWORKS	Sentiment analysis usually refers to the analysis of human-generated content via a polarity filter. Affective computing deals with the exact emotions conveyed through information. Emotional information most frequently cannot be accurately described by a single emotion class. Multilabel classifiers can categorize human-generated content in multiple emotional classes. Ensemble learning can improve the statistical, computational and representation aspects of such classifiers. We present a baseline stacked ensemble and propose a weighted ensemble. Our proposed weighted ensemble can use multiple classifiers to improve classification results without hyperparameter tuning or data overfitting. We evaluate our ensemble models with two datasets. The first dataset is from Semeval2018-Task 1 and contains almost 7000 Tweets, labeled with 11 sentiment classes. The second dataset is the Toxic Comment Dataset with more than 150,000 comments, labeled with six different levels of abuse or harassment. Our results suggest that ensemble learning improves classification results by 1.5% to 5.4%.																		1999-4893				APR	2020	13	4							83	10.3390/a13040083													
J								Hierarchical-Matching-Based Online and Real-Time Multi-Object Tracking with Deep Appearance Features	ALGORITHMS										multiple object tracking; convolutional neural network; data association; pedestrian re-identification; trajectory confidence	MULTITARGET	Based on tracking-by-detection, we propose a hierarchical-matching-based online and real-time multi-object tracking approach with deep appearance features, which can effectively reduce the false positives (FP) in tracking. For the purpose of increasing the accuracy rate of data association, we define the trajectory confidence using its position information, appearance information, and the information of historical relevant detections, after which we can classify the trajectories into different levels. In order to obtain discriminative appearance features, we developed a deep convolutional neural network to extract the appearance features of objects and trained it on a large-scale pedestrian re-identification dataset. Last but not least, we used the proposed diverse and hierarchical matching strategy to associate detection and trajectory sets. Experimental results on the MOT benchmark dataset show that our proposed approach performs well against other online methods, especially for the metrics of FP and frames per second (FPS).																		1999-4893				APR	2020	13	4							80	10.3390/a13040080													
J								Variational Specific Mode Extraction: A Novel Method for Defect Signal Detection of Ferromagnetic Pipeline	ALGORITHMS										defect signal recognition; buried pipeline; magnetic anomaly detection; variational specific mode extraction; non-contact	DECOMPOSITION; INSPECTION; SAR; CORROSION	The non-contact detection of buried ferromagnetic pipeline is a long-standing problem in the field of inspection of outside pipelines, and the extraction of magnetic anomaly signal is a prerequisite for accurate detection. Pipeline defects can cause the fluctuation of magnetic signals, which are easily submerged in wide-band background noise without external excitation sources. Previously, Variational Mode Decomposition (VMD) was used to separate modal components; however, VMD is based on narrow-band signal processing algorithm and the calculation is complex. In this article, a method of pipeline defect signal based on Variational Specific Mode Extraction (VSME) is employed to extract the signal of a specific central frequency by signal modal decomposition, i.e., the specific mode is weak magnetic anomaly signal of pipeline defects. VSME is based on the fact that a wide-band signal can be converted into a narrow-band signal by demodulation method. Furthermore, the problem of wide-band signal decomposition is expressed as an optimal demodulation problem, which can be solved by alternating direction method of multipliers. The proposed algorithm is verified by artificially synthesized signals, and its performance is better than that of VMD. The results showed that the VSME method can extract the magnetic anomaly signal of pipeline damage using experimental data, while obtaining a better accuracy.																		1999-4893				APR	2020	13	4							105	10.3390/a13040105													
J								Application of Generalized Polynomial Chaos for Quantification of Uncertainties of Time Averages and Their Sensitivities in Chaotic Systems	ALGORITHMS										uncertainty quantification; chaos; generalized polynomial chaos; multiple shooting shadowing; sensitivity analysis; Monte-Carlo	SIMULATIONS	In this paper, we consider the effect of stochastic uncertainties on non-linear systems with chaotic behavior. More specifically, we quantify the effect of parametric uncertainties to time-averaged quantities and their sensitivities. Sampling methods for Uncertainty Quantification (UQ), such as the Monte-Carlo (MC), are very costly, while traditional methods for sensitivity analysis, such as the adjoint, fail in chaotic systems. In this work, we employ the non-intrusive generalized Polynomial Chaos (gPC) for UQ, coupled with the Multiple-Shooting Shadowing (MSS) algorithm for sensitivity analysis of chaotic systems. It is shown that the gPC, coupled with MSS, is an appropriate method for conducting UQ in chaotic systems and produces results that match well with those from MC and Finite-Differences (FD).																		1999-4893				APR	2020	13	4							90	10.3390/a13040090													
J								The Need for Machine-Processable Agreements in Health Data Management	ALGORITHMS										data sharing; consent; privacy policies; privacy languages; genomic data; genomic medicine; health data management	FAMILY COMMUNICATION; RISK; RELATIVES; PRIVACY; BRCA1	Data processing agreements in health data management are laid out by organisations in monolithic "Terms and Conditions" documents written in natural legal language. These top-down policies usually protect the interest of the service providers, rather than the data owners. They are coarse-grained and do not allow for more than a few opt-in or opt-out options for individuals to express their consent on personal data processing, and these options often do not transfer to software as they were intended to. In this paper, we study the problem of health data sharing and we advocate the need for individuals to describe their personal contract of data usage in a formal, machine-processable language. We develop an application for sharing patient genomic information and test results, and use interactions with patients and clinicians in order to identify the particular peculiarities a privacy/policy/consent language should offer in this complicated domain. We present how Semantic Web technologies can have a central role in this approach by providing the formal tools and features required in such a language. We present our ongoing approach to construct an ontology-based framework and a policy language that allows patients and clinicians to express fine-grained consent, preferences or suggestions on sharing medical information. Our language offers unique features such as multi-party ownership of data or data sharing dependencies. We evaluate the landscape of policy languages from different areas, and show how they are lacking major requirements needed in health data management. In addition to enabling patients, our approach helps organisations increase technological capabilities, abide by legal requirements, and save resources.																		1999-4893				APR	2020	13	4							87	10.3390/a13040087													
J								A New Way to Store Simple Text Files	ALGORITHMS										Big Data; text file; png image; compression; steganography	ALGORITHM; SYSTEM	In the era of ubiquitous digitization, the Internet of Things (IoT), information plays a vital role. All types of data are collected, and some of this data are stored as text files. An important aspect-regardless of the type of data-is related to file storage, especially the amount of disk space that is required. The less space is used on storing data sets, the lower is the cost of this service. Another important aspect of storing data warehouses in the form of files is the cost of data transmission needed for file transfer and its processing. Moreover, the data that are stored should be minimally protected against access and reading by other entities. The aspects mentioned above are particularly important for large data sets like Big Data. Considering the above criteria, i.e., minimizing storage space, data transfer, ensuring minimum security, the main goal of the article was to show the new way of storing text files. This article presents a method that converts data from text files like txt, json, html, py to images (image files) in png format. Taking into account such criteria as the output size of the file, the results obtained for the test files confirm that presented method enables to reduce the need for disk space, as well as to hide data in an image file. The described method can be used for texts saved in extended ASCII and UTF-8 coding.																		1999-4893				APR	2020	13	4							101	10.3390/a13040101													
J								Path Planning for Laser Cladding Robot on Artificial Joint Surface Based on Topology Reconstruction	ALGORITHMS										laser cladding robot; path planning; topological relation; artificial joint	OF-THE-ART; HIP JOINTS; OPTIMIZATION; SIZE	Artificial joint surface coating is a hot issue in the interdisciplinary fields of manufacturing, materials and biomedicine. Due to the complex surface characteristics of artificial joints, there are some problems with efficiency and precision in automatic cladding path planning for coating fabrication. In this study, a path planning method for a laser cladding robot for artificial joints surface was proposed. The key of this method was the topological reconstruction of the artificial joint surface. On the basis of the topological relation, a set of parallel planes were used to intersect the CAD model to generate a set of continuous, directed and equidistant surface transversals on the artificial joint surface. The arch height error method was used to extract robot interpolation points from surface transversal lines according to machining accuracy requirements. The coordinates and normal vectors of interpolation points were used to calculate the position and pose of the robot tool center point (TCP). To ensure that the laser beam was always perpendicular to the artificial joint surface, a novel laser cladding set-up with a robot was designed, of which the joint part clamped by a six-axis robot moved while the laser head was fixed on the workbench. The proposed methodology was validated with the planned path on the surface of an artificial acetabular cup using simulation and experimentation via an industrial NACHI robot. The results indicated that the path planning method based on topological reconstruction was feasible and more efficient than the traditional robot teaching method.																		1999-4893				APR	2020	13	4							93	10.3390/a13040093													
J								Algebraic Point Projection for Immersed Boundary Analysis on Low Degree NURBS Curves and Surfaces	ALGORITHMS										NURBS; implicit representation; resultant; algebraic level sets; point projection and inversion	ORTHOGONAL PROJECTION; DISTANCE; INVERSION; INTERFACE; ALGORITHM; GEOMETRY; CAD	Point projection is an important geometric need when boundaries described by parametric curves and surfaces are immersed in domains. In problems where an immersed parametric boundary evolves with time as in solidification or fracture analysis, the projection from a point in the domain to the boundary is necessary to determine the interaction of the moving boundary with the underlying domain approximation. Furthermore, during analysis, since the driving force behind interface evolution depends on locally computed curvatures and normals, it is ideal if the parametric entity is not approximated as piecewise-linear. To address this challenge, we present in this paper an algebraic procedure to project a point on to Non-uniform rational B-spline (NURBS) curves and surfaces. The developed technique utilizes the resultant theory to construct implicit forms of parametric Bezier patches, level sets of which are termed algebraic level sets (ALS). Boolean compositions of the algebraic level sets are carried out using the theory of R-functions. The algebraic level sets and their gradients at a given point on the domain are then used to project the point onto the immersed boundary. Beginning with a first-order algorithm, sequentially refined procedures culminating in a second-order projection algorithm are described for NURBS curves and surfaces. Examples are presented to illustrate the efficiency and robustness of the developed method. More importantly, the method is shown to be robust and able to generate valid solutions even for curves and surfaces with high local curvature or g(0) continuity-problems where the Newton-Raphson method fails due to discontinuity in the projected points or because the numerical iterations fail to converge to a solution, respectively.																		1999-4893				APR	2020	13	4							82	10.3390/a13040082													
J								A Case Study for a Big Data and Machine Learning Platform to Improve Medical Decision Support in Population Health Management	ALGORITHMS										decision support systems; population health management; big data; machine learning; deep learning; personalized patient care	CARE; ANALYTICS; RISK	Big data and artificial intelligence are currently two of the most important and trending pieces for innovation and predictive analytics in healthcare, leading the digital healthcare transformation. Keralty organization is already working on developing an intelligent big data analytic platform based on machine learning and data integration principles. We discuss how this platform is the new pillar for the organization to improve population health management, value-based care, and new upcoming challenges in healthcare. The benefits of using this new data platform for community and population health include better healthcare outcomes, improvement of clinical operations, reducing costs of care, and generation of accurate medical information. Several machine learning algorithms implemented by the authors can use the large standardized datasets integrated into the platform to improve the effectiveness of public health interventions, improving diagnosis, and clinical decision support. The data integrated into the platform come from Electronic Health Records (EHR), Hospital Information Systems (HIS), Radiology Information Systems (RIS), and Laboratory Information Systems (LIS), as well as data generated by public health platforms, mobile data, social media, and clinical web portals. This massive volume of data is integrated using big data techniques for storage, retrieval, processing, and transformation. This paper presents the design of a digital health platform in a healthcare organization in Colombia to integrate operational, clinical, and business data repositories with advanced analytics to improve the decision-making process for population health management.																		1999-4893				APR	2020	13	4							102	10.3390/a13040102													
J								A New Lossless DNA Compression Algorithm Based on A Single-Block Encoding Scheme	ALGORITHMS										DNA sequence; storage; lossless compression; FASTA files; FASTQ files; binary encoding scheme; single-block encoding scheme; compression ratio	LOSS-LESS COMPRESSION; SEQUENCE; CHALLENGE	With the emergent evolution in DNA sequencing technology, a massive amount of genomic data is produced every day, mainly DNA sequences, craving for more storage and bandwidth. Unfortunately, managing, analyzing and specifically storing these large amounts of data become a major scientific challenge for bioinformatics. Therefore, to overcome these challenges, compression has become necessary. In this paper, we describe a new reference-free DNA compressor abbreviated as DNAC-SBE. DNAC-SBE is a lossless hybrid compressor that consists of three phases. First, starting from the largest base (Bi), the positions of each Bi are replaced with ones and the positions of other bases that have smaller frequencies than Bi are replaced with zeros. Second, to encode the generated streams, we propose a new single-block encoding scheme (SEB) based on the exploitation of the position of neighboring bits within the block using two different techniques. Finally, the proposed algorithm dynamically assigns the shorter length code to each block. Results show that DNAC-SBE outperforms state-of-the-art compressors and proves its efficiency in terms of special conditions imposed on compressed data, storage space and data transfer rate regardless of the file format or the size of the data.																		1999-4893				APR	2020	13	4							99	10.3390/a13040099													
J								Feasibility Pump Algorithm for Sparse Representation under Gaussian Noise	ALGORITHMS										sparse representations; mixed integer programming; feasibility pump; Gaussian noise; regularization; weight term	HEURISTICS	In this paper, the Feasibility Pump is adapted for the problem of sparse representations of signals affected by Gaussian noise. This adaptation is tested and then compared to Orthogonal Matching Pursuit (OMP) and the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA). The feasibility pump recovers the true support much better than the other two algorithms and, as the SNR decreases and the support size increases, it has a smaller recovery and representation error when compared with its competitors. It is observed that, in order for the algorithm to be efficient, a regularization parameter and a weight term for the error are needed.																		1999-4893				APR	2020	13	4							88	10.3390/a13040088													
J								Stochastic Models to Qualify Stem Tapers	ALGORITHMS										stem taper; stochastic differential equation; probability density function; maximum likelihood procedure	FORM; EQUATIONS; DENSITY	This study examines the performance of 11 tree taper models to predict the diameter of bark at any given height and the total stem volume of eight dominant tree species in the boreal forests of Lithuania. Here, we develop eight new models using stochastic differential equations (SDEs). The symmetrical Vasicek model and asymmetrical Gompertz model are used to describe tree taper evolution, as well as geometric-type diffusion processes. These models are compared with those traditionally used for four tree taper models by using performance statistics and residual analysis. The observed dataset consists of longitudinal measurements of 3703 trees, representing the eight dominant tree species in Lithuania (pine, spruce, oak, ash, birch, black alder, white alder, and aspen). Overall, the best goodness of fit statistics of diameter predictions produced the SDE taper models. All results have been implemented in the Maple computer algebra system using the "Statistics" and "VectorCalculus" packages.																		1999-4893				APR	2020	13	4							94	10.3390/a13040094													
J								Detection and Monitoring of Bottom-Up Cracks in Road Pavement Using a Machine-Learning Approach	ALGORITHMS										machine learning classifier; vibro-acoustic signature; road pavement; structural health monitoring; concealed cracks identification and monitoring	ARTIFICIAL NEURAL-NETWORKS; ASPHALT PAVEMENTS; PROPAGATION; PERFORMANCE	The current methods that aim at monitoring the structural health status (SHS) of road pavements allow detecting surface defects and failures. This notwithstanding, there is a lack of methods and systems that are able to identify concealed cracks (particularly, bottom-up cracks) and monitor their growth over time. For this reason, the objective of this study is to set up a supervised machine learning (ML)-based method for the identification and classification of the SHS of a differently cracked road pavement based on its vibro-acoustic signature. The method aims at collecting these signatures (using acoustic-sensors, located at the roadside) and classifying the pavement's SHS through ML models. Different ML classifiers (i.e., multilayer perceptron, MLP, convolutional neural network, CNN, random forest classifier, RFC, and support vector classifier, SVC) were used and compared. Results show the possibility of associating with great accuracy (i.e., MLP = 91.8%, CNN = 95.6%, RFC = 91.0%, and SVC = 99.1%) a specific vibro-acoustic signature to a differently cracked road pavement. These results are encouraging and represent the bases for the application of the proposed method in real contexts, such as monitoring roads and bridges using wireless sensor networks, which is the target of future studies.																		1999-4893				APR	2020	13	4							81	10.3390/a13040081													
J								Investigating Feature Selection and Random Forests for Inter-Patient Heartbeat Classification	ALGORITHMS										ECG feature selection; heartbeat classification; arrhythmia detection; random forest classifier	WAVELET TRANSFORMATION; COMPLEXES; ENSEMBLE; SYSTEM	Finding an optimal combination of features and classifier is still an open problem in the development of automatic heartbeat classification systems, especially when applications that involve resource-constrained devices are considered. In this paper, a novel study of the selection of informative features and the use of a random forest classifier while following the recommendations of the Association for the Advancement of Medical Instrumentation (AAMI) and an inter-patient division of datasets is presented. Features were selected using a filter method based on the mutual information ranking criterion on the training set. Results showed that normalized beat-to-beat (R-R) intervals and features relative to the width of the ventricular depolarization waves (QRS complex) are the most discriminative among those considered. The best results achieved on the MIT-BIH Arrhythmia Database were an overall accuracy of 96.14% and F1-scores of 97.97%, 73.06%, and 90.85% in the classification of normal beats, supraventricular ectopic beats, and ventricular ectopic beats, respectively. In comparison with other state-of-the-art approaches tested under similar constraints, this work represents one of the highest performances reported to date while relying on a very small feature vector.																		1999-4893				APR	2020	13	4							75	10.3390/a13040075													
J								A Hybrid Grasshopper Optimization Algorithm Applied to the Open Vehicle Routing Problem	ALGORITHMS										optimization; combinatorial optimization; open vehicle routing problem; grasshopper optimization algorithm	VARIABLE NEIGHBORHOOD SEARCH; TABU SEARCH; FORMULATION; COLONY; SYSTEM	This paper presents a hybrid grasshopper optimization algorithm using a novel decoder and local search to solve instances of the open vehicle routing problem with capacity and distance constraints. The algorithm's decoder first defines the number of vehicles to be used and then it partitions the clients, assigning them to the available routes. The algorithm performs a local search in three neighborhoods after decoding. When a new best solution is found, every route is locally optimized by solving a traveling salesman problem, considering the depot and clients in the route. Three sets containing a total of 30 benchmark problems from the literature were used to test the algorithm. The experiments considered two cases of the problem. In the first, the primary objective is to minimize the total number of vehicles and then the total distance to be traveled. In the second case, the total distance traveled by the vehicles is minimized. The obtained results showed the algorithm's proficient performance. For the first case, the algorithm was able to improve or match the best-known solutions for 21 of the 30 benchmark problems. For the second case, the best-known solutions for 18 of the 30 benchmark problems were found or improved by the algorithm. Finally, a case study from a real-life problem is included.																		1999-4893				APR	2020	13	4							96	10.3390/a13040096													
J								Confidence-Based Voting for the Design of Interpretable Ensembles with Fuzzy Systems	ALGORITHMS										classification; fuzzy logic; ensemble of classifiers; classifier voting; neural network; Doppler sensor; vital sensing		In this study, a new voting procedure for combining the fuzzy logic based classifiers and other classifiers called confidence-based voting is proposed. This method combines two classifiers, namely the fuzzy classification system, and for the cases when the fuzzy system returns high confidence levels, i.e., the returned membership value is large, the fuzzy system is used to perform classification, otherwise, the second classifier is applied. As a result, most of the sample is classified by the explainable and interpretable fuzzy system, and the second, more accurate, but less interpretable classifier is applied only for the most difficult cases. To show the efficiency of the proposed approach, a set of experiments is performed on test datasets, as well as two problems of estimating the person's emotional state with the data collected by non-contact vital sensors, which use the Doppler effect. To validate the accuracies of the proposed approach, the statistical tests were used for comparison. The obtained results demonstrate the efficiency of the proposed technique, as it allows for both improving the classification accuracy and explaining the decision making process.																		1999-4893				APR	2020	13	4							86	10.3390/a13040086													
J								How to Identify Varying Lead-Lag Effects in Time Series Data: Implementation, Validation, and Application of the Generalized Causality Algorithm	ALGORITHMS										lead-lag effect; structural break; generalized causality algorithm; optimal causal path; simulation study; quantitative economics; finance	NONPARAMETRIC DETERMINATION; PATH; RECOGNITION; PERFORMANCE; ARBITRAGE; SILVER; MODEL; GOLD	This paper develops the generalized causality algorithm and applies it to a multitude of data from the fields of economics and finance. Specifically, our parameter-free algorithm efficiently determines the optimal non-linear mapping and identifies varying lead-lag effects between two given time series. This procedure allows an elastic adjustment of the time axis to find similar but phase-shifted sequences-structural breaks in their relationship are also captured. A large-scale simulation study validates the outperformance in the vast majority of parameter constellations in terms of efficiency, robustness, and feasibility. Finally, the presented methodology is applied to real data from the areas of macroeconomics, finance, and metal. Highest similarity show the pairs of gross domestic product and consumer price index (macroeconomics), S&P 500 index and Deutscher Aktienindex (finance), as well as gold and silver (metal). In addition, the algorithm takes full use of its flexibility and identifies both various structural breaks and regime patterns over time, which are (partly) well documented in the literature.																		1999-4893				APR	2020	13	4							95	10.3390/a13040095													
J								A Generalized Alternating Linearization Bundle Method for Structured Convex Optimization with Inexact First-Order Oracles	ALGORITHMS										nonsmooth convex optimization; bundle method; alternating linearization; on-demand accurary; global convergence	DIRECTION METHOD; UNIT COMMITMENT; DECOMPOSITION; SUBSTITUTION; RELAXATION	In this paper, we consider a class of structured optimization problems whose objective function is the summation of two convex functions: f and h, which are not necessarily differentiable. We focus particularly on the case where the function f is general and its exact first-order information (function value and subgradient) may be difficult to obtain, while the function h is relatively simple. We propose a generalized alternating linearization bundle method for solving this class of problems, which can handle inexact first-order information of on-demand accuracy. The inexact information can be very general, which covers various oracles, such as inexact, partially inexact and asymptotically exact oracles, and so forth. At each iteration, the algorithm solves two interrelated subproblems: one aims to find the proximal point of the polyhedron model of f plus the linearization of h; the other aims to find the proximal point of the linearization of f plus h. We establish global convergence of the algorithm under different types of inexactness. Finally, some preliminary numerical results on a set of two-stage stochastic linear programming problems show that our method is very encouraging.																		1999-4893				APR	2020	13	4							91	10.3390/a13040091													
J								Representation of Traffic Congestion Data for Urban Road Traffic Networks Based on Pooling Operations	ALGORITHMS										road network; traffic congestion; representation method; data compression; short-term traffic prediction; deep learning	FLOW PREDICTION; SVR	In order to improve the efficiency of transportation networks, it is critical to forecast traffic congestion. Large-scale traffic congestion data have become available and accessible, yet they need to be properly represented in order to avoid overfitting, reduce the requirements of computational resources, and be utilized effectively by various methodologies and models. Inspired by pooling operations in deep learning, we propose a representation framework for traffic congestion data in urban road traffic networks. This framework consists of grid-based partition of urban road traffic networks and a pooling operation to reduce multiple values into an aggregated one. We also propose using a pooling operation to calculate the maximum value in each grid (MAV). Raw snapshots of traffic congestion maps are transformed and represented as a series of matrices which are used as inputs to a spatiotemporal congestion prediction network (STCN) to evaluate the effectiveness of representation when predicting traffic congestion. STCN combines convolutional neural networks (CNNs) and long short-term memory neural network (LSTMs) for their spatiotemporal capability. CNNs can extract spatial features and dependencies of traffic congestion between roads, and LSTMs can learn their temporal evolution patterns and correlations. An empirical experiment on an urban road traffic network shows that when incorporated into our proposed representation framework, MAV outperforms other pooling operations in the effectiveness of the representation of traffic congestion data for traffic congestion prediction, and that the framework is cost-efficient in terms of computational resources.																		1999-4893				APR	2020	13	4							84	10.3390/a13040084													
J								Evaluation of State-of-the-Art Paraphrase Identification and Its Application to Automatic Plagiarism Detection	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Paraphrase identification; similarity-based methods; classification methods; deep learning; automatic plagiarism detection		Paraphrase identification is a natural language processing (NLP) problem that involves the determination of whether two text segments have the same meaning. Various NLP applications rely on a solution to this problem, including automatic plagiarism detection, text summarization, machine translation (MT), and question answering. The methods for identifying paraphrases found in the literature fall into two main classes: similarity-based methods and classification methods. This paper presents a critical study and an evaluation of existing methods for paraphrase identification and its application to automatic plagiarism detection. It presents the classes of paraphrase phenomena, the main methods, and the sets of features used by each particular method. All the methods and features used are discussed and enumerated in a table for easy comparison. Their performances on benchmark corpora are also discussed and compared via tables. Automatic plagiarism detection is presented as an application of paraphrase identification. The performances on benchmark corpora of existing plagiarism detection systems able to detect paraphrases are compared and discussed. The main outcome of this study is the identification of word overlap, structural representations, and MT measures as feature subsets that lead to the best performance results for support vector machines in both paraphrase identification and plagiarism detection on corpora. The performance results achieved by deep learning techniques highlight that these techniques are the most promising research direction in this field.																	0218-0014	1793-6381				APR	2020	34	4							2053004	10.1142/S0218001420530043													
J								Learned versus Handcrafted Features for Person Re-identification	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Person re-identification; Siamese convolutional neural network; handcrafted features; learned features; pre-trained models; transfer learning		Person re-identification is one of the indispensable elements for visual surveillance. It assigns consistent labeling for the same person within the field of view of the same camera or even across multiple cameras. While handcrafted feature extraction is certainly one way of approaching this problem, in many cases, these features are becoming more and more complex. Besides, training a deep convolutional neural network (CNN) from scratch is difficult because it requires a large amount of labeled training data and a great deal of expertise to ensure proper convergence. This paper explores the following three main strategies for solving the person re-identification problem: (i) using handcrafted features, (ii) using transfer learning based on a pre-trained deep CNN (trained for object categorization) and (iii) training a deep CNN from scratch. Our experiments consistently demonstrated that: (1) The handcrafted features may still have favorable characteristics and benefits especially in cases where the learning database is not sufficient to train a deep network. (2) A fully trained Siamese CNN outperforms handcrafted approaches and the combination of pre-trained CNN with different re-identification processes. (3) Moreover, our experiments demonstrated that pre-trained features and handcrafted features perform equally well. These experiments have also revealed the most discriminative parts in the human body.																	0218-0014	1793-6381				APR	2020	34	4							2055009	10.1142/S0218001420550095													
J								Design and Analysis of Hard X-Ray Microscope Employing Toroidal Mirrors Working at Grazing-Incidence	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										X-ray microscope; grazing incidence; toroidal mirror; ray-tracing simulation		A high resolution microscope is designed for plasma hard X-ray (10-20 keV) imaging diagnosis. This system consists of two toroidal mirrors, which are nearly parallel, with an angle twice that of the grazing incidence angle and a plane mirror for spectral selection and correction of optical axis offset. The imaging characteristics of single toroidal mirror and double mirrors are analyzed in detail by the optical path function. The optical design, parameter optimization, image quality simulation and analysis of the microscope are carried out. The optimized hard X-ray microscope has a resolution better than 5 mu m at 1 mm object field of view. The experimental data shows that the variation of the resolution is smaller in the direction of incident angle decrease than that in the increasing direction.																	0218-0014	1793-6381				APR	2020	34	4							2055010	10.1142/S0218001420550101													
J								Extracting the Boundaries of Clusters: A Post-Clustering Tool for Spatial Datasets	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Boundary extraction; clustering; data mining; dataset reduction; data visualization	DEFECT PATTERNS; SHAPE; SET; ALGORITHM; POINTS; REGION; SEGMENTATION; COMPUTATION; BORDER	Boundary extraction is a fundamental post-clustering problem. It facilitates interpretability and usability of clustering results. Also, it provides visualization and dataset reduction. However, it has not attracted much attention compared to the clustering problem itself. In this work, we address the boundary extraction of clusters in 2- and 3-dimensional spatial datasets. We propose two algorithms based on Delaunay Triangulation (DT). Numerical experiments show that the proposed algorithms generate the cluster boundaries effectively. Also, they yield significant amounts of dataset reduction.																	0218-0014	1793-6381				APR	2020	34	4							2050008	10.1142/S0218001420500081													
J								Efficient and Adaptive Tone Mapping Algorithm Based on Guided Image Filter	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Adaptive; tone mapping; efficient; guided image filter; realtime	MODEL	The recognition rate of computer vision algorithms is highly dependent on the image quality. To enhance the visual quality of the images captured under high-dynamic range (HDR) scenes, we propose an efficient and adaptive tone mapping algorithm based on guided image filter (GIF). The HDR image is compressed adaptively according to its average luminance. Then we decompose it into a base layer and a detail layer using the guided image filter. We improve the base layer and enhance the detail layer simultaneously, and combine the two layers to get the final low-dynamic range (LDR) image. Since the parameters are linked with image statistics, they adaptively fit to various kinds of images. The objective evaluation results on HDR image sets demonstrate the superiority of our proposed algorithm. Meanwhile, the result of our algorithm can reduce the halo artifacts and preserve more detail by subjective observation.																	0218-0014	1793-6381				APR	2020	34	4							2054012	10.1142/S0218001420540129													
J								Characterizing the Minimal Essential Graphs of Maximal Ancestral Graphs	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Bayesian networks; maximal ancestral graph; latent variable; selection variable; essential graph; minimal collider path	MARKOV EQUIVALENCE; LATENT	Learning ancestor graph is a typical NP-hard problem. We consider the problem to represent a Markov equivalence class of ancestral graphs with a compact representation. Firstly, the minimal essential graph is defined to represent the equivalent class of maximal ancestral graphs with the minimum number of invariant arrowheads. Then, an algorithm is proposed to learn the minimal essential graph of ancestral graphs based on the detection of minimal collider paths. It is the first algorithm to use necessary and sufficient conditions for Markov equivalence as a base to seek essential graphs. Finally, a set of orientation rules is presented to orient edge marks of a minimal essential graph. Theory analysis shows our algorithm is sound, and complete in the sense of recognizing all minimal collider paths in a given ancestral graph. And the experiment results show we can discover all invariant marks by these orientation rules.																	0218-0014	1793-6381				APR	2020	34	4							2059009	10.1142/S0218001420590090													
J								A Filled Flatten Function Method Based on Basin Deepening and Adaptive Initial Point for Global Optimization	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Filled function; global optimization; adaptive initial point strategy; basin deepening	ONE-PARAMETER; TUNNELING ALGORITHM; MINIMIZER; TRUST	Currently, there are four drawbacks for filled function methods: (1) too many local optimal solutions result in huge difficulty to search global optimal solutions; (2) difficult to control the parameter(s); (3) difficult to determine the initial point for minimization of filled function; (4) the shallow basins will affect the solution precision during the minimization of the filled function. To overcome these drawbacks, in this paper, we adopt a flatten function to eliminate many local optimal solutions first, and then a new filled function with one parameter is proposed, and its parameter is easy to control. Furthermore, we propose an efficient method for determining initial point of the filled function by using adaptive step size. Moreover, when some basins of the filled function are shallow ones, it will result in inefficiency of searching these basins during the minimization of the filled function. To tackle this issue and make the search for global optimal solutions much easier, we propose a strategy of basin deepening. By integrating these schemes, we propose a new efficient filled flatten function method. Numerical results indicate the efficiency and effectiveness of the proposed filled function methods.																	0218-0014	1793-6381				APR	2020	34	4							2059011	10.1142/S0218001420590119													
J								A Novel Fault Classification Technique in Series Compensated Transmission Line using Ensemble Method	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Series compensation; multi-resolution wavelet analysis (MRA); Support Vector Machine; Ensemble Classifier	NEURAL-NETWORK; WAVELET TRANSFORM; PROTECTION SCHEME	The paper presents a novel approach for classification of faults in Extra High Voltage (EHV) transmission line with series compensation. The proposed algorithm utilizes single end currents extracted from three phases of a transmission line given to a fault detection and classification system. A detailed model is constructed to analyze fault patterns occurring on a dual feed system with multiple series compensation provided on EHV transmission line. The algorithm uses a full cycle of post-fault currents. A Multiresolution Analysis Wavelet-based decomposition technique is used to provide a joint time frequency analysis. Extensive study is done by designing different fault classifiers using Support Vector Machine (SVM) with different feature vector groups. The SVMs with different parameters are compared to show performance of SVMs in given feature space of faults. A new algorithm is proposed with Ensemble method having group of different classifiers, namely, Artificial Neural Network (ANN), K-Nearest Neighborhood (KNN) and SVM. A combination of feature selection with Ensemble Classifiers is trained and tested on a wide range of fault patterns. A significant improvement in performance is obtained with different combination of features in Ensemble Classifier using subspace partition method.The proposed Ensemble Classifier provides an accuracy of 99.5%.																	0218-0014	1793-6381				APR	2020	34	4							2050009	10.1142/S0218001420500093													
J								Recovering Truncated Streaming Event Log Using Coupled Hidden Markov Model	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Backward method; coupled Hidden Markov Model; incomplete trace; truncated streaming event log; Viterbi method		Process discovery is a technique for obtaining process model based on traces recorded in the event log. Nowadays, information systems produce streaming event logs to record their huge processes. The truncated streaming event log is a big issue in process discovery because it inflicts incomplete traces that make process discovery depict wrong processes in a process model. Earlier research suggested several methods for recovering the truncated streaming event log and none of them utilized Coupled Hidden Markov Model. This research proposes a method that combines Coupled Hidden Markov Model with Double States and the Modification of Viterbi-Backward method for recovering the truncated streaming event log. The first layer of states contains the transition probability of activities. The second layer of states uses patterns for detecting traces which have a low appearance in the event log. The experiment results showed that the proposed method recovered appropriately the truncated streaming event log. These results also have proven that the accuracies of recovered traces obtained by the proposed method are higher than those obtained by the Hidden Markov Model and the Coupled Hidden Markov Model.																	0218-0014	1793-6381				APR	2020	34	4							2059012	10.1142/S0218001420590120													
